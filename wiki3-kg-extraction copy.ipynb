{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# wiki3 Knowledge Graph Extraction Pipeline\n",
        "\n",
        "Extract structured knowledge from Wikipedia articles using LLMs, generate embeddings, and prepare for browser-based querying with DuckDB-Wasm.\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "[ERR_PACKAGE_PATH_NOT_EXPORTED] Package subpath './output_parsers/structured' is not defined by \"exports\" in '/home/jovyan/.cache/deno/npm/registry.npmjs.org/@langchain/core/1.1.6/package.json' imported from 'file:///home/jovyan/work/$deno$repl.mts'",
          "output_type": "error",
          "traceback": [
            "Stack trace:",
            "TypeError: [ERR_PACKAGE_PATH_NOT_EXPORTED] Package subpath './output_parsers/structured' is not defined by \"exports\" in '/home/jovyan/.cache/deno/npm/registry.npmjs.org/@langchain/core/1.1.6/package.json' imported from 'file:///home/jovyan/work/$deno$repl.mts'",
            "    at async <anonymous>:6:36"
          ]
        }
      ],
      "source": [
        "import { ChatOpenAI } from \"npm:@langchain/openai\";\n",
        "import { WikipediaQueryRun } from \"npm:@langchain/community/tools/wikipedia_query_run\";\n",
        "import { RecursiveCharacterTextSplitter } from \"npm:@langchain/textsplitters\";\n",
        "import { ChatPromptTemplate } from \"npm:@langchain/core/prompts\";\n",
        "import { z } from \"npm:zod\";\n",
        "\n",
        "console.log(\"Dependencies loaded\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wikipedia Loader & Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "const wikipediaFetcher = new WikipediaQueryRun();\n",
        "const articleTitle = \"Albert Einstein\";\n",
        "const rawContent = await wikipediaFetcher.call(articleTitle);\n",
        "\n",
        "console.log(`Fetched: ${articleTitle}`);\n",
        "console.log(`Content length: ${rawContent.length} characters`);\n",
        "console.log(`Preview: ${rawContent.substring(0, 300)}...`);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure LLM for Knowledge Graph Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "const llm = new ChatOpenAI({\n",
        "  model: \"gpt-4o-mini\",\n",
        "  temperature: 0,\n",
        "  apiKey: Deno.env.get(\"OPENAI_API_KEY\"),\n",
        "});\n",
        "\n",
        "// Define the extraction schema using Zod\n",
        "const extractionSchema = z.object({\n",
        "  entities: z.array(\n",
        "    z.object({\n",
        "      id: z.string().describe(\"Unique entity identifier (e.g., person_1, org_2)\"),\n",
        "      label: z.string().describe(\"Entity name or label\"),\n",
        "      type: z.enum([\"Person\", \"Organization\", \"Place\", \"Concept\", \"Event\", \"Work\"]),\n",
        "      description: z.string().describe(\"Brief entity description from text\"),\n",
        "    })\n",
        "  ).describe(\"List of entities extracted from the text\"),\n",
        "  relations: z.array(\n",
        "    z.object({\n",
        "      source_id: z.string().describe(\"Source entity ID\"),\n",
        "      target_id: z.string().describe(\"Target entity ID\"),\n",
        "      relation_type: z.string().describe(\"Type of relationship (e.g., BORN_IN, WORKED_AT, DISCOVERED)\"),\n",
        "      description: z.string().describe(\"Description of the relationship\"),\n",
        "    })\n",
        "  ).describe(\"List of relationships between entities\"),\n",
        "});\n",
        "\n",
        "// Use withStructuredOutput for reliable JSON extraction\n",
        "const structuredLlm = llm.withStructuredOutput(extractionSchema);\n",
        "\n",
        "console.log(\"LLM and schema configured with withStructuredOutput\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract Knowledge Graph from Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "const splitter = new RecursiveCharacterTextSplitter({\n",
        "  chunkSize: 1024,\n",
        "  chunkOverlap: 128,\n",
        "});\n",
        "\n",
        "const chunks = await splitter.splitText(rawContent);\n",
        "console.log(`Split into ${chunks.length} chunks`);\n",
        "console.log(`Chunk 1 preview: ${chunks[0].substring(0, 150)}...`);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "const extractionPrompt = ChatPromptTemplate.fromMessages([\n",
        "  [\"system\", `You are an expert at extracting knowledge graphs from text.\n",
        "Extract entities and relationships from the provided text.\n",
        "Assign unique IDs to each entity using the format: type_number (e.g., \"person_1\", \"org_2\", \"place_3\").\n",
        "Identify relationships between entities that appear in the same context.\n",
        "Focus on factual relationships like: BORN_IN, WORKED_AT, DISCOVERED, FOUNDED, RECEIVED, MARRIED_TO, DEVELOPED, etc.`],\n",
        "  [\"human\", \"Extract entities and relationships from this text:\\n\\n{text}\"]\n",
        "]);\n",
        "\n",
        "const extractionChain = extractionPrompt.pipe(structuredLlm);\n",
        "console.log(\"Extraction chain created\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "const allEntities = new Map();\n",
        "const allRelations = [];\n",
        "const processChunks = 2;\n",
        "\n",
        "for (let i = 0; i < Math.min(processChunks, chunks.length); i++) {\n",
        "  console.log(`Processing chunk ${i + 1}/${Math.min(processChunks, chunks.length)}...`);\n",
        "  try {\n",
        "    const result = await extractionChain.invoke({\n",
        "      text: chunks[i],\n",
        "    });\n",
        "\n",
        "    // Deduplicate entities by label\n",
        "    for (const entity of result.entities) {\n",
        "      const key = entity.label.toLowerCase();\n",
        "      if (!allEntities.has(key)) {\n",
        "        allEntities.set(key, {\n",
        "          id: `${entity.type.toLowerCase()}_${allEntities.size + 1}`,\n",
        "          label: entity.label,\n",
        "          type: entity.type,\n",
        "          description: entity.description,\n",
        "        });\n",
        "      }\n",
        "    }\n",
        "\n",
        "    // Map relations to deduplicated entity IDs\n",
        "    for (const relation of result.relations) {\n",
        "      const sourceEntity = result.entities.find(e => e.id === relation.source_id);\n",
        "      const targetEntity = result.entities.find(e => e.id === relation.target_id);\n",
        "      \n",
        "      if (sourceEntity && targetEntity) {\n",
        "        const sourceKey = sourceEntity.label.toLowerCase();\n",
        "        const targetKey = targetEntity.label.toLowerCase();\n",
        "        \n",
        "        allRelations.push({\n",
        "          source_id: allEntities.get(sourceKey)?.id || relation.source_id,\n",
        "          target_id: allEntities.get(targetKey)?.id || relation.target_id,\n",
        "          relation_type: relation.relation_type,\n",
        "          description: relation.description,\n",
        "        });\n",
        "      }\n",
        "    }\n",
        "\n",
        "    console.log(`  Extracted ${result.entities.length} entities, ${result.relations.length} relations`);\n",
        "  } catch (error) {\n",
        "    console.error(`  Error processing chunk ${i + 1}:`, error.message);\n",
        "  }\n",
        "}\n",
        "\n",
        "console.log(`\\nTotal unique entities: ${allEntities.size}`);\n",
        "console.log(`Total relations: ${allRelations.length}`);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect Extracted Knowledge Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "console.log(\"\\n=== ENTITIES ===\");\n",
        "for (const [, entity] of allEntities) {\n",
        "  console.log(`[${entity.id}] ${entity.label} (${entity.type})`);\n",
        "  console.log(`  â†’ ${entity.description}`);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "console.log(\"\\n=== RELATIONS ===\");\n",
        "const entitiesArr = Array.from(allEntities.values());\n",
        "for (const relation of allRelations) {\n",
        "  const source = entitiesArr.find(e => e.id === relation.source_id);\n",
        "  const target = entitiesArr.find(e => e.id === relation.target_id);\n",
        "  console.log(`${source?.label} --[${relation.relation_type}]--> ${target?.label}`);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import { Sha256 } from \"https://deno.land/std@0.208.0/crypto/sha256.ts\";\n",
        "\n",
        "function getMockEmbedding(text) {\n",
        "  const hash = new Sha256().update(text).digest();\n",
        "  const embedding = [];\n",
        "  for (let i = 0; i < 384; i++) {\n",
        "    embedding.push((hash[i % hash.length] - 128) / 128);\n",
        "  }\n",
        "  return embedding;\n",
        "}\n",
        "\n",
        "const entityEmbeddings = new Map();\n",
        "for (const [, entity] of allEntities) {\n",
        "  entityEmbeddings.set(entity.id, {\n",
        "    ...entity,\n",
        "    embedding: getMockEmbedding(entity.label + entity.description),\n",
        "  });\n",
        "}\n",
        "\n",
        "console.log(`Generated embeddings for ${entityEmbeddings.size} entities`);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export to DuckDB-Wasm Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "const duckdbFormat = {\n",
        "  metadata: {\n",
        "    source: articleTitle,\n",
        "    extracted_at: new Date().toISOString(),\n",
        "    entity_count: allEntities.size,\n",
        "    relation_count: allRelations.length,\n",
        "  },\n",
        "  entities: Array.from(entityEmbeddings.values()),\n",
        "  relations: allRelations,\n",
        "};\n",
        "\n",
        "console.log(\"\\n=== EXPORT ===\");\n",
        "console.log(JSON.stringify(duckdbFormat, null, 2).substring(0, 500) + \"...\");"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Deno",
      "language": "typescript",
      "name": "deno"
    },
    "language_info": {
      "codemirror_mode": "typescript",
      "file_extension": ".ts",
      "mimetype": "text/x.typescript",
      "name": "typescript",
      "nbconvert_exporter": "script",
      "pygments_lexer": "typescript",
      "version": "5.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
