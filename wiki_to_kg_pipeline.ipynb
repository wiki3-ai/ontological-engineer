{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236e29d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# mamba install -c conda-forge langchain langchain-text-splitters langchain-openai langchain-community wikipedia pydantic nbformat nbconvert\n",
    "%pip install -q langchain langchain-text-splitters langchain-openai langchain-community wikipedia pydantic nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833f695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded (with CID support)\n",
      "Cell timeout: 60s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "from urllib.parse import quote\n",
    "\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell, new_raw_cell\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# === Content ID (CID) Functions ===\n",
    "\n",
    "def compute_cid(content: str) -> str:\n",
    "    \"\"\"Compute SHA256 content ID for a string.\"\"\"\n",
    "    return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "def make_signature(cell_num: int, cell_type: str, cid: str, from_cid: str) -> dict:\n",
    "    \"\"\"Create a signature dict for a generated cell.\"\"\"\n",
    "    return {\n",
    "        \"cell\": cell_num,\n",
    "        \"type\": cell_type,\n",
    "        \"cid\": cid,\n",
    "        \"from_cid\": from_cid,\n",
    "    }\n",
    "\n",
    "def parse_signature(raw_content: str) -> Optional[dict]:\n",
    "    \"\"\"Parse a signature from raw cell content. Returns None if not a valid signature.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(raw_content.strip())\n",
    "        if all(k in data for k in (\"cell\", \"type\", \"cid\", \"from_cid\")):\n",
    "            return data\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def extract_signatures(notebook) -> dict:\n",
    "    \"\"\"Extract all signatures from a notebook, keyed by cell number.\"\"\"\n",
    "    signatures = {}\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == 'raw':\n",
    "            sig = parse_signature(cell.source)\n",
    "            if sig:\n",
    "                signatures[sig[\"cell\"]] = sig\n",
    "    return signatures\n",
    "\n",
    "# === Timeout Configuration ===\n",
    "# Note: SIGALRM doesn't work for blocking I/O (HTTP requests)\n",
    "# Instead, we use the timeout parameter on the ChatOpenAI client\n",
    "\n",
    "CELL_TIMEOUT_SECONDS = 60  # 1 minute per cell max\n",
    "MAX_ITERATIONS = 150     # Max tools calls per cell processing\n",
    "\n",
    "def log_progress(msg: str, end=\"\\n\"):\n",
    "    \"\"\"Print with immediate flush for real-time progress.\"\"\"\n",
    "    print(msg, end=end, flush=True)\n",
    "\n",
    "print(\"Dependencies loaded (with CID support)\")\n",
    "print(f\"Cell timeout: {CELL_TIMEOUT_SECONDS}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252ef95",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79c403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline configured for: Albert Einstein\n",
      "Output directory: data/albert_einstein/20251218_202954\n",
      "Run timestamp: 20251218_202954\n"
     ]
    }
   ],
   "source": [
    "# Pipeline configuration\n",
    "ARTICLE_TITLE = \"Albert Einstein\"\n",
    "OUTPUT_DIR = \"data\"\n",
    "CHUNK_SIZE = 2000\n",
    "CHUNK_OVERLAP = 128\n",
    "\n",
    "# Continue from a previous run (set to None for fresh run)\n",
    "# Example: \"data/albert_einstein/20241218_143022\"\n",
    "CONTINUE_FROM_RUN = \"data/albert_einstein/20251218_202954\"\n",
    "\n",
    "# LLM configuration (shared across stages)\n",
    "LLM_CONFIG = {\n",
    "    \"provider\": \"lm_studio\",  # or \"openai\"\n",
    "    \"model\": \"qwen/qwen3-coder-30b\",\n",
    "    \"temperature\": 1,\n",
    "    \"base_url\": os.environ.get(\"LM_STUDIO_BASE_URL\", \"http://host.docker.internal:1234/v1\"),\n",
    "}\n",
    "\n",
    "# Generate run timestamp and article-specific output directory\n",
    "# Structure: data/{article_slug}/{timestamp}/\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "article_slug = ARTICLE_TITLE.lower().replace(' ', '_')\n",
    "RUN_OUTPUT_DIR = os.path.join(OUTPUT_DIR, article_slug, RUN_TIMESTAMP)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RUN_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Copy files from previous run if specified\n",
    "if CONTINUE_FROM_RUN:\n",
    "    import shutil\n",
    "    if os.path.isdir(CONTINUE_FROM_RUN):\n",
    "        print(f\"Continuing from previous run: {CONTINUE_FROM_RUN}\")\n",
    "        copied_files = []\n",
    "        for filename in os.listdir(CONTINUE_FROM_RUN):\n",
    "            src = os.path.join(CONTINUE_FROM_RUN, filename)\n",
    "            dst = os.path.join(RUN_OUTPUT_DIR, filename)\n",
    "            if os.path.isfile(src):\n",
    "                shutil.copy2(src, dst)\n",
    "                copied_files.append(filename)\n",
    "        print(f\"  Copied {len(copied_files)} files: {', '.join(copied_files)}\")\n",
    "        print(f\"  Existing cells with matching CIDs will be skipped\")\n",
    "    else:\n",
    "        print(f\"WARNING: Previous run not found: {CONTINUE_FROM_RUN}\")\n",
    "        print(f\"  Starting fresh run instead\")\n",
    "\n",
    "print(f\"Pipeline configured for: {ARTICLE_TITLE}\")\n",
    "print(f\"Output directory: {RUN_OUTPUT_DIR}\")\n",
    "print(f\"Run timestamp: {RUN_TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df90b5b",
   "metadata": {},
   "source": [
    "## Entity Registry\n",
    "\n",
    "Manages entity identity across chunks with stable URIs derived from source URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a66f9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntityRegistry class defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class EntityRegistry:\n",
    "    \"\"\"Tracks entities with stable IDs derived from source URL.\"\"\"\n",
    "    source_url: str\n",
    "    entities: dict = field(default_factory=dict)  # normalized_key -> entity\n",
    "    aliases: dict = field(default_factory=dict)   # alias -> canonical_key\n",
    "    \n",
    "    def normalize_key(self, label: str) -> str:\n",
    "        \"\"\"Create consistent key from entity label.\"\"\"\n",
    "        return re.sub(r'[^a-z0-9]+', '_', label.lower().strip()).strip('_')\n",
    "    \n",
    "    def generate_uri(self, entity_type: str, label: str) -> str:\n",
    "        \"\"\"Generate URI based on source URL with fragment identifier.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        # Use source URL as base, add fragment for entity\n",
    "        return f\"{self.source_url}#{entity_type.lower()}_{key}\"\n",
    "    \n",
    "    def generate_id(self, entity_type: str, label: str) -> str:\n",
    "        \"\"\"Generate local ID for internal reference.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        return f\"{entity_type.lower()}_{key}\"\n",
    "    \n",
    "    def register(self, label: str, entity_type: str, description: str = \"\",\n",
    "                 aliases: list = None, source_chunk: int = None) -> str:\n",
    "        \"\"\"Register or update an entity, return canonical ID.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        entity_id = self.generate_id(entity_type, label)\n",
    "        entity_uri = self.generate_uri(entity_type, label)\n",
    "        \n",
    "        if key not in self.entities:\n",
    "            self.entities[key] = {\n",
    "                \"id\": entity_id,\n",
    "                \"uri\": entity_uri,\n",
    "                \"label\": label,\n",
    "                \"type\": entity_type,\n",
    "                \"descriptions\": [description] if description else [],\n",
    "                \"source_chunks\": [source_chunk] if source_chunk is not None else [],\n",
    "                \"aliases\": list(aliases or []),\n",
    "            }\n",
    "        else:\n",
    "            existing = self.entities[key]\n",
    "            if description and description not in existing[\"descriptions\"]:\n",
    "                existing[\"descriptions\"].append(description)\n",
    "            if source_chunk is not None and source_chunk not in existing[\"source_chunks\"]:\n",
    "                existing[\"source_chunks\"].append(source_chunk)\n",
    "            if aliases:\n",
    "                existing[\"aliases\"] = list(set(existing[\"aliases\"]) | set(aliases))\n",
    "        \n",
    "        # Register aliases\n",
    "        for alias in (aliases or []):\n",
    "            self.aliases[self.normalize_key(alias)] = key\n",
    "        \n",
    "        return entity_id\n",
    "    \n",
    "    def lookup(self, label: str) -> Optional[dict]:\n",
    "        \"\"\"Find entity by label or alias.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        canonical_key = self.aliases.get(key, key)\n",
    "        return self.entities.get(canonical_key)\n",
    "    \n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Serialize registry to JSON.\"\"\"\n",
    "        return json.dumps({\n",
    "            \"source_url\": self.source_url,\n",
    "            \"entities\": self.entities,\n",
    "            \"aliases\": self.aliases,\n",
    "        }, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, json_str: str) -> 'EntityRegistry':\n",
    "        \"\"\"Deserialize registry from JSON.\"\"\"\n",
    "        data = json.loads(json_str)\n",
    "        registry = cls(source_url=data[\"source_url\"])\n",
    "        registry.entities = data[\"entities\"]\n",
    "        registry.aliases = data[\"aliases\"]\n",
    "        return registry\n",
    "\n",
    "print(\"EntityRegistry class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094a385",
   "metadata": {},
   "source": [
    "## Section Hierarchy Parser\n",
    "\n",
    "Extracts Wikipedia section structure for breadcrumb context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fe68df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section hierarchy parser defined\n"
     ]
    }
   ],
   "source": [
    "def extract_section_hierarchy(content: str) -> list[dict]:\n",
    "    \"\"\"Parse Wikipedia == headers == into hierarchical structure with positions.\"\"\"\n",
    "    header_pattern = re.compile(r'^(={2,6})\\s*(.+?)\\s*\\1\\s*$', re.MULTILINE)\n",
    "    \n",
    "    sections = []\n",
    "    current_path = []  # Stack of (level, title)\n",
    "    \n",
    "    for match in header_pattern.finditer(content):\n",
    "        level = len(match.group(1))  # Number of '=' signs\n",
    "        title = match.group(2).strip()\n",
    "        \n",
    "        # Pop stack until we're at parent level\n",
    "        while current_path and current_path[-1][0] >= level:\n",
    "            current_path.pop()\n",
    "        \n",
    "        current_path.append((level, title))\n",
    "        breadcrumb = \" > \".join(t for _, t in current_path)\n",
    "        \n",
    "        sections.append({\n",
    "            \"level\": level,\n",
    "            \"title\": title,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"start_pos\": match.start(),\n",
    "            \"end_pos\": match.end(),\n",
    "        })\n",
    "    \n",
    "    return sections\n",
    "\n",
    "\n",
    "def get_section_context(position: int, sections: list[dict], article_title: str) -> dict:\n",
    "    \"\"\"Find the section context for a given character position.\"\"\"\n",
    "    active_section = {\n",
    "        \"title\": \"Introduction\",\n",
    "        \"breadcrumb\": \"Introduction\",\n",
    "        \"level\": 1,\n",
    "    }\n",
    "    \n",
    "    for section in sections:\n",
    "        if section[\"start_pos\"] <= position:\n",
    "            active_section = section\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"section_title\": active_section[\"title\"],\n",
    "        \"breadcrumb\": f\"{article_title} > {active_section['breadcrumb']}\",\n",
    "    }\n",
    "\n",
    "print(\"Section hierarchy parser defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360c37f",
   "metadata": {},
   "source": [
    "## Fetch Wikipedia Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b91833b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched: Albert Einstein\n",
      "Source URL: https://en.wikipedia.org/wiki/Albert_Einstein\n",
      "Content length: 87959 characters\n",
      "License: CC BY-SA 4.0\n"
     ]
    }
   ],
   "source": [
    "# Fetch Wikipedia article\n",
    "loader = WikipediaLoader(query=ARTICLE_TITLE, load_max_docs=1, doc_content_chars_max=100000)\n",
    "docs = loader.load()\n",
    "\n",
    "if not docs:\n",
    "    raise ValueError(f\"Could not fetch article: {ARTICLE_TITLE}\")\n",
    "\n",
    "raw_content = docs[0].page_content\n",
    "metadata = docs[0].metadata\n",
    "\n",
    "# Construct source URL and provenance\n",
    "source_url = f\"https://en.wikipedia.org/wiki/{quote(ARTICLE_TITLE.replace(' ', '_'))}\"\n",
    "\n",
    "provenance = {\n",
    "    \"source_url\": source_url,\n",
    "    \"article_title\": ARTICLE_TITLE,\n",
    "    \"fetched_at\": datetime.now().isoformat(),\n",
    "    \"content_length\": len(raw_content),\n",
    "    # Wikipedia license - standard for all Wikipedia content\n",
    "    \"license\": \"CC BY-SA 4.0\",\n",
    "    \"license_url\": \"https://creativecommons.org/licenses/by-sa/4.0/\",\n",
    "    \"attribution\": \"Wikipedia contributors\",\n",
    "}\n",
    "\n",
    "print(f\"Fetched: {ARTICLE_TITLE}\")\n",
    "print(f\"Source URL: {source_url}\")\n",
    "print(f\"Content length: {len(raw_content)} characters\")\n",
    "print(f\"License: {provenance['license']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1276b5",
   "metadata": {},
   "source": [
    "## Create Contextual Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3146caad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 sections\n",
      "Split into 63 chunks\n",
      "\n",
      "Chunks with context:\n",
      "  Chunk 1: Albert Einstein > Introduction\n",
      "  Chunk 2: Albert Einstein > Introduction\n",
      "  Chunk 3: Albert Einstein > Life and career\n"
     ]
    }
   ],
   "source": [
    "# Parse section hierarchy\n",
    "sections = extract_section_hierarchy(raw_content)\n",
    "print(f\"Found {len(sections)} sections\")\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "raw_chunks = splitter.split_text(raw_content)\n",
    "print(f\"Split into {len(raw_chunks)} chunks\")\n",
    "\n",
    "# Add context to each chunk\n",
    "@dataclass\n",
    "class ContextualChunk:\n",
    "    content: str\n",
    "    chunk_index: int\n",
    "    total_chunks: int\n",
    "    breadcrumb: str\n",
    "    section_title: str\n",
    "    char_start: int\n",
    "    char_end: int\n",
    "\n",
    "contextual_chunks = []\n",
    "current_pos = 0\n",
    "\n",
    "for i, chunk_text in enumerate(raw_chunks):\n",
    "    # Find position in original content\n",
    "    chunk_start = raw_content.find(chunk_text, current_pos)\n",
    "    if chunk_start == -1:\n",
    "        chunk_start = current_pos  # Fallback\n",
    "    chunk_end = chunk_start + len(chunk_text)\n",
    "    \n",
    "    # Get section context\n",
    "    context = get_section_context(chunk_start, sections, ARTICLE_TITLE)\n",
    "    \n",
    "    contextual_chunks.append(ContextualChunk(\n",
    "        content=chunk_text,\n",
    "        chunk_index=i,\n",
    "        total_chunks=len(raw_chunks),\n",
    "        breadcrumb=context[\"breadcrumb\"],\n",
    "        section_title=context[\"section_title\"],\n",
    "        char_start=chunk_start,\n",
    "        char_end=chunk_end,\n",
    "    ))\n",
    "    \n",
    "    current_pos = chunk_start + 1\n",
    "\n",
    "print(f\"\\nChunks with context:\")\n",
    "for chunk in contextual_chunks[:3]:\n",
    "    print(f\"  Chunk {chunk.chunk_index + 1}: {chunk.breadcrumb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3af602",
   "metadata": {},
   "source": [
    "## Initialize Entity Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34b0e8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity registry initialized with subject: Albert Einstein\n",
      "Subject URI: https://en.wikipedia.org/wiki/Albert_Einstein#person_albert_einstein\n"
     ]
    }
   ],
   "source": [
    "# Initialize entity registry with article subject\n",
    "registry = EntityRegistry(source_url=source_url)\n",
    "\n",
    "# Pre-seed with the article subject\n",
    "registry.register(\n",
    "    label=ARTICLE_TITLE,\n",
    "    entity_type=\"Person\",  # Adjust based on article type\n",
    "    description=f\"Subject of Wikipedia article: {ARTICLE_TITLE}\",\n",
    "    aliases=[ARTICLE_TITLE.split()[-1]],  # Last name as alias\n",
    ")\n",
    "\n",
    "print(f\"Entity registry initialized with subject: {ARTICLE_TITLE}\")\n",
    "print(f\"Subject URI: {registry.entities[registry.normalize_key(ARTICLE_TITLE)]['uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd3cdd",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "These prompts are embedded in the generated notebooks for transparency and adjustability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3089027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates defined\n"
     ]
    }
   ],
   "source": [
    "FACTS_EXTRACTION_PROMPT = \"\"\"You are an expert at extracting factual information from text.\n",
    "\n",
    "Given text from a Wikipedia article, extract simple English statements that capture the consensus factual information. Each statement should:\n",
    "- Be a single, clear sentence\n",
    "- Contain one main fact or relationship\n",
    "- Use the full name of entities on first mention\n",
    "- Be verifiable from the source text\n",
    "- Avoid opinions, interpretations, or hedged language\n",
    "\n",
    "The text comes from: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Known entities (use consistent names):\n",
    "{known_entities}\n",
    "\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\n",
    "Extract factual statements as a bulleted list. Also identify any new entities (people, places, organizations, concepts, events, works) that should be added to the registry.\n",
    "\"\"\"\n",
    "\n",
    "RDF_GENERATION_PROMPT = \"\"\"You are an expert at converting factual statements to RDF triples in Turtle format.\n",
    "\n",
    "Convert the following factual statements to RDF using schema.org vocabulary where possible.\n",
    "\n",
    "Source: {source_url}\n",
    "Section: {breadcrumb}\n",
    "\n",
    "Use these prefixes:\n",
    "{prefixes}\n",
    "\n",
    "Entity registry (use these URIs):\n",
    "{entity_registry}\n",
    "\n",
    "Guidelines:\n",
    "- Use schema.org properties (schema:birthDate, schema:birthPlace, schema:worksFor, etc.)\n",
    "- For relationships not in schema.org, use wiki3: prefix\n",
    "- Include rdfs:label for entities\n",
    "- Use xsd datatypes for dates and numbers\n",
    "- Entity URIs should use the source URL as base with fragment identifiers\n",
    "\n",
    "---\n",
    "{facts}\n",
    "---\n",
    "\n",
    "Generate Turtle RDF:\n",
    "\"\"\"\n",
    "\n",
    "RDF_PREFIXES = \"\"\"@prefix schema: <https://schema.org/> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix wiki3: <https://wiki3.ai/vocab/> .\n",
    "@base <{source_url}> .\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prompt templates defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa35e61",
   "metadata": {},
   "source": [
    "## Generate Chunks Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d121080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: data/albert_einstein/20251218_202954/chunks.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_chunks_notebook(chunks: list, provenance: dict, registry: EntityRegistry, \n",
    "                             llm_config: dict, output_path: str):\n",
    "    \"\"\"Generate a notebook with chunked source text and context metadata.\n",
    "    \n",
    "    Each chunk cell is followed by a signature raw cell with its CID.\n",
    "    \"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    # Cell 0: Provenance markdown\n",
    "    provenance_yaml = f\"\"\"# Chunked Text: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "fetched_at: {provenance['fetched_at']}\n",
    "content_length: {provenance['content_length']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "attribution: {provenance['attribution']}\n",
    "chunk_size: {CHUNK_SIZE}\n",
    "chunk_overlap: {CHUNK_OVERLAP}\n",
    "total_chunks: {len(chunks)}\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "Each chunk below contains source text with contextual metadata. The context line (before the separator) provides:\n",
    "- **Context**: Hierarchical breadcrumb showing article > section path\n",
    "- **Chunk**: Position in sequence\n",
    "\n",
    "The text below the `---` separator is the unchanged source content.\n",
    "Each chunk is followed by a signature cell containing its Content ID (CID).\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_yaml))\n",
    "    \n",
    "    # Cell 1: Entity registry (raw cell)\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    # Chunk cells with signatures\n",
    "    # Compute CID of raw source for provenance\n",
    "    source_cid = compute_cid(provenance['source_url'] + str(provenance['content_length']))\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Content cell\n",
    "        chunk_content = f\"\"\"**Context:** {chunk.breadcrumb}\n",
    "**Chunk:** {chunk.chunk_index + 1} of {chunk.total_chunks}\n",
    "\n",
    "---\n",
    "\n",
    "{chunk.content}\n",
    "\"\"\"\n",
    "        nb.cells.append(new_markdown_cell(chunk_content))\n",
    "        \n",
    "        # Signature cell\n",
    "        chunk_cid = compute_cid(chunk_content)\n",
    "        signature = make_signature(\n",
    "            cell_num=chunk.chunk_index + 1,\n",
    "            cell_type=\"chunk\",\n",
    "            cid=chunk_cid,\n",
    "            from_cid=source_cid\n",
    "        )\n",
    "        nb.cells.append(new_raw_cell(json.dumps(signature)))\n",
    "    \n",
    "    # Write notebook\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(nb, f)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate chunks notebook (simple filename, timestamp is in directory)\n",
    "# Always regenerate chunks since they come from fresh Wikipedia fetch\n",
    "chunks_path = os.path.join(RUN_OUTPUT_DIR, \"chunks.ipynb\")\n",
    "generate_chunks_notebook(contextual_chunks, provenance, registry, LLM_CONFIG, chunks_path)\n",
    "print(f\"Generated: {chunks_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c23dc4",
   "metadata": {},
   "source": [
    "## Generate Facts Notebook (Structure Only)\n",
    "\n",
    "Creates the facts notebook with placeholders. Actual content is generated in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4acab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: data/albert_einstein/20251218_202954/facts.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_facts_notebook(chunks: list, provenance: dict, registry: EntityRegistry,\n",
    "                            llm_config: dict, source_notebook: str, output_path: str):\n",
    "    \"\"\"Generate a notebook structure for extracted facts.\n",
    "    \n",
    "    Creates header cells only - actual fact content is populated by the pipeline\n",
    "    during the facts extraction phase.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of ContextualChunk objects (for count info)\n",
    "        provenance: Dictionary with article provenance info\n",
    "        registry: EntityRegistry for consistent URIs\n",
    "        llm_config: Dict with model, endpoint config\n",
    "        source_notebook: Relative path to source chunks notebook\n",
    "        output_path: Where to write the facts notebook\n",
    "    \"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    # Cell 0: Provenance markdown\n",
    "    provenance_yaml = f\"\"\"# Extracted Facts: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_notebook: {source_notebook}\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "extraction_model: {llm_config['model']}\n",
    "extraction_timestamp: {datetime.now().isoformat()}\n",
    "total_chunks: {len(chunks)}\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "This notebook contains factual statements extracted from source chunks.\n",
    "Each facts cell is followed by a signature cell with CID provenance linking back to its source chunk.\n",
    "\n",
    "To regenerate a specific cell: delete both the content cell and its signature, then re-run the pipeline.\n",
    "\n",
    "Structure:\n",
    "- **Context**: Section breadcrumb from source\n",
    "- **Chunk**: Position in sequence\n",
    "- **Facts**: Bulleted list of extracted factual statements\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_yaml))\n",
    "    \n",
    "    # Cell 1: Entity registry (raw cell)\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    # Note: Actual fact cells are added by the pipeline during processing\n",
    "    # Each will be: markdown content cell + raw signature cell\n",
    "    \n",
    "    # Write notebook\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(nb, f)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate facts notebook (only if doesn't exist - preserve previous run's content)\n",
    "facts_path = os.path.join(RUN_OUTPUT_DIR, \"facts.ipynb\")\n",
    "if not os.path.exists(facts_path):\n",
    "    generate_facts_notebook(contextual_chunks, provenance, registry, LLM_CONFIG, \n",
    "                           \"chunks.ipynb\", facts_path)\n",
    "    print(f\"Generated: {facts_path}\")\n",
    "else:\n",
    "    print(f\"Using existing: {facts_path} (from previous run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256713fd",
   "metadata": {},
   "source": [
    "## Generate RDF Notebook (Structure Only)\n",
    "\n",
    "Creates the RDF notebook with placeholders. Actual content is generated after facts extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7ae750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created RDF notebook: data/albert_einstein/20251218_202954/rdf.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_rdf_notebook_header(provenance: dict, registry: EntityRegistry,\n",
    "                                  llm_config: dict, prompt_template: str, prefixes: str) -> nbformat.NotebookNode:\n",
    "    \"\"\"Generate just the header cells for RDF notebook.\"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    formatted_prefixes = prefixes.format(source_url=provenance['source_url'])\n",
    "    provenance_md = f\"\"\"# RDF Triples: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "source_notebook: facts.ipynb\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "llm_provider: {llm_config['provider']}\n",
    "llm_model: {llm_config['model']}\n",
    "llm_temperature: {llm_config['temperature']}\n",
    "rdf_format: Turtle\n",
    "```\n",
    "\n",
    "## RDF Prefixes\n",
    "\n",
    "```turtle\n",
    "{formatted_prefixes}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "This notebook contains RDF triples in Turtle format, one cell per source facts cell.\n",
    "Each content cell is followed by a signature cell with CID provenance.\n",
    "\n",
    "To regenerate a specific cell: delete both the content cell and its signature, then re-run the pipeline.\n",
    "\n",
    "## Prompt Template\n",
    "\n",
    "```\n",
    "{prompt_template}\n",
    "```\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_md))\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    return nb\n",
    "\n",
    "# Create initial RDF notebook (only if doesn't exist - preserve previous run's content)\n",
    "rdf_path = os.path.join(RUN_OUTPUT_DIR, \"rdf.ipynb\")\n",
    "\n",
    "if not os.path.exists(rdf_path):\n",
    "    rdf_nb = generate_rdf_notebook_header(provenance, registry, LLM_CONFIG, RDF_GENERATION_PROMPT, RDF_PREFIXES)\n",
    "    with open(rdf_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(rdf_nb, f)\n",
    "    print(f\"Created RDF notebook: {rdf_path}\")\n",
    "else:\n",
    "    print(f\"Using existing: {rdf_path} (from previous run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696afdf",
   "metadata": {},
   "source": [
    "## Process Chunks → Extract Facts\n",
    "\n",
    "Run the LLM on each chunk to extract factual statements and update the facts notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2350ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks notebook...\n",
      "Found 63 chunks with CIDs\n",
      "Found 0 existing fact signatures\n",
      "HTTP timeout: 60s per request\n",
      "--------------------------------------------------\n",
      "  Chunk 1: + Generating\n",
      "    Context: Albert Einstein > Introduction\n",
      "    Input: Albert Einstein (14 March 1879 – 18 April 1955) was a German-born theoretical ph...\n",
      "    [Calling LLM...] ✓ (2078 chars, 6.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 2: + Generating\n",
      "    Context: Albert Einstein > Introduction\n",
      "    Input: In 1905, sometimes described as his annus mirabilis (miracle year), he published...\n",
      "    [Calling LLM...] ✓ (1715 chars, 4.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 3: + Generating\n",
      "    Context: Albert Einstein > Life and career\n",
      "    Input: == Life and career ==   === Childhood, youth and education ===  Albert Einstein ...\n",
      "    [Calling LLM...] ✓ (1116 chars, 3.5s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 4: + Generating\n",
      "    Context: Albert Einstein > Life and career > Childhood, youth and education\n",
      "    Input: In 1894, Hermann and Jakob's company tendered for a contract to install electric...\n",
      "    [Calling LLM...] ✓ (1075 chars, 3.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 5: + Generating\n",
      "    Context: Albert Einstein > Life and career > Childhood, youth and education\n",
      "    Input: Einstein excelled at physics and mathematics from an early age, and soon acquire...\n",
      "    [Calling LLM...] ✓ (1092 chars, 2.8s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 6: + Generating\n",
      "    Context: Albert Einstein > Life and career > Childhood, youth and education\n",
      "    Input: At thirteen, when his range of enthusiasms had broadened to include music and ph...\n",
      "    [Calling LLM...] ✓ (1039 chars, 3.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 7: + Generating\n",
      "    Context: Albert Einstein > Life and career > Childhood, youth and education\n",
      "    Input: In January 1896, with his father's approval, Einstein renounced his citizenship ...\n",
      "    [Calling LLM...] ✓ (1505 chars, 4.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 8: + Generating\n",
      "    Context: Albert Einstein > Life and career > Marriages, relationships and children\n",
      "    Input: === Marriages, relationships and children ===  Correspondence between Einstein a...\n",
      "    [Calling LLM...] ✓ (1627 chars, 5.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 9: + Generating\n",
      "    Context: Albert Einstein > Life and career > Marriages, relationships and children\n",
      "    Input: A volume of Einstein's letters released by Hebrew University of Jerusalem in 200...\n",
      "    [Calling LLM...] ✓ (1609 chars, 4.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 10: + Generating\n",
      "    Context: Albert Einstein > Life and career > Assistant at the Swiss Patent Office (1902–1909)\n",
      "    Input: === Assistant at the Swiss Patent Office (1902–1909) ===  Einstein graduated fro...\n",
      "    [Calling LLM...] ✓ (1513 chars, 4.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 11: + Generating\n",
      "    Context: Albert Einstein > Life and career > First scientific papers (1900–1905)\n",
      "    Input: === First scientific papers (1900–1905) ===  Einstein's first paper, \"Folgerunge...\n",
      "    [Calling LLM...] ✓ (1534 chars, 5.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 12: + Generating\n",
      "    Context: Albert Einstein > Life and career > Academic career in Europe (1908–1933)\n",
      "    Input: From 30 October to 3 November 1911, Einstein attended the first Solvay Conferenc...\n",
      "    [Calling LLM...] ✓ (1531 chars, 4.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 13: + Generating\n",
      "    Context: Albert Einstein > Life and career > Academic career in Europe (1908–1933)\n",
      "    Input: The outbreak of the First World War in July 1914 marked the beginning of Einstei...\n",
      "    [Calling LLM...] ✓ (946 chars, 3.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 14: + Generating\n",
      "    Context: Albert Einstein > Life and career > Academic career in Europe (1908–1933)\n",
      "    Input: Einstein resigned from the Prussian Academy in March 1933. His accomplishments i...\n",
      "    [Calling LLM...] ✓ (399 chars, 1.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 15: + Generating\n",
      "    Context: Albert Einstein > Life and career > Putting general relativity to the test (1919)\n",
      "    Input: === Putting general relativity to the test (1919) ===  In 1907, Einstein reached...\n",
      "    [Calling LLM...] ✓ (1298 chars, 3.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 16: + Generating\n",
      "    Context: Albert Einstein > Life and career > Coming to terms with fame (1921–1923)\n",
      "    Input: With Eddington's eclipse observations widely reported not just in academic journ...\n",
      "    [Calling LLM...] ✓ (1140 chars, 3.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 17: + Generating\n",
      "    Context: Albert Einstein > Life and career > Coming to terms with fame (1921–1923)\n",
      "    Input: In 1922, Einstein's travels were to the old world rather than the new. He devote...\n",
      "    [Calling LLM...] ✓ (1422 chars, 3.8s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 18: + Generating\n",
      "    Context: Albert Einstein > Life and career > Coming to terms with fame (1921–1923)\n",
      "    Input: Einstein's decision to tour the eastern hemisphere in 1922 meant that he was una...\n",
      "    [Calling LLM...] ✓ (745 chars, 2.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 19: + Generating\n",
      "    Context: Albert Einstein > Life and career > Serving the League of Nations (1922–1932)\n",
      "    Input: === Serving the League of Nations (1922–1932) ===  From 1922 until 1932, with th...\n",
      "    [Calling LLM...] ✓ (1570 chars, 4.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 20: + Generating\n",
      "    Context: Albert Einstein > Life and career > Touring the US (1930–1931)\n",
      "    Input: === Touring the US (1930–1931) ===  In December 1930, Einstein began another sig...\n",
      "    [Calling LLM...] ✓ (1319 chars, 3.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 21: + Generating\n",
      "    Context: Albert Einstein > Life and career > Touring the US (1930–1931)\n",
      "    Input: Einstein next traveled to California, where he met Caltech president and Nobel l...\n",
      "    [Calling LLM...] ✓ (1936 chars, 5.3s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 22: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933)\n",
      "    Input: === Emigration to the US (1933) ===  In February 1933, while on a visit to the U...\n",
      "    [Calling LLM...] ✓ (1285 chars, 3.8s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 23: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > Refugee status\n",
      "    Input: In April 1933, Einstein discovered that the new German government had passed law...\n",
      "    [Calling LLM...] ✓ (1230 chars, 3.3s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 24: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > Refugee status\n",
      "    Input: Einstein was now without a permanent home, unsure where he would live and work, ...\n",
      "    [Calling LLM...] ✓ (938 chars, 3.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 25: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > Refugee status\n",
      "    Input: Locker-Lampson took Einstein to meet Winston Churchill at his home, and later, A...\n",
      "    [Calling LLM...] ✓ (1286 chars, 3.3s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 26: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > Resident scholar at the Institute for Advanced Study\n",
      "    Input: ==== Resident scholar at the Institute for Advanced Study ====  On 3 October 193...\n",
      "    [Calling LLM...] ✓ (1467 chars, 4.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 27: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > World War II and the Manhattan Project\n",
      "    Input: In 1939, a group of Hungarian scientists that included émigré physicist Leó Szil...\n",
      "    [Calling LLM...] ✓ (1617 chars, 4.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 28: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > World War II and the Manhattan Project\n",
      "    Input: For Einstein, \"war was a disease ... [and] he called for resistance to war.\" By ...\n",
      "    [Calling LLM...] ✓ (1191 chars, 3.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 29: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > US citizenship\n",
      "    Input: ==== US citizenship ====...\n",
      "    [Calling LLM...] ✓ (504 chars, 1.5s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 30: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > US citizenship\n",
      "    Input: Einstein became an American citizen in 1940. Not long after settling into his ca...\n",
      "    [Calling LLM...] ✓ (651 chars, 2.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 31: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > US citizenship\n",
      "    Input: In 1946, Einstein visited Lincoln University in Pennsylvania, a historically bla...\n",
      "    [Calling LLM...] ✓ (1109 chars, 2.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 32: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views\n",
      "    Input: === Personal views ===   ==== Political views ====  In 1918, Einstein was one of...\n",
      "    [Calling LLM...] ✓ (661 chars, 2.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 33: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Political views\n",
      "    Input: In Lenin I honor a man, who in total sacrifice of his own person has committed h...\n",
      "    [Calling LLM...] ✓ (1316 chars, 3.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 34: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Relationship with Zionism\n",
      "    Input: ==== Relationship with Zionism ====  Einstein was a figurehead leader in the est...\n",
      "    [Calling LLM...] ✓ (1284 chars, 3.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 35: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Religious and philosophical views\n",
      "    Input: ==== Religious and philosophical views ====  Per Lee Smolin, \"I believe what all...\n",
      "    [Calling LLM...] ✓ (1227 chars, 3.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 36: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Religious and philosophical views\n",
      "    Input: The word God is for me nothing more than the expression and product of human wea...\n",
      "    [Calling LLM...] ✓ (1181 chars, 3.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 37: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Love of music\n",
      "    Input: If I were not a physicist, I would probably be a musician. I often think in musi...\n",
      "    [Calling LLM...] ✓ (1471 chars, 4.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 38: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Love of music\n",
      "    Input: Music took on a pivotal and permanent role in Einstein's life from that period o...\n",
      "    [Calling LLM...] ✓ (1314 chars, 3.8s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 39: + Generating\n",
      "    Context: Albert Einstein > Life and career > Death\n",
      "    Input: === Death === On 17 April 1955, Einstein experienced internal bleeding caused by...\n",
      "    [Calling LLM...] ✓ (1141 chars, 3.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 40: + Generating\n",
      "    Context: Albert Einstein > Scientific career\n",
      "    Input: == Scientific career == Throughout his life, Einstein published hundreds of book...\n",
      "    [Calling LLM...] ✓ (951 chars, 3.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 41: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Statistical mechanics > Theory of critical opalescence\n",
      "    Input: ==== Theory of critical opalescence ====  Einstein returned to the problem of th...\n",
      "    [Calling LLM...] ✓ (1548 chars, 4.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 42: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Special relativity\n",
      "    Input: === Special relativity ===  Einstein's \"Zur Elektrodynamik bewegter Körper\" (\"On...\n",
      "    [Calling LLM...] ✓ (1437 chars, 4.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 43: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity\n",
      "    Input: === General relativity ===   ==== General relativity and the equivalence princip...\n",
      "    [Calling LLM...] ✓ (1224 chars, 3.5s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 44: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Gravitational waves\n",
      "    Input: ==== Gravitational waves ==== In 1916, Einstein predicted gravitational waves, r...\n",
      "    [Calling LLM...] ✓ (1554 chars, 4.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 45: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Physical cosmology\n",
      "    Input: In 1917, Einstein applied the general theory of relativity to the structure of t...\n",
      "    [Calling LLM...] ✓ (1450 chars, 3.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 46: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Physical cosmology\n",
      "    Input: In late 2013, a team led by the Irish physicist Cormac O'Raifeartaigh discovered...\n",
      "    [Calling LLM...] ✓ (1047 chars, 2.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 47: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Energy momentum pseudotensor\n",
      "    Input: ==== Energy momentum pseudotensor ====  General relativity includes a dynamical ...\n",
      "    [Calling LLM...] ✓ (1105 chars, 2.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 48: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Einstein–Cartan theory\n",
      "    Input: ==== Einstein–Cartan theory ====  In order to incorporate spinning point particl...\n",
      "    [Calling LLM...] ✓ (405 chars, 1.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 49: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Equations of motion\n",
      "    Input: ==== Equations of motion ====  In general relativity, gravitational force is rei...\n",
      "    [Calling LLM...] ✓ (1724 chars, 4.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 50: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Old quantum theory\n",
      "    Input: === Old quantum theory ===   ==== Photons and energy quanta ====  In a 1905 pape...\n",
      "    [Calling LLM...] ✓ (1180 chars, 3.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 51: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Old quantum theory > Bose–Einstein statistics\n",
      "    Input: ==== Bose–Einstein statistics ====  In 1924, Einstein received a description of ...\n",
      "    [Calling LLM...] ✓ (1693 chars, 4.8s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 52: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Old quantum theory > Zero-point energy\n",
      "    Input: ==== Zero-point energy ==== In a series of works completed from 1911 to 1913, Pl...\n",
      "    [Calling LLM...] ✓ (871 chars, 2.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 53: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Quantum mechanics\n",
      "    Input: === Quantum mechanics ===   ==== Einstein's objections to quantum mechanics ====...\n",
      "    [Calling LLM...] ✓ (989 chars, 2.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 54: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Quantum mechanics > Einstein–Podolsky–Rosen paradox\n",
      "    Input: Einstein never fully accepted quantum mechanics. While he recognized that it mad...\n",
      "    [Calling LLM...] ✓ (1237 chars, 3.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 55: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Quantum mechanics > Einstein–Podolsky–Rosen paradox\n",
      "    Input: A more famous version of this argument came in 1935, when Einstein published a p...\n",
      "    [Calling LLM...] ✓ (1574 chars, 4.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 56: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Quantum mechanics > Einstein–Podolsky–Rosen paradox\n",
      "    Input: In 1964, John Stewart Bell carried the analysis of quantum entanglement much fur...\n",
      "    [Calling LLM...] ✓ (1131 chars, 2.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 57: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Unified field theory\n",
      "    Input: === Unified field theory ===  Encouraged by his success with general relativity,...\n",
      "    [Calling LLM...] ✓ (1148 chars, 2.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 58: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Collaboration with other scientists > Einstein–de Haas experiment\n",
      "    Input: ==== Einstein–de Haas experiment ====  In 1908, Owen Willans Richardson predicte...\n",
      "    [Calling LLM...] ✓ (1605 chars, 4.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 59: + Generating\n",
      "    Context: Albert Einstein > Legacy\n",
      "    Input: == Legacy ==   === Non-scientific ===  While traveling, Einstein wrote daily to ...\n",
      "    [Calling LLM...] ✓ (1934 chars, 5.5s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 60: + Generating\n",
      "    Context: Albert Einstein > Legacy > Scientific\n",
      "    Input: === Scientific === In 1999, a survey of the top 100 physicists voted for Einstei...\n",
      "    [Calling LLM...] ✓ (1103 chars, 3.5s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 61: + Generating\n",
      "    Context: Albert Einstein > In popular culture\n",
      "    Input: == In popular culture ==  Einstein became one of the most famous scientific cele...\n",
      "    [Calling LLM...] ✓ (1536 chars, 4.3s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 62: + Generating\n",
      "    Context: Albert Einstein > Publications\n",
      "    Input: == Publications ==   === Scientific ===   === Popular ===   === Political === Ei...\n",
      "    [Calling LLM...] ✓ (1131 chars, 3.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Chunk 63: + Generating\n",
      "    Context: Albert Einstein > External links > Archival materials collections\n",
      "    Input: === Archival materials collections === Albert Einstein Historical Letters, Docum...\n",
      "    [Calling LLM...] ✓ (1201 chars, 3.1s)\n",
      "    [Saving notebook...] saved\n",
      "--------------------------------------------------\n",
      "Facts extraction complete:\n",
      "  - 63 generated\n",
      "  - 0 skipped (up-to-date)\n",
      "  - 0 errors/timeouts\n"
     ]
    }
   ],
   "source": [
    "# Create LLM client with timeout for facts extraction\n",
    "facts_llm = ChatOpenAI(\n",
    "    model=LLM_CONFIG[\"model\"],\n",
    "    temperature=LLM_CONFIG[\"temperature\"],\n",
    "    base_url=LLM_CONFIG[\"base_url\"],\n",
    "    api_key=\"lm-studio\",\n",
    "    timeout=CELL_TIMEOUT_SECONDS,  # HTTP timeout in seconds\n",
    "    max_retries=0,  # Don't retry on timeout\n",
    ")\n",
    "\n",
    "# Create facts extraction prompt\n",
    "facts_prompt = ChatPromptTemplate.from_template(\"\"\"You are an expert at extracting factual information from text.\n",
    "\n",
    "Given text from a Wikipedia article, extract simple English statements that capture the consensus factual information. Each statement should:\n",
    "- Be a single, clear sentence\n",
    "- Contain one main fact or relationship\n",
    "- Use the full name of entities on first mention\n",
    "- Be verifiable from the source text\n",
    "- Avoid opinions, interpretations, or hedged language\n",
    "\n",
    "Source: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Known entities (use consistent names):\n",
    "{known_entities}\n",
    "\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\n",
    "Extract factual statements as a bulleted list:\"\"\")\n",
    "\n",
    "facts_chain = facts_prompt | facts_llm\n",
    "\n",
    "def get_known_entities_text(registry: EntityRegistry) -> str:\n",
    "    \"\"\"Format known entities for prompt context.\"\"\"\n",
    "    lines = []\n",
    "    for entity in registry.entities.values():\n",
    "        lines.append(f\"- {entity['label']} ({entity['type']})\")\n",
    "    return \"\\n\".join(lines) if lines else \"None yet\"\n",
    "\n",
    "# Read chunks notebook to get source content and CIDs\n",
    "log_progress(\"Reading chunks notebook...\")\n",
    "chunks_nb = nbformat.read(chunks_path, as_version=4)\n",
    "chunk_signatures = extract_signatures(chunks_nb)\n",
    "\n",
    "# Build list of chunk content with CIDs\n",
    "# Chunks notebook structure: [provenance, registry, chunk1, sig1, chunk2, sig2, ...]\n",
    "chunk_data = []\n",
    "cell_idx = 2  # Skip provenance and registry\n",
    "while cell_idx < len(chunks_nb.cells):\n",
    "    cell = chunks_nb.cells[cell_idx]\n",
    "    if cell.cell_type == 'markdown':\n",
    "        content = cell.source\n",
    "        # Get corresponding signature (next cell)\n",
    "        sig = None\n",
    "        if cell_idx + 1 < len(chunks_nb.cells):\n",
    "            sig = parse_signature(chunks_nb.cells[cell_idx + 1].source)\n",
    "        \n",
    "        # Extract breadcrumb\n",
    "        context_match = re.search(r'\\*\\*Context:\\*\\*\\s*(.+)', content)\n",
    "        breadcrumb = context_match.group(1) if context_match else \"Unknown\"\n",
    "        \n",
    "        # Extract chunk text (after ---)\n",
    "        parts = content.split(\"---\\n\", 1)\n",
    "        chunk_text = parts[1].strip() if len(parts) > 1 else content\n",
    "        \n",
    "        chunk_data.append({\n",
    "            \"cell_num\": sig[\"cell\"] if sig else len(chunk_data) + 1,\n",
    "            \"content\": content,\n",
    "            \"chunk_text\": chunk_text,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"cid\": sig[\"cid\"] if sig else compute_cid(content),\n",
    "        })\n",
    "        cell_idx += 2  # Skip content and signature\n",
    "    else:\n",
    "        cell_idx += 1\n",
    "\n",
    "log_progress(f\"Found {len(chunk_data)} chunks with CIDs\")\n",
    "\n",
    "# Read existing facts notebook and extract signatures\n",
    "facts_nb = nbformat.read(facts_path, as_version=4)\n",
    "facts_signatures = extract_signatures(facts_nb)\n",
    "\n",
    "log_progress(f\"Found {len(facts_signatures)} existing fact signatures\")\n",
    "log_progress(f\"HTTP timeout: {CELL_TIMEOUT_SECONDS}s per request\")\n",
    "log_progress(\"-\" * 50)\n",
    "\n",
    "# Process each chunk\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for chunk in chunk_data:\n",
    "    cell_num = chunk[\"cell_num\"]\n",
    "    source_cid = chunk[\"cid\"]\n",
    "    \n",
    "    # Check if we already have up-to-date facts for this chunk\n",
    "    existing_sig = facts_signatures.get(cell_num)\n",
    "    if existing_sig and existing_sig[\"from_cid\"] == source_cid:\n",
    "        log_progress(f\"  Chunk {cell_num}: ⊘ Up-to-date (CID match), skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Need to generate (or regenerate) this cell\n",
    "    status = \"↻ Regenerating\" if existing_sig else \"+ Generating\"\n",
    "    chunk_preview = chunk[\"chunk_text\"][:80].replace('\\n', ' ')\n",
    "    log_progress(f\"  Chunk {cell_num}: {status}\")\n",
    "    log_progress(f\"    Context: {chunk['breadcrumb']}\")\n",
    "    log_progress(f\"    Input: {chunk_preview}...\")\n",
    "    log_progress(f\"    [Calling LLM...]\", end=\" \")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Call LLM to extract facts\n",
    "    try:\n",
    "        result = facts_chain.invoke({\n",
    "            \"source_url\": provenance[\"source_url\"],\n",
    "            \"breadcrumb\": chunk[\"breadcrumb\"],\n",
    "            \"known_entities\": get_known_entities_text(registry),\n",
    "            \"chunk_text\": chunk[\"chunk_text\"],\n",
    "        })\n",
    "        facts_content = result.content\n",
    "        elapsed = time.time() - start_time\n",
    "        log_progress(f\"✓ ({len(facts_content)} chars, {elapsed:.1f}s)\")\n",
    "        processed_count += 1\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)[:100]\n",
    "        facts_content = f\"# Error: {error_type}: {e}\"\n",
    "        log_progress(f\"✗ {error_type} after {elapsed:.1f}s\")\n",
    "        log_progress(f\"    Error: {error_msg}\")\n",
    "        error_count += 1\n",
    "    \n",
    "    # Build the facts cell content\n",
    "    facts_cell_content = f\"\"\"**Context:** {chunk['breadcrumb']}\n",
    "**Chunk:** {cell_num} of {len(chunk_data)}\n",
    "\n",
    "---\n",
    "\n",
    "{facts_content}\n",
    "\"\"\"\n",
    "    facts_cid = compute_cid(facts_cell_content)\n",
    "    signature = make_signature(cell_num, \"facts\", facts_cid, source_cid)\n",
    "    \n",
    "    # Find where to insert/update in the notebook\n",
    "    # If there's an existing signature for this cell, find and remove old content+sig\n",
    "    if existing_sig:\n",
    "        # Find and remove the old cells\n",
    "        new_cells = [facts_nb.cells[0], facts_nb.cells[1]]  # Keep header\n",
    "        i = 2\n",
    "        while i < len(facts_nb.cells):\n",
    "            cell = facts_nb.cells[i]\n",
    "            if cell.cell_type == 'raw':\n",
    "                sig = parse_signature(cell.source)\n",
    "                if sig and sig[\"cell\"] == cell_num:\n",
    "                    # Skip this signature and its preceding content cell\n",
    "                    i += 1\n",
    "                    continue\n",
    "            # Check if this is content for the cell we're replacing\n",
    "            if i > 0 and i + 1 < len(facts_nb.cells):\n",
    "                next_sig = parse_signature(facts_nb.cells[i + 1].source) if facts_nb.cells[i + 1].cell_type == 'raw' else None\n",
    "                if next_sig and next_sig[\"cell\"] == cell_num:\n",
    "                    i += 2  # Skip content and signature\n",
    "                    continue\n",
    "            new_cells.append(cell)\n",
    "            i += 1\n",
    "        facts_nb.cells = new_cells\n",
    "    \n",
    "    # Append new content and signature\n",
    "    facts_nb.cells.append(new_markdown_cell(facts_cell_content))\n",
    "    facts_nb.cells.append(new_raw_cell(json.dumps(signature)))\n",
    "    \n",
    "    # Update signatures dict\n",
    "    facts_signatures[cell_num] = signature\n",
    "    \n",
    "    # Save notebook after each cell\n",
    "    log_progress(f\"    [Saving notebook...]\", end=\" \")\n",
    "    with open(facts_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(facts_nb, f)\n",
    "    log_progress(\"saved\")\n",
    "\n",
    "log_progress(\"-\" * 50)\n",
    "log_progress(f\"Facts extraction complete:\")\n",
    "log_progress(f\"  - {processed_count} generated\")\n",
    "log_progress(f\"  - {skipped_count} skipped (up-to-date)\")\n",
    "log_progress(f\"  - {error_count} errors/timeouts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d29b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts notebook: data/albert_einstein/20251218_202954/facts.ipynb\n",
      "  - Updated incrementally during processing\n",
      "  - Ready for review/editing before RDF generation\n"
     ]
    }
   ],
   "source": [
    "# Summary of facts extraction (notebook already updated incrementally)\n",
    "print(f\"Facts notebook: {facts_path}\")\n",
    "print(f\"  - Updated incrementally during processing\")\n",
    "print(f\"  - Ready for review/editing before RDF generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de7f1f",
   "metadata": {},
   "source": [
    "## Process Facts → Generate RDF\n",
    "\n",
    "Run the LLM on each facts cell to generate RDF triples and update the RDF notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afd0797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded schema matcher with 1 vocabularies\n",
      "Reading facts notebook...\n",
      "Found 63 facts cells with CIDs\n",
      "Found 0 existing RDF signatures\n",
      "HTTP timeout: 60s per request\n",
      "Max iterations: 150 per cell\n",
      "--------------------------------------------------\n",
      "  Facts 1: + Generating\n",
      "    Context: Albert Einstein > Introduction\n",
      "    Input: • Albert Einstein was born on 14 March 1879 in the German Empire. • Albert Einst...\n",
      "    [Calling LLM with tools...] ✓ (37 triples, 5 iters, 51.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 2: + Generating\n",
      "    Context: Albert Einstein > Introduction\n",
      "    Input: • In 1905, Albert Einstein published four groundbreaking papers that outlined a ...\n",
      "    [Calling LLM with tools...] ✓ (34 triples, 5 iters, 43.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 3: + Generating\n",
      "    Context: Albert Einstein > Life and career\n",
      "    Input: • Albert Einstein was born in Ulm, in the Kingdom of Württemberg in the German E...\n",
      "    [Calling LLM with tools...] ✓ (24 triples, 4 iters, 22.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 4: + Generating\n",
      "    Context: Albert Einstein > Life and career > Childhood, youth and education\n",
      "    Input: • In 1894, Hermann and Jakob's company tendered for a contract to install electr...\n",
      "    [Calling LLM with tools...] ✓ (98 triples, 23 iters, 80.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 5: + Generating\n",
      "    Context: Albert Einstein > Life and career > Childhood, youth and education\n",
      "    Input: • Albert Einstein excelled at physics and mathematics from an early age. • Alber...\n",
      "    [Calling LLM with tools...] ✓ (11 triples, 6 iters, 29.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 6: + Generating\n",
      "    Context: Albert Einstein > Life and career > Childhood, youth and education\n",
      "    Input: • Albert Einstein was thirteen years old when he was introduced to Kant's Critiq...\n",
      "    [Calling LLM with tools...] ✓ (14 triples, 7 iters, 28.3s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 7: + Generating\n",
      "    Context: Albert Einstein > Life and career > Childhood, youth and education\n",
      "    Input: • In January 1896, Albert Einstein renounced his citizenship of the German Kingd...\n",
      "    [Calling LLM with tools...] ✓ (34 triples, 7 iters, 46.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 8: + Generating\n",
      "    Context: Albert Einstein > Life and career > Marriages, relationships and children\n",
      "    Input: - Albert Einstein and Mileva Marić had a daughter named Lieserl in early 1902. -...\n",
      "    [Calling LLM with tools...] ✓ (30 triples, 7 iters, 49.8s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 9: + Generating\n",
      "    Context: Albert Einstein > Life and career > Marriages, relationships and children\n",
      "    Input: • Albert Einstein was married to Mileva Marić from 1903 to 1919. • Albert Einste...\n",
      "    [Calling LLM with tools...] ✗ ValidationError after 16.1s\n",
      "    Error: 1 validation error for emit_triples\n",
      "triples\n",
      "  Input should be a valid list [type=list_type, input_va\n",
      "    [Saving notebook...] saved\n",
      "  Facts 10: + Generating\n",
      "    Context: Albert Einstein > Life and career > Assistant at the Swiss Patent Office (1902–1909)\n",
      "    Input: • Albert Einstein graduated from the federal polytechnic school in 1900 and was ...\n",
      "    [Calling LLM with tools...] ✓ (21 triples, 7 iters, 40.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 11: + Generating\n",
      "    Context: Albert Einstein > Life and career > First scientific papers (1900–1905)\n",
      "    Input: - Albert Einstein's first scientific paper, titled \"Folgerungen aus den Capillar...\n",
      "    [Calling LLM with tools...] ✓ (37 triples, 8 iters, 59.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 12: + Generating\n",
      "    Context: Albert Einstein > Life and career > Academic career in Europe (1908–1933)\n",
      "    Input: • Albert Einstein attended the first Solvay Conference on Physics from 30 Octobe...\n",
      "    [Calling LLM with tools...] ✓ (28 triples, 7 iters, 42.8s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 13: + Generating\n",
      "    Context: Albert Einstein > Life and career > Academic career in Europe (1908–1933)\n",
      "    Input: - Albert Einstein was elected president of the German Physical Society in 1916. ...\n",
      "    [Calling LLM with tools...] ✓ (81 triples, 33 iters, 90.8s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 14: + Generating\n",
      "    Context: Albert Einstein > Life and career > Academic career in Europe (1908–1933)\n",
      "    Input: • Albert Einstein resigned from the Prussian Academy in March 1933. • Albert Ein...\n",
      "    [Calling LLM with tools...] ✓ (26 triples, 13 iters, 32.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 15: + Generating\n",
      "    Context: Albert Einstein > Life and career > Putting general relativity to the test (1919)\n",
      "    Input: • In 1907, Albert Einstein formulated the equivalence principle, which states th...\n",
      "    [Calling LLM with tools...] ✓ (51 triples, 10 iters, 58.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 16: + Generating\n",
      "    Context: Albert Einstein > Life and career > Coming to terms with fame (1921–1923)\n",
      "    Input: • Albert Einstein became \"perhaps the world's first celebrity scientist\" after E...\n",
      "    [Calling LLM with tools...] ✓ (48 triples, 11 iters, 82.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 17: + Generating\n",
      "    Context: Albert Einstein > Life and career > Coming to terms with fame (1921–1923)\n",
      "    Input: • In 1922, Albert Einstein undertook a six-month tour of Asia. • During his Asia...\n",
      "    [Calling LLM with tools...] ✓ (87 triples, 8 iters, 71.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 18: + Generating\n",
      "    Context: Albert Einstein > Life and career > Coming to terms with fame (1921–1923)\n",
      "    Input: • Albert Einstein decided to tour the eastern hemisphere in 1922 • Einstein was ...\n",
      "    [Calling LLM with tools...] ✓ (21 triples, 7 iters, 39.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 19: + Generating\n",
      "    Context: Albert Einstein > Life and career > Serving the League of Nations (1922–1932)\n",
      "    Input: • Albert Einstein was a member of the International Committee on Intellectual Co...\n",
      "    [Calling LLM with tools...] ✓ (21 triples, 6 iters, 33.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 20: + Generating\n",
      "    Context: Albert Einstein > Life and career > Touring the US (1930–1931)\n",
      "    Input: • Albert Einstein began a significant sojourn in the United States in December 1...\n",
      "    [Calling LLM with tools...] ✓ (24 triples, 6 iters, 39.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 21: + Generating\n",
      "    Context: Albert Einstein > Life and career > Touring the US (1930–1931)\n",
      "    Input: • Albert Einstein traveled to California in 1930-1931. • In California, Albert E...\n",
      "    [Calling LLM with tools...] ✓ (48 triples, 4 iters, 70.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 22: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933)\n",
      "    Input: • Albert Einstein knew he could not return to Germany in February 1933 due to th...\n",
      "    [Calling LLM with tools...] ✓ (24 triples, 8 iters, 42.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 23: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > Refugee status\n",
      "    Input: • In April 1933, Albert Einstein discovered that the new German government had p...\n",
      "    [Calling LLM with tools...] ✓ (19 triples, 7 iters, 34.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 24: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > Refugee status\n",
      "    Input: • Albert Einstein was without a permanent home and unsure where he would live an...\n",
      "    [Calling LLM with tools...] ⚠ (740 triples, 150 iters [MAX], 835.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 25: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > Refugee status\n",
      "    Input: • Albert Einstein contacted British leaders including Winston Churchill, Austen ...\n",
      "    [Calling LLM with tools...] ✓ (29 triples, 6 iters, 55.3s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 26: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > Resident scholar at the Institute for Advanced Study\n",
      "    Input: • Albert Einstein delivered a speech on academic freedom at the Royal Albert Hal...\n",
      "    [Calling LLM with tools...] ✓ (26 triples, 6 iters, 40.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 27: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > World War II and the Manhattan Project\n",
      "    Input: • In 1939, a group of Hungarian scientists including physicist Leó Szilárd attem...\n",
      "    [Calling LLM with tools...] ✗ APITimeoutError after 1484.7s\n",
      "    Error: Request timed out.\n",
      "    [Saving notebook...] saved\n",
      "  Facts 28: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > World War II and the Manhattan Project\n",
      "    Input: • Albert Einstein called war a disease and advocated for resistance to war. • Al...\n",
      "    [Calling LLM with tools...] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 331\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# Call LLM with tool support - now returns (summary, triples, iterations)\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     summary, triples, iterations_used \u001b[38;5;241m=\u001b[39m \u001b[43mcall_llm_with_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovenance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbreadcrumb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfacts_item\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbreadcrumb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity_registry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_entity_registry_for_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfacts_item\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacts_text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    338\u001b[0m     triple_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(triples)\n",
      "Cell \u001b[0;32mIn[15], line 204\u001b[0m, in \u001b[0;36mcall_llm_with_tools\u001b[0;34m(prompt_vars, max_iterations)\u001b[0m\n\u001b[1;32m    201\u001b[0m messages \u001b[38;5;241m=\u001b[39m rdf_prompt\u001b[38;5;241m.\u001b[39mformat_messages(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprompt_vars)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iterations):\n\u001b[0;32m--> 204\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrdf_llm_with_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# Check if there are tool calls\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtool_calls\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtool_calls:\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;66;03m# Add assistant message with tool calls\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:5548\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5541\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5546\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5547\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5549\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5550\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5551\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    392\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[1;32m    393\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    396\u001b[0m         cast(\n\u001b[1;32m    397\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    408\u001b[0m         )\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m    409\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1115\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1116\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:927\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    926\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 927\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m         )\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1221\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1225\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1375\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[1;32m   1369\u001b[0m             response,\n\u001b[1;32m   1370\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[1;32m   1371\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1372\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[1;32m   1373\u001b[0m         )\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m   1190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_retention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/openai/_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 982\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    988\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load schema matcher with pre-built vocabulary index\n",
    "from schema_matcher import SchemaMatcher\n",
    "from langchain_core.tools import tool\n",
    "from typing import List\n",
    "\n",
    "VOCAB_CACHE_DIR = \"data/vocab_cache\"\n",
    "schema_matcher = SchemaMatcher.load(VOCAB_CACHE_DIR, embed_base_url=LLM_CONFIG[\"base_url\"])\n",
    "log_progress(f\"Loaded schema matcher with {len(schema_matcher.vocabularies)} vocabularies\")\n",
    "\n",
    "# Collector for emitted triples (reset per cell)\n",
    "emitted_triples: List[dict] = []\n",
    "\n",
    "# Iteration tracking for statistics\n",
    "iteration_counts: List[int] = []  # Track iterations per cell\n",
    "\n",
    "# === Schema lookup tools ===\n",
    "\n",
    "@tool\n",
    "def find_rdf_class(description: str) -> str:\n",
    "    \"\"\"Find the best schema.org class/type for an entity based on a natural language description.\n",
    "    \n",
    "    Use this when you need to determine the rdf:type of an entity.\n",
    "    Example: find_rdf_class(\"a person who does scientific research\")\n",
    "    \n",
    "    Args:\n",
    "        description: Natural language description of the entity type\n",
    "    \n",
    "    Returns:\n",
    "        Top matching schema.org classes with URIs and descriptions\n",
    "    \"\"\"\n",
    "    results = schema_matcher.find_class(description, top_k=5)\n",
    "    if not results:\n",
    "        return \"No matches found. Use a generic type like schema:Thing\"\n",
    "    \n",
    "    lines = [\"Top matching classes:\"]\n",
    "    for r in results:\n",
    "        lines.append(f\"  {r['prefix']} ({r['score']:.2f})\")\n",
    "        lines.append(f\"    URI: {r['uri']}\")\n",
    "        if r['description']:\n",
    "            lines.append(f\"    Description: {r['description'][:100]}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "@tool\n",
    "def find_rdf_property(description: str, subject_type: str = \"\", object_type: str = \"\") -> str:\n",
    "    \"\"\"Find the best schema.org property/predicate for a relationship.\n",
    "    \n",
    "    Use this when you need to find the right predicate for a triple.\n",
    "    Example: find_rdf_property(\"the date when someone was born\", subject_type=\"Person\")\n",
    "    \n",
    "    Args:\n",
    "        description: Natural language description of the relationship\n",
    "        subject_type: Optional - the type of the subject (e.g., \"Person\", \"Organization\")  \n",
    "        object_type: Optional - the type of the object/value (e.g., \"Date\", \"Place\")\n",
    "    \n",
    "    Returns:\n",
    "        Top matching schema.org properties with URIs, domains, and ranges\n",
    "    \"\"\"\n",
    "    results = schema_matcher.find_property(\n",
    "        description, \n",
    "        subject_type=subject_type or None,\n",
    "        object_type=object_type or None,\n",
    "        top_k=5\n",
    "    )\n",
    "    if not results:\n",
    "        return \"No matches found. Consider using rdfs:label or a descriptive URI fragment.\"\n",
    "    \n",
    "    lines = [\"Top matching properties:\"]\n",
    "    for r in results:\n",
    "        lines.append(f\"  {r['prefix']} ({r['score']:.2f})\")\n",
    "        lines.append(f\"    URI: {r['uri']}\")\n",
    "        if r['domain']:\n",
    "            lines.append(f\"    Domain: {r['domain']}\")\n",
    "        if r['range']:\n",
    "            lines.append(f\"    Range: {r['range']}\")\n",
    "        if r['description']:\n",
    "            lines.append(f\"    Description: {r['description'][:80]}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# === Triple output tools ===\n",
    "\n",
    "@tool\n",
    "def emit_triple(subject: str, predicate: str, object_value: str) -> str:\n",
    "    \"\"\"Emit a single RDF triple. Use this to output each triple you generate.\n",
    "    \n",
    "    Args:\n",
    "        subject: The subject URI (e.g., \"<https://example.org#entity>\") or prefixed (e.g., \"schema:Person\")\n",
    "        predicate: The predicate URI or prefixed term (e.g., \"schema:birthDate\", \"rdf:type\")\n",
    "        object_value: The object - a URI, prefixed term, or literal with datatype \n",
    "                      (e.g., '\"1879-03-14\"^^xsd:date', '\"Albert Einstein\"@en', '<https://...>')\n",
    "    \n",
    "    Returns:\n",
    "        Confirmation message\n",
    "    \"\"\"\n",
    "    emitted_triples.append({\n",
    "        \"subject\": subject,\n",
    "        \"predicate\": predicate,\n",
    "        \"object\": object_value,\n",
    "    })\n",
    "    return f\"Triple recorded: {subject} {predicate} {object_value}\"\n",
    "\n",
    "@tool  \n",
    "def emit_triples(triples: List[dict]) -> str:\n",
    "    \"\"\"Emit multiple RDF triples at once. More efficient than calling emit_triple repeatedly.\n",
    "    \n",
    "    Args:\n",
    "        triples: List of triple dictionaries, each with keys:\n",
    "                 - subject: The subject URI or prefixed term\n",
    "                 - predicate: The predicate URI or prefixed term  \n",
    "                 - object: The object URI, prefixed term, or literal\n",
    "                 \n",
    "    Example:\n",
    "        emit_triples([\n",
    "            {\"subject\": \"<#person_einstein>\", \"predicate\": \"rdf:type\", \"object\": \"schema:Person\"},\n",
    "            {\"subject\": \"<#person_einstein>\", \"predicate\": \"schema:name\", \"object\": '\"Albert Einstein\"'},\n",
    "            {\"subject\": \"<#person_einstein>\", \"predicate\": \"schema:birthDate\", \"object\": '\"1879-03-14\"^^xsd:date'}\n",
    "        ])\n",
    "    \n",
    "    Returns:\n",
    "        Confirmation with count of triples recorded\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for t in triples:\n",
    "        if isinstance(t, dict) and all(k in t for k in (\"subject\", \"predicate\", \"object\")):\n",
    "            emitted_triples.append({\n",
    "                \"subject\": t[\"subject\"],\n",
    "                \"predicate\": t[\"predicate\"],\n",
    "                \"object\": t[\"object\"],\n",
    "            })\n",
    "            count += 1\n",
    "    return f\"Recorded {count} triples\"\n",
    "\n",
    "# Create LLM client with timeout\n",
    "rdf_llm = ChatOpenAI(\n",
    "    model=LLM_CONFIG[\"model\"],\n",
    "    temperature=LLM_CONFIG[\"temperature\"],\n",
    "    base_url=LLM_CONFIG[\"base_url\"],\n",
    "    api_key=\"lm-studio\",  # type: ignore\n",
    "    timeout=CELL_TIMEOUT_SECONDS,\n",
    "    max_retries=0,\n",
    ")\n",
    "\n",
    "# Bind all tools to the LLM\n",
    "rdf_tools = [find_rdf_class, find_rdf_property, emit_triple, emit_triples]\n",
    "rdf_llm_with_tools = rdf_llm.bind_tools(rdf_tools)\n",
    "\n",
    "# Create RDF generation prompt\n",
    "rdf_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert at converting factual statements to RDF triples.\n",
    "\n",
    "You have access to these tools:\n",
    "\n",
    "LOOKUP TOOLS (use first to find correct vocabulary):\n",
    "- find_rdf_class: Find the best schema.org class/type for an entity\n",
    "- find_rdf_property: Find the best predicate for a relationship\n",
    "\n",
    "OUTPUT TOOLS (use to emit your triples):\n",
    "- emit_triple: Output a single triple (subject, predicate, object)\n",
    "- emit_triples: Output multiple triples at once (more efficient)\n",
    "\n",
    "WORKFLOW:\n",
    "1. First use find_rdf_class and find_rdf_property to look up appropriate schema.org terms\n",
    "2. Then use emit_triple or emit_triples to output each RDF triple\n",
    "3. Finally, provide a brief summary noting any quality concerns\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT write Turtle syntax in your response - use the emit tools instead\n",
    "- Use schema.org terms you looked up (e.g., schema:Person, schema:birthDate)\n",
    "- For URIs, use angle brackets: <https://...> or fragment references: <#entity_id>\n",
    "- For literals, use quotes with optional datatype: \"value\"^^xsd:date or \"text\"@en\n",
    "- For prefixed terms as objects, just use the prefix: schema:Person\"\"\"),\n",
    "    (\"human\", \"\"\"Convert these factual statements to RDF triples.\n",
    "\n",
    "Source: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Entity Registry (use these URIs for known entities):\n",
    "{entity_registry}\n",
    "\n",
    "Factual statements to convert:\n",
    "{facts}\n",
    "\n",
    "Use the lookup tools to find schema.org terms, then emit your triples. End with a brief summary.\"\"\"),\n",
    "])\n",
    "\n",
    "def triples_to_turtle(triples: List[dict]) -> str:\n",
    "    \"\"\"Convert collected triples to Turtle format.\"\"\"\n",
    "    if not triples:\n",
    "        return \"# No triples emitted\"\n",
    "    \n",
    "    lines = []\n",
    "    for t in triples:\n",
    "        lines.append(f\"{t['subject']} {t['predicate']} {t['object']} .\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Tool-calling loop that collects emitted triples\n",
    "def call_llm_with_tools(prompt_vars: dict, max_iterations: int = MAX_ITERATIONS) -> tuple[str, List[dict], int]:\n",
    "    \"\"\"Call LLM with tool support. Returns (summary_response, collected_triples, iterations_used).\"\"\"\n",
    "    global emitted_triples\n",
    "    emitted_triples = []  # Reset collector\n",
    "    \n",
    "    messages = rdf_prompt.format_messages(**prompt_vars)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        response = rdf_llm_with_tools.invoke(messages)\n",
    "        \n",
    "        # Check if there are tool calls\n",
    "        if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "            # Add assistant message with tool calls\n",
    "            messages.append(response)\n",
    "            \n",
    "            # Execute each tool call\n",
    "            for tool_call in response.tool_calls:\n",
    "                tool_name = tool_call['name']\n",
    "                tool_args = tool_call['args']\n",
    "                \n",
    "                # Find and execute the tool\n",
    "                if tool_name == 'find_rdf_class':\n",
    "                    tool_result = find_rdf_class.invoke(tool_args)\n",
    "                elif tool_name == 'find_rdf_property':\n",
    "                    tool_result = find_rdf_property.invoke(tool_args)\n",
    "                elif tool_name == 'emit_triple':\n",
    "                    tool_result = emit_triple.invoke(tool_args)\n",
    "                elif tool_name == 'emit_triples':\n",
    "                    tool_result = emit_triples.invoke(tool_args)\n",
    "                else:\n",
    "                    tool_result = f\"Unknown tool: {tool_name}\"\n",
    "                \n",
    "                # Add tool result message\n",
    "                from langchain_core.messages import ToolMessage\n",
    "                messages.append(ToolMessage(content=str(tool_result), tool_call_id=tool_call['id']))\n",
    "        else:\n",
    "            # No tool calls, return the summary and collected triples\n",
    "            summary = response.content if hasattr(response, 'content') else str(response)\n",
    "            return summary, list(emitted_triples), iteration + 1\n",
    "    \n",
    "    # Max iterations reached\n",
    "    summary = response.content if hasattr(response, 'content') else \"Max iterations reached\"\n",
    "    return summary, list(emitted_triples), max_iterations\n",
    "\n",
    "def format_entity_registry_for_prompt(registry: EntityRegistry) -> str:\n",
    "    \"\"\"Format registry for RDF prompt.\"\"\"\n",
    "    lines = []\n",
    "    for entity in registry.entities.values():\n",
    "        lines.append(f\"<{entity['uri']}> # {entity['label']} ({entity['type']})\")\n",
    "    return \"\\n\".join(lines) if lines else \"# No entities registered yet\"\n",
    "\n",
    "# Read facts notebook to get source content and CIDs\n",
    "log_progress(\"Reading facts notebook...\")\n",
    "facts_nb = nbformat.read(facts_path, as_version=4)\n",
    "facts_signatures = extract_signatures(facts_nb)\n",
    "\n",
    "# Build list of facts content with CIDs\n",
    "facts_data = []\n",
    "cell_idx = 2  # Skip provenance and registry\n",
    "while cell_idx < len(facts_nb.cells):\n",
    "    cell = facts_nb.cells[cell_idx]\n",
    "    if cell.cell_type == 'markdown':\n",
    "        content = cell.source\n",
    "        # Get corresponding signature (next cell)\n",
    "        sig = None\n",
    "        if cell_idx + 1 < len(facts_nb.cells):\n",
    "            sig = parse_signature(facts_nb.cells[cell_idx + 1].source)\n",
    "        \n",
    "        # Extract breadcrumb\n",
    "        context_match = re.search(r'\\*\\*Context:\\*\\*\\s*(.+)', content)\n",
    "        breadcrumb = context_match.group(1) if context_match else \"Unknown\"\n",
    "        \n",
    "        # Extract facts (after ---)\n",
    "        parts = content.split(\"---\\n\", 1)\n",
    "        facts_text = parts[1].strip() if len(parts) > 1 else content\n",
    "        \n",
    "        facts_data.append({\n",
    "            \"cell_num\": sig[\"cell\"] if sig else len(facts_data) + 1,\n",
    "            \"content\": content,\n",
    "            \"facts_text\": facts_text,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"cid\": sig[\"cid\"] if sig else compute_cid(content),\n",
    "        })\n",
    "        cell_idx += 2  # Skip content and signature\n",
    "    else:\n",
    "        cell_idx += 1\n",
    "\n",
    "log_progress(f\"Found {len(facts_data)} facts cells with CIDs\")\n",
    "\n",
    "# Read existing RDF notebook and extract signatures\n",
    "rdf_nb = nbformat.read(rdf_path, as_version=4)\n",
    "rdf_signatures = extract_signatures(rdf_nb)\n",
    "\n",
    "log_progress(f\"Found {len(rdf_signatures)} existing RDF signatures\")\n",
    "log_progress(f\"HTTP timeout: {CELL_TIMEOUT_SECONDS}s per request\")\n",
    "log_progress(f\"Max iterations: {MAX_ITERATIONS} per cell\")\n",
    "log_progress(\"-\" * 50)\n",
    "\n",
    "# Process each facts cell\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "total_triples = 0\n",
    "iteration_counts = []  # Reset for this run\n",
    "max_iterations_hit = 0  # Count cells that hit the limit\n",
    "\n",
    "for facts_item in facts_data:\n",
    "    cell_num = facts_item[\"cell_num\"]\n",
    "    source_cid = facts_item[\"cid\"]\n",
    "    \n",
    "    # Skip cells that had errors in facts extraction\n",
    "    if facts_item[\"facts_text\"].startswith(\"# Error:\"):\n",
    "        log_progress(f\"  Facts {cell_num}: ⊘ Source had error, skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Check if we already have up-to-date RDF for these facts\n",
    "    existing_sig = rdf_signatures.get(cell_num)\n",
    "    if existing_sig and existing_sig[\"from_cid\"] == source_cid:\n",
    "        log_progress(f\"  Facts {cell_num}: ⊘ Up-to-date (CID match), skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Need to generate (or regenerate) this cell\n",
    "    status = \"↻ Regenerating\" if existing_sig else \"+ Generating\"\n",
    "    facts_preview = facts_item[\"facts_text\"][:80].replace('\\n', ' ')\n",
    "    log_progress(f\"  Facts {cell_num}: {status}\")\n",
    "    log_progress(f\"    Context: {facts_item['breadcrumb']}\")\n",
    "    log_progress(f\"    Input: {facts_preview}...\")\n",
    "    log_progress(f\"    [Calling LLM with tools...]\", end=\" \")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Call LLM with tool support - now returns (summary, triples, iterations)\n",
    "    try:\n",
    "        summary, triples, iterations_used = call_llm_with_tools({\n",
    "            \"source_url\": provenance[\"source_url\"],\n",
    "            \"breadcrumb\": facts_item[\"breadcrumb\"],\n",
    "            \"entity_registry\": format_entity_registry_for_prompt(registry),\n",
    "            \"facts\": facts_item[\"facts_text\"],\n",
    "        })\n",
    "        elapsed = time.time() - start_time\n",
    "        triple_count = len(triples)\n",
    "        total_triples += triple_count\n",
    "        iteration_counts.append(iterations_used)\n",
    "        \n",
    "        # Check if we hit the limit\n",
    "        if iterations_used >= MAX_ITERATIONS:\n",
    "            max_iterations_hit += 1\n",
    "            log_progress(f\"⚠ ({triple_count} triples, {iterations_used} iters [MAX], {elapsed:.1f}s)\")\n",
    "        else:\n",
    "            log_progress(f\"✓ ({triple_count} triples, {iterations_used} iters, {elapsed:.1f}s)\")\n",
    "        \n",
    "        # Convert triples to Turtle\n",
    "        rdf_content = triples_to_turtle(triples)\n",
    "        \n",
    "        # Include summary as comment if it has content\n",
    "        if summary and summary.strip():\n",
    "            # Clean up summary for comment\n",
    "            summary_lines = summary.strip().split('\\n')\n",
    "            summary_comment = '\\n'.join(f\"# {line}\" for line in summary_lines[:5])\n",
    "            rdf_content = f\"{summary_comment}\\n\\n{rdf_content}\"\n",
    "        \n",
    "        processed_count += 1\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)[:100]\n",
    "        rdf_content = f\"# Error: {error_type}: {e}\"\n",
    "        log_progress(f\"✗ {error_type} after {elapsed:.1f}s\")\n",
    "        log_progress(f\"    Error: {error_msg}\")\n",
    "        error_count += 1\n",
    "    \n",
    "    # Build the RDF cell content\n",
    "    rdf_cell_content = f\"\"\"# Context: {facts_item['breadcrumb']}\n",
    "# Cell: {cell_num} of {len(facts_data)}\n",
    "\n",
    "{rdf_content}\n",
    "\"\"\"\n",
    "    rdf_cid = compute_cid(rdf_cell_content)\n",
    "    signature = make_signature(cell_num, \"rdf\", rdf_cid, source_cid)\n",
    "    \n",
    "    # Find where to insert/update in the notebook\n",
    "    if existing_sig:\n",
    "        # Find and remove the old cells\n",
    "        new_cells = [rdf_nb.cells[0], rdf_nb.cells[1]]  # Keep header\n",
    "        i = 2\n",
    "        while i < len(rdf_nb.cells):\n",
    "            cell = rdf_nb.cells[i]\n",
    "            if cell.cell_type == 'raw':\n",
    "                sig = parse_signature(cell.source)\n",
    "                if sig and sig[\"cell\"] == cell_num:\n",
    "                    i += 1\n",
    "                    continue\n",
    "            if i > 0 and i + 1 < len(rdf_nb.cells):\n",
    "                next_sig = parse_signature(rdf_nb.cells[i + 1].source) if rdf_nb.cells[i + 1].cell_type == 'raw' else None\n",
    "                if next_sig and next_sig[\"cell\"] == cell_num:\n",
    "                    i += 2\n",
    "                    continue\n",
    "            new_cells.append(cell)\n",
    "            i += 1\n",
    "        rdf_nb.cells = new_cells\n",
    "    \n",
    "    # Append new content and signature\n",
    "    rdf_nb.cells.append(new_raw_cell(rdf_cell_content))\n",
    "    rdf_nb.cells.append(new_raw_cell(json.dumps(signature)))\n",
    "    \n",
    "    # Update signatures dict\n",
    "    rdf_signatures[cell_num] = signature\n",
    "    \n",
    "    # Save notebook after each cell\n",
    "    log_progress(f\"    [Saving notebook...]\", end=\" \")\n",
    "    with open(rdf_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(rdf_nb, f)\n",
    "    log_progress(\"saved\")\n",
    "\n",
    "log_progress(\"-\" * 50)\n",
    "log_progress(f\"RDF generation complete:\")\n",
    "log_progress(f\"  - {processed_count} cells processed\")\n",
    "log_progress(f\"  - {total_triples} total triples emitted\")\n",
    "log_progress(f\"  - {skipped_count} skipped (up-to-date)\")\n",
    "log_progress(f\"  - {error_count} errors/timeouts\")\n",
    "\n",
    "# Iteration statistics\n",
    "if iteration_counts:\n",
    "    log_progress(f\"\\nIteration statistics (max={MAX_ITERATIONS}):\")\n",
    "    log_progress(f\"  - Min: {min(iteration_counts)}\")\n",
    "    log_progress(f\"  - Max: {max(iteration_counts)}\")\n",
    "    log_progress(f\"  - Mean: {sum(iteration_counts)/len(iteration_counts):.1f}\")\n",
    "    log_progress(f\"  - Hit limit: {max_iterations_hit} cells\")\n",
    "    \n",
    "    # Distribution buckets\n",
    "    buckets = {\"1-5\": 0, \"6-10\": 0, \"11-20\": 0, \"21-50\": 0, \"51-100\": 0, \"100+\": 0}\n",
    "    for count in iteration_counts:\n",
    "        if count <= 5:\n",
    "            buckets[\"1-5\"] += 1\n",
    "        elif count <= 10:\n",
    "            buckets[\"6-10\"] += 1\n",
    "        elif count <= 20:\n",
    "            buckets[\"11-20\"] += 1\n",
    "        elif count <= 50:\n",
    "            buckets[\"21-50\"] += 1\n",
    "        elif count <= 100:\n",
    "            buckets[\"51-100\"] += 1\n",
    "        else:\n",
    "            buckets[\"100+\"] += 1\n",
    "    \n",
    "    log_progress(f\"  - Distribution: {buckets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69a28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF notebook: data/albert_einstein_rdf.ipynb\n",
      "  - Updated incrementally during processing\n",
      "  - Ready for review/editing before final export\n"
     ]
    }
   ],
   "source": [
    "# Summary of RDF generation (notebook already updated incrementally)\n",
    "print(f\"RDF notebook: {rdf_path}\")\n",
    "print(f\"  - Updated incrementally during processing\")\n",
    "print(f\"  - Ready for review/editing before final export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a59d9",
   "metadata": {},
   "source": [
    "## Export Combined RDF\n",
    "\n",
    "Combine all RDF cells into a single Turtle file with prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d061083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported RDF to: data/albert_einstein.ttl\n",
      "  - 57 chunks of triples\n",
      "  - 198405 characters total\n"
     ]
    }
   ],
   "source": [
    "# Combine all RDF into a single Turtle file\n",
    "all_triples = []\n",
    "\n",
    "# Read the updated RDF notebook\n",
    "rdf_nb = nbformat.read(rdf_path, as_version=4)\n",
    "\n",
    "# Collect RDF from all raw cells (skip provenance, registry, and signature cells)\n",
    "for cell in rdf_nb.cells[2:]:\n",
    "    if cell.cell_type == 'raw':\n",
    "        content = cell.source.strip()\n",
    "        \n",
    "        # Skip signature cells (JSON objects)\n",
    "        if content.startswith('{') and '\"cid\"' in content:\n",
    "            continue\n",
    "        \n",
    "        # Skip empty or error-only cells\n",
    "        if not content or content.startswith('# Error:'):\n",
    "            continue\n",
    "        \n",
    "        # Skip comment-only cells\n",
    "        lines = [line for line in content.split('\\n') \n",
    "                 if line.strip() and not line.strip().startswith('#')]\n",
    "        if lines:\n",
    "            all_triples.append(content)\n",
    "\n",
    "# Format prefixes for the Turtle file\n",
    "turtle_prefixes = RDF_PREFIXES.format(source_url=provenance['source_url'])\n",
    "\n",
    "# Build complete Turtle file\n",
    "turtle_output = f\"\"\"# RDF Knowledge Graph: {provenance['article_title']}\n",
    "# Source: {provenance['source_url']}\n",
    "# License: {provenance['license']}\n",
    "# Generated: {datetime.now().isoformat()}\n",
    "\n",
    "{turtle_prefixes}\n",
    "\n",
    "# === Triples ===\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "turtle_output += \"\\n\\n\".join(all_triples)\n",
    "\n",
    "# Save to file with simple filename (timestamp is in directory)\n",
    "turtle_path = os.path.join(RUN_OUTPUT_DIR, \"triples.ttl\")\n",
    "with open(turtle_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(turtle_output)\n",
    "\n",
    "print(f\"Exported RDF to: {turtle_path}\")\n",
    "print(f\"  - {len(all_triples)} chunks of triples\")\n",
    "print(f\"  - {len(turtle_output)} characters total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c4e37",
   "metadata": {},
   "source": [
    "## Save Initial Entity Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d416a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/entity_registry.json\n",
      "\n",
      "Initial entities: 1\n",
      "  - Albert Einstein (Person): https://en.wikipedia.org/wiki/Albert_Einstein#person_albert_einstein\n"
     ]
    }
   ],
   "source": [
    "# Save entity registry to JSON file with simple filename (timestamp is in directory)\n",
    "registry_path = os.path.join(RUN_OUTPUT_DIR, \"registry.json\")\n",
    "with open(registry_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(registry.to_json())\n",
    "\n",
    "print(f\"Saved: {registry_path}\")\n",
    "print(f\"\\nInitial entities: {len(registry.entities)}\")\n",
    "for key, entity in registry.entities.items():\n",
    "    print(f\"  - {entity['label']} ({entity['type']}): {entity['uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2368a",
   "metadata": {},
   "source": [
    "## Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bf97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE COMPLETE\n",
      "============================================================\n",
      "\n",
      "Article: Albert Einstein\n",
      "Source: https://en.wikipedia.org/wiki/Albert_Einstein\n",
      "License: CC BY-SA 4.0\n",
      "\n",
      "Generated artifacts:\n",
      "  1. data/albert_einstein_chunks.ipynb\n",
      "     - 63 chunks with breadcrumb context\n",
      "  2. data/albert_einstein_facts.ipynb\n",
      "     - Facts extracted from chunks (see notebook for details)\n",
      "  3. data/albert_einstein_rdf.ipynb\n",
      "     - RDF triples generated from facts (see notebook for details)\n",
      "  4. data/albert_einstein.ttl\n",
      "     - Combined Turtle file for import\n",
      "\n",
      "Entity registry: data/entity_registry.json\n",
      "\n",
      "The intermediate notebooks can be reviewed and edited before re-export.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nArticle: {ARTICLE_TITLE}\")\n",
    "print(f\"Source: {source_url}\")\n",
    "print(f\"License: {provenance['license']}\")\n",
    "print(f\"Run timestamp: {RUN_TIMESTAMP}\")\n",
    "print(f\"\\nOutput directory: {RUN_OUTPUT_DIR}\")\n",
    "print(f\"\\nGenerated artifacts:\")\n",
    "print(f\"  1. chunks.ipynb\")\n",
    "print(f\"     - {len(contextual_chunks)} chunks with breadcrumb context\")\n",
    "print(f\"  2. facts.ipynb\")\n",
    "print(f\"     - Facts extracted from chunks\")\n",
    "print(f\"  3. rdf.ipynb\")\n",
    "print(f\"     - RDF triples generated via tool calls\")\n",
    "print(f\"  4. triples.ttl\")\n",
    "print(f\"     - Combined Turtle file for import\")\n",
    "print(f\"  5. registry.json\")\n",
    "print(f\"     - Entity registry snapshot\")\n",
    "print(f\"\\nThe intermediate notebooks can be reviewed and edited before re-export.\")\n",
    "print(f\"Files can reference each other with relative paths within the run directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef4bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
