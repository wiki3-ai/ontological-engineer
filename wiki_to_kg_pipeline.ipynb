{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236e29d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# mamba install -c conda-forge langchain langchain-text-splitters langchain-openai langchain-community wikipedia pydantic nbformat nbconvert\n",
    "%pip install -q langchain langchain-text-splitters langchain-openai langchain-community wikipedia pydantic nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833f695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded (with CID support)\n",
      "Cell timeout: 60s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "from urllib.parse import quote\n",
    "\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell, new_raw_cell\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# === Content ID (CID) Functions ===\n",
    "\n",
    "def compute_cid(content: str) -> str:\n",
    "    \"\"\"Compute SHA256 content ID for a string.\"\"\"\n",
    "    return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "def make_signature(cell_num: int, cell_type: str, cid: str, from_cid: str) -> dict:\n",
    "    \"\"\"Create a signature dict for a generated cell.\"\"\"\n",
    "    return {\n",
    "        \"cell\": cell_num,\n",
    "        \"type\": cell_type,\n",
    "        \"cid\": cid,\n",
    "        \"from_cid\": from_cid,\n",
    "    }\n",
    "\n",
    "def parse_signature(raw_content: str) -> Optional[dict]:\n",
    "    \"\"\"Parse a signature from raw cell content. Returns None if not a valid signature.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(raw_content.strip())\n",
    "        if all(k in data for k in (\"cell\", \"type\", \"cid\", \"from_cid\")):\n",
    "            return data\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def extract_signatures(notebook) -> dict:\n",
    "    \"\"\"Extract all signatures from a notebook, keyed by cell number.\"\"\"\n",
    "    signatures = {}\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == 'raw':\n",
    "            sig = parse_signature(cell.source)\n",
    "            if sig:\n",
    "                signatures[sig[\"cell\"]] = sig\n",
    "    return signatures\n",
    "\n",
    "# === Timeout Configuration ===\n",
    "# Note: SIGALRM doesn't work for blocking I/O (HTTP requests)\n",
    "# Instead, we use the timeout parameter on the ChatOpenAI client\n",
    "\n",
    "CELL_TIMEOUT_SECONDS = 60  # 1 minute per cell max\n",
    "MAX_ITERATIONS = 150     # Max tools calls per cell processing\n",
    "\n",
    "def log_progress(msg: str, end=\"\\n\"):\n",
    "    \"\"\"Print with immediate flush for real-time progress.\"\"\"\n",
    "    print(msg, end=end, flush=True)\n",
    "\n",
    "print(\"Dependencies loaded (with CID support)\")\n",
    "print(f\"Cell timeout: {CELL_TIMEOUT_SECONDS}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252ef95",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f79c403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuing from previous run: data/albert_einstein/20251218_202954\n",
      "  Copied 3 files: rdf.ipynb, facts.ipynb, chunks.ipynb\n",
      "  Existing cells with matching CIDs will be skipped\n",
      "Pipeline configured for: Albert Einstein\n",
      "Output directory: data/albert_einstein/20251218_215312\n",
      "Run timestamp: 20251218_215312\n"
     ]
    }
   ],
   "source": [
    "# Pipeline configuration\n",
    "ARTICLE_TITLE = \"Albert Einstein\"\n",
    "OUTPUT_DIR = \"data\"\n",
    "CHUNK_SIZE = 2000\n",
    "CHUNK_OVERLAP = 128\n",
    "\n",
    "# Continue from a previous run (set to None for fresh run)\n",
    "# Example: \"data/albert_einstein/20241218_143022\"\n",
    "CONTINUE_FROM_RUN = \"data/albert_einstein/20251218_202954\"\n",
    "\n",
    "# LLM configuration (shared across stages)\n",
    "LLM_CONFIG = {\n",
    "    \"provider\": \"lm_studio\",  # or \"openai\"\n",
    "    \"model\": \"qwen/qwen3-coder-30b\",\n",
    "    \"temperature\": 1,\n",
    "    \"base_url\": os.environ.get(\"LM_STUDIO_BASE_URL\", \"http://host.docker.internal:1234/v1\"),\n",
    "}\n",
    "\n",
    "# Generate run timestamp and article-specific output directory\n",
    "# Structure: data/{article_slug}/{timestamp}/\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "article_slug = ARTICLE_TITLE.lower().replace(' ', '_')\n",
    "RUN_OUTPUT_DIR = os.path.join(OUTPUT_DIR, article_slug, RUN_TIMESTAMP)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RUN_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Copy files from previous run if specified\n",
    "if CONTINUE_FROM_RUN:\n",
    "    import shutil\n",
    "    if os.path.isdir(CONTINUE_FROM_RUN):\n",
    "        print(f\"Continuing from previous run: {CONTINUE_FROM_RUN}\")\n",
    "        copied_files = []\n",
    "        for filename in os.listdir(CONTINUE_FROM_RUN):\n",
    "            src = os.path.join(CONTINUE_FROM_RUN, filename)\n",
    "            dst = os.path.join(RUN_OUTPUT_DIR, filename)\n",
    "            if os.path.isfile(src):\n",
    "                shutil.copy2(src, dst)\n",
    "                copied_files.append(filename)\n",
    "        print(f\"  Copied {len(copied_files)} files: {', '.join(copied_files)}\")\n",
    "        print(f\"  Existing cells with matching CIDs will be skipped\")\n",
    "    else:\n",
    "        print(f\"WARNING: Previous run not found: {CONTINUE_FROM_RUN}\")\n",
    "        print(f\"  Starting fresh run instead\")\n",
    "\n",
    "print(f\"Pipeline configured for: {ARTICLE_TITLE}\")\n",
    "print(f\"Output directory: {RUN_OUTPUT_DIR}\")\n",
    "print(f\"Run timestamp: {RUN_TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df90b5b",
   "metadata": {},
   "source": [
    "## Entity Registry\n",
    "\n",
    "Manages entity identity across chunks with stable URIs derived from source URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a66f9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntityRegistry class defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class EntityRegistry:\n",
    "    \"\"\"Tracks entities with stable IDs derived from source URL.\"\"\"\n",
    "    source_url: str\n",
    "    entities: dict = field(default_factory=dict)  # normalized_key -> entity\n",
    "    aliases: dict = field(default_factory=dict)   # alias -> canonical_key\n",
    "    \n",
    "    def normalize_key(self, label: str) -> str:\n",
    "        \"\"\"Create consistent key from entity label.\"\"\"\n",
    "        return re.sub(r'[^a-z0-9]+', '_', label.lower().strip()).strip('_')\n",
    "    \n",
    "    def generate_uri(self, entity_type: str, label: str) -> str:\n",
    "        \"\"\"Generate URI based on source URL with fragment identifier.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        # Use source URL as base, add fragment for entity\n",
    "        return f\"{self.source_url}#{entity_type.lower()}_{key}\"\n",
    "    \n",
    "    def generate_id(self, entity_type: str, label: str) -> str:\n",
    "        \"\"\"Generate local ID for internal reference.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        return f\"{entity_type.lower()}_{key}\"\n",
    "    \n",
    "    def register(self, label: str, entity_type: str, description: str = \"\",\n",
    "                 aliases: list = None, source_chunk: int = None) -> str:\n",
    "        \"\"\"Register or update an entity, return canonical ID.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        entity_id = self.generate_id(entity_type, label)\n",
    "        entity_uri = self.generate_uri(entity_type, label)\n",
    "        \n",
    "        if key not in self.entities:\n",
    "            self.entities[key] = {\n",
    "                \"id\": entity_id,\n",
    "                \"uri\": entity_uri,\n",
    "                \"label\": label,\n",
    "                \"type\": entity_type,\n",
    "                \"descriptions\": [description] if description else [],\n",
    "                \"source_chunks\": [source_chunk] if source_chunk is not None else [],\n",
    "                \"aliases\": list(aliases or []),\n",
    "            }\n",
    "        else:\n",
    "            existing = self.entities[key]\n",
    "            if description and description not in existing[\"descriptions\"]:\n",
    "                existing[\"descriptions\"].append(description)\n",
    "            if source_chunk is not None and source_chunk not in existing[\"source_chunks\"]:\n",
    "                existing[\"source_chunks\"].append(source_chunk)\n",
    "            if aliases:\n",
    "                existing[\"aliases\"] = list(set(existing[\"aliases\"]) | set(aliases))\n",
    "        \n",
    "        # Register aliases\n",
    "        for alias in (aliases or []):\n",
    "            self.aliases[self.normalize_key(alias)] = key\n",
    "        \n",
    "        return entity_id\n",
    "    \n",
    "    def lookup(self, label: str) -> Optional[dict]:\n",
    "        \"\"\"Find entity by label or alias.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        canonical_key = self.aliases.get(key, key)\n",
    "        return self.entities.get(canonical_key)\n",
    "    \n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Serialize registry to JSON.\"\"\"\n",
    "        return json.dumps({\n",
    "            \"source_url\": self.source_url,\n",
    "            \"entities\": self.entities,\n",
    "            \"aliases\": self.aliases,\n",
    "        }, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, json_str: str) -> 'EntityRegistry':\n",
    "        \"\"\"Deserialize registry from JSON.\"\"\"\n",
    "        data = json.loads(json_str)\n",
    "        registry = cls(source_url=data[\"source_url\"])\n",
    "        registry.entities = data[\"entities\"]\n",
    "        registry.aliases = data[\"aliases\"]\n",
    "        return registry\n",
    "\n",
    "print(\"EntityRegistry class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094a385",
   "metadata": {},
   "source": [
    "## Section Hierarchy Parser\n",
    "\n",
    "Extracts Wikipedia section structure for breadcrumb context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fe68df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section hierarchy parser defined\n"
     ]
    }
   ],
   "source": [
    "def extract_section_hierarchy(content: str) -> list[dict]:\n",
    "    \"\"\"Parse Wikipedia == headers == into hierarchical structure with positions.\"\"\"\n",
    "    header_pattern = re.compile(r'^(={2,6})\\s*(.+?)\\s*\\1\\s*$', re.MULTILINE)\n",
    "    \n",
    "    sections = []\n",
    "    current_path = []  # Stack of (level, title)\n",
    "    \n",
    "    for match in header_pattern.finditer(content):\n",
    "        level = len(match.group(1))  # Number of '=' signs\n",
    "        title = match.group(2).strip()\n",
    "        \n",
    "        # Pop stack until we're at parent level\n",
    "        while current_path and current_path[-1][0] >= level:\n",
    "            current_path.pop()\n",
    "        \n",
    "        current_path.append((level, title))\n",
    "        breadcrumb = \" > \".join(t for _, t in current_path)\n",
    "        \n",
    "        sections.append({\n",
    "            \"level\": level,\n",
    "            \"title\": title,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"start_pos\": match.start(),\n",
    "            \"end_pos\": match.end(),\n",
    "        })\n",
    "    \n",
    "    return sections\n",
    "\n",
    "\n",
    "def get_section_context(position: int, sections: list[dict], article_title: str) -> dict:\n",
    "    \"\"\"Find the section context for a given character position.\"\"\"\n",
    "    active_section = {\n",
    "        \"title\": \"Introduction\",\n",
    "        \"breadcrumb\": \"Introduction\",\n",
    "        \"level\": 1,\n",
    "    }\n",
    "    \n",
    "    for section in sections:\n",
    "        if section[\"start_pos\"] <= position:\n",
    "            active_section = section\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"section_title\": active_section[\"title\"],\n",
    "        \"breadcrumb\": f\"{article_title} > {active_section['breadcrumb']}\",\n",
    "    }\n",
    "\n",
    "print(\"Section hierarchy parser defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360c37f",
   "metadata": {},
   "source": [
    "## Fetch Wikipedia Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b91833b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched: Albert Einstein\n",
      "Source URL: https://en.wikipedia.org/wiki/Albert_Einstein\n",
      "Content length: 87959 characters\n",
      "License: CC BY-SA 4.0\n"
     ]
    }
   ],
   "source": [
    "# Fetch Wikipedia article\n",
    "loader = WikipediaLoader(query=ARTICLE_TITLE, load_max_docs=1, doc_content_chars_max=100000)\n",
    "docs = loader.load()\n",
    "\n",
    "if not docs:\n",
    "    raise ValueError(f\"Could not fetch article: {ARTICLE_TITLE}\")\n",
    "\n",
    "raw_content = docs[0].page_content\n",
    "metadata = docs[0].metadata\n",
    "\n",
    "# Construct source URL and provenance\n",
    "source_url = f\"https://en.wikipedia.org/wiki/{quote(ARTICLE_TITLE.replace(' ', '_'))}\"\n",
    "\n",
    "provenance = {\n",
    "    \"source_url\": source_url,\n",
    "    \"article_title\": ARTICLE_TITLE,\n",
    "    \"fetched_at\": datetime.now().isoformat(),\n",
    "    \"content_length\": len(raw_content),\n",
    "    # Wikipedia license - standard for all Wikipedia content\n",
    "    \"license\": \"CC BY-SA 4.0\",\n",
    "    \"license_url\": \"https://creativecommons.org/licenses/by-sa/4.0/\",\n",
    "    \"attribution\": \"Wikipedia contributors\",\n",
    "}\n",
    "\n",
    "print(f\"Fetched: {ARTICLE_TITLE}\")\n",
    "print(f\"Source URL: {source_url}\")\n",
    "print(f\"Content length: {len(raw_content)} characters\")\n",
    "print(f\"License: {provenance['license']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1276b5",
   "metadata": {},
   "source": [
    "## Create Contextual Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3146caad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 sections\n",
      "Split into 63 chunks\n",
      "\n",
      "Chunks with context:\n",
      "  Chunk 1: Albert Einstein > Introduction\n",
      "  Chunk 2: Albert Einstein > Introduction\n",
      "  Chunk 3: Albert Einstein > Life and career\n"
     ]
    }
   ],
   "source": [
    "# Parse section hierarchy\n",
    "sections = extract_section_hierarchy(raw_content)\n",
    "print(f\"Found {len(sections)} sections\")\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "raw_chunks = splitter.split_text(raw_content)\n",
    "print(f\"Split into {len(raw_chunks)} chunks\")\n",
    "\n",
    "# Add context to each chunk\n",
    "@dataclass\n",
    "class ContextualChunk:\n",
    "    content: str\n",
    "    chunk_index: int\n",
    "    total_chunks: int\n",
    "    breadcrumb: str\n",
    "    section_title: str\n",
    "    char_start: int\n",
    "    char_end: int\n",
    "\n",
    "contextual_chunks = []\n",
    "current_pos = 0\n",
    "\n",
    "for i, chunk_text in enumerate(raw_chunks):\n",
    "    # Find position in original content\n",
    "    chunk_start = raw_content.find(chunk_text, current_pos)\n",
    "    if chunk_start == -1:\n",
    "        chunk_start = current_pos  # Fallback\n",
    "    chunk_end = chunk_start + len(chunk_text)\n",
    "    \n",
    "    # Get section context\n",
    "    context = get_section_context(chunk_start, sections, ARTICLE_TITLE)\n",
    "    \n",
    "    contextual_chunks.append(ContextualChunk(\n",
    "        content=chunk_text,\n",
    "        chunk_index=i,\n",
    "        total_chunks=len(raw_chunks),\n",
    "        breadcrumb=context[\"breadcrumb\"],\n",
    "        section_title=context[\"section_title\"],\n",
    "        char_start=chunk_start,\n",
    "        char_end=chunk_end,\n",
    "    ))\n",
    "    \n",
    "    current_pos = chunk_start + 1\n",
    "\n",
    "print(f\"\\nChunks with context:\")\n",
    "for chunk in contextual_chunks[:3]:\n",
    "    print(f\"  Chunk {chunk.chunk_index + 1}: {chunk.breadcrumb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3af602",
   "metadata": {},
   "source": [
    "## Initialize Entity Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34b0e8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity registry initialized with subject: Albert Einstein\n",
      "Subject URI: https://en.wikipedia.org/wiki/Albert_Einstein#person_albert_einstein\n"
     ]
    }
   ],
   "source": [
    "# Initialize entity registry with article subject\n",
    "registry = EntityRegistry(source_url=source_url)\n",
    "\n",
    "# Pre-seed with the article subject\n",
    "registry.register(\n",
    "    label=ARTICLE_TITLE,\n",
    "    entity_type=\"Person\",  # Adjust based on article type\n",
    "    description=f\"Subject of Wikipedia article: {ARTICLE_TITLE}\",\n",
    "    aliases=[ARTICLE_TITLE.split()[-1]],  # Last name as alias\n",
    ")\n",
    "\n",
    "print(f\"Entity registry initialized with subject: {ARTICLE_TITLE}\")\n",
    "print(f\"Subject URI: {registry.entities[registry.normalize_key(ARTICLE_TITLE)]['uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd3cdd",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "These prompts are embedded in the generated notebooks for transparency and adjustability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3089027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates defined\n"
     ]
    }
   ],
   "source": [
    "FACTS_EXTRACTION_PROMPT = \"\"\"You are an expert at extracting factual information from text.\n",
    "\n",
    "Given text from a Wikipedia article, extract simple English statements that capture the consensus factual information. Each statement should:\n",
    "- Be a single, clear sentence\n",
    "- Contain one main fact or relationship\n",
    "- Use the full name of entities on first mention\n",
    "- Be verifiable from the source text\n",
    "- Avoid opinions, interpretations, or hedged language\n",
    "\n",
    "The text comes from: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Known entities (use consistent names):\n",
    "{known_entities}\n",
    "\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\n",
    "Extract factual statements as a bulleted list. Also identify any new entities (people, places, organizations, concepts, events, works) that should be added to the registry.\n",
    "\"\"\"\n",
    "\n",
    "RDF_GENERATION_PROMPT = \"\"\"You are an expert at converting factual statements to RDF triples in Turtle format.\n",
    "\n",
    "Convert the following factual statements to RDF using schema.org vocabulary where possible.\n",
    "\n",
    "Source: {source_url}\n",
    "Section: {breadcrumb}\n",
    "\n",
    "Use these prefixes:\n",
    "{prefixes}\n",
    "\n",
    "Entity registry (use these URIs):\n",
    "{entity_registry}\n",
    "\n",
    "Guidelines:\n",
    "- Use schema.org properties (schema:birthDate, schema:birthPlace, schema:worksFor, etc.)\n",
    "- For relationships not in schema.org, use wiki3: prefix\n",
    "- Include rdfs:label for entities\n",
    "- Use xsd datatypes for dates and numbers\n",
    "- Entity URIs should use the source URL as base with fragment identifiers\n",
    "\n",
    "---\n",
    "{facts}\n",
    "---\n",
    "\n",
    "Generate Turtle RDF:\n",
    "\"\"\"\n",
    "\n",
    "RDF_PREFIXES = \"\"\"@prefix schema: <https://schema.org/> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix wiki3: <https://wiki3.ai/vocab/> .\n",
    "@base <{source_url}> .\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prompt templates defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa35e61",
   "metadata": {},
   "source": [
    "## Generate Chunks Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d121080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: data/albert_einstein/20251218_215312/chunks.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_chunks_notebook(chunks: list, provenance: dict, registry: EntityRegistry, \n",
    "                             llm_config: dict, output_path: str):\n",
    "    \"\"\"Generate a notebook with chunked source text and context metadata.\n",
    "    \n",
    "    Each chunk cell is followed by a signature raw cell with its CID.\n",
    "    \"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    # Cell 0: Provenance markdown\n",
    "    provenance_yaml = f\"\"\"# Chunked Text: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "fetched_at: {provenance['fetched_at']}\n",
    "content_length: {provenance['content_length']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "attribution: {provenance['attribution']}\n",
    "chunk_size: {CHUNK_SIZE}\n",
    "chunk_overlap: {CHUNK_OVERLAP}\n",
    "total_chunks: {len(chunks)}\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "Each chunk below contains source text with contextual metadata. The context line (before the separator) provides:\n",
    "- **Context**: Hierarchical breadcrumb showing article > section path\n",
    "- **Chunk**: Position in sequence\n",
    "\n",
    "The text below the `---` separator is the unchanged source content.\n",
    "Each chunk is followed by a signature cell containing its Content ID (CID).\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_yaml))\n",
    "    \n",
    "    # Cell 1: Entity registry (raw cell)\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    # Chunk cells with signatures\n",
    "    # Compute CID of raw source for provenance\n",
    "    source_cid = compute_cid(provenance['source_url'] + str(provenance['content_length']))\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Content cell\n",
    "        chunk_content = f\"\"\"**Context:** {chunk.breadcrumb}\n",
    "**Chunk:** {chunk.chunk_index + 1} of {chunk.total_chunks}\n",
    "\n",
    "---\n",
    "\n",
    "{chunk.content}\n",
    "\"\"\"\n",
    "        nb.cells.append(new_markdown_cell(chunk_content))\n",
    "        \n",
    "        # Signature cell\n",
    "        chunk_cid = compute_cid(chunk_content)\n",
    "        signature = make_signature(\n",
    "            cell_num=chunk.chunk_index + 1,\n",
    "            cell_type=\"chunk\",\n",
    "            cid=chunk_cid,\n",
    "            from_cid=source_cid\n",
    "        )\n",
    "        nb.cells.append(new_raw_cell(json.dumps(signature)))\n",
    "    \n",
    "    # Write notebook\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(nb, f)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate chunks notebook (simple filename, timestamp is in directory)\n",
    "# Always regenerate chunks since they come from fresh Wikipedia fetch\n",
    "chunks_path = os.path.join(RUN_OUTPUT_DIR, \"chunks.ipynb\")\n",
    "generate_chunks_notebook(contextual_chunks, provenance, registry, LLM_CONFIG, chunks_path)\n",
    "print(f\"Generated: {chunks_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c23dc4",
   "metadata": {},
   "source": [
    "## Generate Facts Notebook (Structure Only)\n",
    "\n",
    "Creates the facts notebook with placeholders. Actual content is generated in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd4acab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing: data/albert_einstein/20251218_215312/facts.ipynb (from previous run)\n"
     ]
    }
   ],
   "source": [
    "def generate_facts_notebook(chunks: list, provenance: dict, registry: EntityRegistry,\n",
    "                            llm_config: dict, source_notebook: str, output_path: str):\n",
    "    \"\"\"Generate a notebook structure for extracted facts.\n",
    "    \n",
    "    Creates header cells only - actual fact content is populated by the pipeline\n",
    "    during the facts extraction phase.\n",
    "    \n",
    "    Args:\n",
    "        chunks: List of ContextualChunk objects (for count info)\n",
    "        provenance: Dictionary with article provenance info\n",
    "        registry: EntityRegistry for consistent URIs\n",
    "        llm_config: Dict with model, endpoint config\n",
    "        source_notebook: Relative path to source chunks notebook\n",
    "        output_path: Where to write the facts notebook\n",
    "    \"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    # Cell 0: Provenance markdown\n",
    "    provenance_yaml = f\"\"\"# Extracted Facts: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_notebook: {source_notebook}\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "extraction_model: {llm_config['model']}\n",
    "extraction_timestamp: {datetime.now().isoformat()}\n",
    "total_chunks: {len(chunks)}\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "This notebook contains factual statements extracted from source chunks.\n",
    "Each facts cell is followed by a signature cell with CID provenance linking back to its source chunk.\n",
    "\n",
    "To regenerate a specific cell: delete both the content cell and its signature, then re-run the pipeline.\n",
    "\n",
    "Structure:\n",
    "- **Context**: Section breadcrumb from source\n",
    "- **Chunk**: Position in sequence\n",
    "- **Facts**: Bulleted list of extracted factual statements\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_yaml))\n",
    "    \n",
    "    # Cell 1: Entity registry (raw cell)\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    # Note: Actual fact cells are added by the pipeline during processing\n",
    "    # Each will be: markdown content cell + raw signature cell\n",
    "    \n",
    "    # Write notebook\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(nb, f)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate facts notebook (only if doesn't exist - preserve previous run's content)\n",
    "facts_path = os.path.join(RUN_OUTPUT_DIR, \"facts.ipynb\")\n",
    "if not os.path.exists(facts_path):\n",
    "    generate_facts_notebook(contextual_chunks, provenance, registry, LLM_CONFIG, \n",
    "                           \"chunks.ipynb\", facts_path)\n",
    "    print(f\"Generated: {facts_path}\")\n",
    "else:\n",
    "    print(f\"Using existing: {facts_path} (from previous run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256713fd",
   "metadata": {},
   "source": [
    "## Generate RDF Notebook (Structure Only)\n",
    "\n",
    "Creates the RDF notebook with placeholders. Actual content is generated after facts extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a7ae750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing: data/albert_einstein/20251218_215312/rdf.ipynb (from previous run)\n"
     ]
    }
   ],
   "source": [
    "def generate_rdf_notebook_header(provenance: dict, registry: EntityRegistry,\n",
    "                                  llm_config: dict, prompt_template: str, prefixes: str) -> nbformat.NotebookNode:\n",
    "    \"\"\"Generate just the header cells for RDF notebook.\"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    formatted_prefixes = prefixes.format(source_url=provenance['source_url'])\n",
    "    provenance_md = f\"\"\"# RDF Triples: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "source_notebook: facts.ipynb\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "llm_provider: {llm_config['provider']}\n",
    "llm_model: {llm_config['model']}\n",
    "llm_temperature: {llm_config['temperature']}\n",
    "rdf_format: Turtle\n",
    "```\n",
    "\n",
    "## RDF Prefixes\n",
    "\n",
    "```turtle\n",
    "{formatted_prefixes}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "This notebook contains RDF triples in Turtle format, one cell per source facts cell.\n",
    "Each content cell is followed by a signature cell with CID provenance.\n",
    "\n",
    "To regenerate a specific cell: delete both the content cell and its signature, then re-run the pipeline.\n",
    "\n",
    "## Prompt Template\n",
    "\n",
    "```\n",
    "{prompt_template}\n",
    "```\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_md))\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    return nb\n",
    "\n",
    "# Create initial RDF notebook (only if doesn't exist - preserve previous run's content)\n",
    "rdf_path = os.path.join(RUN_OUTPUT_DIR, \"rdf.ipynb\")\n",
    "\n",
    "if not os.path.exists(rdf_path):\n",
    "    rdf_nb = generate_rdf_notebook_header(provenance, registry, LLM_CONFIG, RDF_GENERATION_PROMPT, RDF_PREFIXES)\n",
    "    with open(rdf_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(rdf_nb, f)\n",
    "    print(f\"Created RDF notebook: {rdf_path}\")\n",
    "else:\n",
    "    print(f\"Using existing: {rdf_path} (from previous run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696afdf",
   "metadata": {},
   "source": [
    "## Process Chunks → Extract Facts\n",
    "\n",
    "Run the LLM on each chunk to extract factual statements and update the facts notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2350ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks notebook...\n",
      "Found 63 chunks with CIDs\n",
      "Found 63 existing fact signatures\n",
      "HTTP timeout: 60s per request\n",
      "--------------------------------------------------\n",
      "  Chunk 1: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 2: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 3: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 4: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 5: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 6: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 7: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 8: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 9: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 10: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 11: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 12: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 13: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 14: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 15: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 16: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 17: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 18: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 19: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 20: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 21: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 22: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 23: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 24: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 25: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 26: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 27: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 28: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 29: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 30: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 31: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 32: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 33: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 34: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 35: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 36: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 37: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 38: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 39: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 40: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 41: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 42: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 43: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 44: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 45: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 46: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 47: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 48: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 49: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 50: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 51: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 52: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 53: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 54: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 55: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 56: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 57: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 58: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 59: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 60: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 61: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 62: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 63: ⊘ Up-to-date (CID match), skipping\n",
      "--------------------------------------------------\n",
      "Facts extraction complete:\n",
      "  - 0 generated\n",
      "  - 63 skipped (up-to-date)\n",
      "  - 0 errors/timeouts\n"
     ]
    }
   ],
   "source": [
    "# Create LLM client with timeout for facts extraction\n",
    "facts_llm = ChatOpenAI(\n",
    "    model=LLM_CONFIG[\"model\"],\n",
    "    temperature=LLM_CONFIG[\"temperature\"],\n",
    "    base_url=LLM_CONFIG[\"base_url\"],\n",
    "    api_key=\"lm-studio\",\n",
    "    timeout=CELL_TIMEOUT_SECONDS,  # HTTP timeout in seconds\n",
    "    max_retries=0,  # Don't retry on timeout\n",
    ")\n",
    "\n",
    "# Create facts extraction prompt\n",
    "facts_prompt = ChatPromptTemplate.from_template(\"\"\"You are an expert at extracting factual information from text.\n",
    "\n",
    "Given text from a Wikipedia article, extract simple English statements that capture the consensus factual information. Each statement should:\n",
    "- Be a single, clear sentence\n",
    "- Contain one main fact or relationship\n",
    "- Use the full name of entities on first mention\n",
    "- Be verifiable from the source text\n",
    "- Avoid opinions, interpretations, or hedged language\n",
    "\n",
    "Source: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Known entities (use consistent names):\n",
    "{known_entities}\n",
    "\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\n",
    "Extract factual statements as a bulleted list:\"\"\")\n",
    "\n",
    "facts_chain = facts_prompt | facts_llm\n",
    "\n",
    "def get_known_entities_text(registry: EntityRegistry) -> str:\n",
    "    \"\"\"Format known entities for prompt context.\"\"\"\n",
    "    lines = []\n",
    "    for entity in registry.entities.values():\n",
    "        lines.append(f\"- {entity['label']} ({entity['type']})\")\n",
    "    return \"\\n\".join(lines) if lines else \"None yet\"\n",
    "\n",
    "# Read chunks notebook to get source content and CIDs\n",
    "log_progress(\"Reading chunks notebook...\")\n",
    "chunks_nb = nbformat.read(chunks_path, as_version=4)\n",
    "chunk_signatures = extract_signatures(chunks_nb)\n",
    "\n",
    "# Build list of chunk content with CIDs\n",
    "# Chunks notebook structure: [provenance, registry, chunk1, sig1, chunk2, sig2, ...]\n",
    "chunk_data = []\n",
    "cell_idx = 2  # Skip provenance and registry\n",
    "while cell_idx < len(chunks_nb.cells):\n",
    "    cell = chunks_nb.cells[cell_idx]\n",
    "    if cell.cell_type == 'markdown':\n",
    "        content = cell.source\n",
    "        # Get corresponding signature (next cell)\n",
    "        sig = None\n",
    "        if cell_idx + 1 < len(chunks_nb.cells):\n",
    "            sig = parse_signature(chunks_nb.cells[cell_idx + 1].source)\n",
    "        \n",
    "        # Extract breadcrumb\n",
    "        context_match = re.search(r'\\*\\*Context:\\*\\*\\s*(.+)', content)\n",
    "        breadcrumb = context_match.group(1) if context_match else \"Unknown\"\n",
    "        \n",
    "        # Extract chunk text (after ---)\n",
    "        parts = content.split(\"---\\n\", 1)\n",
    "        chunk_text = parts[1].strip() if len(parts) > 1 else content\n",
    "        \n",
    "        chunk_data.append({\n",
    "            \"cell_num\": sig[\"cell\"] if sig else len(chunk_data) + 1,\n",
    "            \"content\": content,\n",
    "            \"chunk_text\": chunk_text,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"cid\": sig[\"cid\"] if sig else compute_cid(content),\n",
    "        })\n",
    "        cell_idx += 2  # Skip content and signature\n",
    "    else:\n",
    "        cell_idx += 1\n",
    "\n",
    "log_progress(f\"Found {len(chunk_data)} chunks with CIDs\")\n",
    "\n",
    "# Read existing facts notebook and extract signatures\n",
    "facts_nb = nbformat.read(facts_path, as_version=4)\n",
    "facts_signatures = extract_signatures(facts_nb)\n",
    "\n",
    "log_progress(f\"Found {len(facts_signatures)} existing fact signatures\")\n",
    "log_progress(f\"HTTP timeout: {CELL_TIMEOUT_SECONDS}s per request\")\n",
    "log_progress(\"-\" * 50)\n",
    "\n",
    "# Process each chunk\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for chunk in chunk_data:\n",
    "    cell_num = chunk[\"cell_num\"]\n",
    "    source_cid = chunk[\"cid\"]\n",
    "    \n",
    "    # Check if we already have up-to-date facts for this chunk\n",
    "    existing_sig = facts_signatures.get(cell_num)\n",
    "    if existing_sig and existing_sig[\"from_cid\"] == source_cid:\n",
    "        log_progress(f\"  Chunk {cell_num}: ⊘ Up-to-date (CID match), skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Need to generate (or regenerate) this cell\n",
    "    status = \"↻ Regenerating\" if existing_sig else \"+ Generating\"\n",
    "    chunk_preview = chunk[\"chunk_text\"][:80].replace('\\n', ' ')\n",
    "    log_progress(f\"  Chunk {cell_num}: {status}\")\n",
    "    log_progress(f\"    Context: {chunk['breadcrumb']}\")\n",
    "    log_progress(f\"    Input: {chunk_preview}...\")\n",
    "    log_progress(f\"    [Calling LLM...]\", end=\" \")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Call LLM to extract facts\n",
    "    try:\n",
    "        result = facts_chain.invoke({\n",
    "            \"source_url\": provenance[\"source_url\"],\n",
    "            \"breadcrumb\": chunk[\"breadcrumb\"],\n",
    "            \"known_entities\": get_known_entities_text(registry),\n",
    "            \"chunk_text\": chunk[\"chunk_text\"],\n",
    "        })\n",
    "        facts_content = result.content\n",
    "        elapsed = time.time() - start_time\n",
    "        log_progress(f\"✓ ({len(facts_content)} chars, {elapsed:.1f}s)\")\n",
    "        processed_count += 1\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)[:100]\n",
    "        facts_content = f\"# Error: {error_type}: {e}\"\n",
    "        log_progress(f\"✗ {error_type} after {elapsed:.1f}s\")\n",
    "        log_progress(f\"    Error: {error_msg}\")\n",
    "        error_count += 1\n",
    "    \n",
    "    # Build the facts cell content\n",
    "    facts_cell_content = f\"\"\"**Context:** {chunk['breadcrumb']}\n",
    "**Chunk:** {cell_num} of {len(chunk_data)}\n",
    "\n",
    "---\n",
    "\n",
    "{facts_content}\n",
    "\"\"\"\n",
    "    facts_cid = compute_cid(facts_cell_content)\n",
    "    signature = make_signature(cell_num, \"facts\", facts_cid, source_cid)\n",
    "    \n",
    "    # Find where to insert/update in the notebook\n",
    "    # If there's an existing signature for this cell, find and remove old content+sig\n",
    "    if existing_sig:\n",
    "        # Find and remove the old cells\n",
    "        new_cells = [facts_nb.cells[0], facts_nb.cells[1]]  # Keep header\n",
    "        i = 2\n",
    "        while i < len(facts_nb.cells):\n",
    "            cell = facts_nb.cells[i]\n",
    "            if cell.cell_type == 'raw':\n",
    "                sig = parse_signature(cell.source)\n",
    "                if sig and sig[\"cell\"] == cell_num:\n",
    "                    # Skip this signature and its preceding content cell\n",
    "                    i += 1\n",
    "                    continue\n",
    "            # Check if this is content for the cell we're replacing\n",
    "            if i > 0 and i + 1 < len(facts_nb.cells):\n",
    "                next_sig = parse_signature(facts_nb.cells[i + 1].source) if facts_nb.cells[i + 1].cell_type == 'raw' else None\n",
    "                if next_sig and next_sig[\"cell\"] == cell_num:\n",
    "                    i += 2  # Skip content and signature\n",
    "                    continue\n",
    "            new_cells.append(cell)\n",
    "            i += 1\n",
    "        facts_nb.cells = new_cells\n",
    "    \n",
    "    # Append new content and signature\n",
    "    facts_nb.cells.append(new_markdown_cell(facts_cell_content))\n",
    "    facts_nb.cells.append(new_raw_cell(json.dumps(signature)))\n",
    "    \n",
    "    # Update signatures dict\n",
    "    facts_signatures[cell_num] = signature\n",
    "    \n",
    "    # Save notebook after each cell\n",
    "    log_progress(f\"    [Saving notebook...]\", end=\" \")\n",
    "    with open(facts_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(facts_nb, f)\n",
    "    log_progress(\"saved\")\n",
    "\n",
    "log_progress(\"-\" * 50)\n",
    "log_progress(f\"Facts extraction complete:\")\n",
    "log_progress(f\"  - {processed_count} generated\")\n",
    "log_progress(f\"  - {skipped_count} skipped (up-to-date)\")\n",
    "log_progress(f\"  - {error_count} errors/timeouts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d29b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts notebook: data/albert_einstein/20251218_215312/facts.ipynb\n",
      "  - Updated incrementally during processing\n",
      "  - Ready for review/editing before RDF generation\n"
     ]
    }
   ],
   "source": [
    "# Summary of facts extraction (notebook already updated incrementally)\n",
    "print(f\"Facts notebook: {facts_path}\")\n",
    "print(f\"  - Updated incrementally during processing\")\n",
    "print(f\"  - Ready for review/editing before RDF generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de7f1f",
   "metadata": {},
   "source": [
    "## Process Facts → Generate RDF\n",
    "\n",
    "Run the LLM on each facts cell to generate RDF triples and update the RDF notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afd0797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded schema matcher with 1 vocabularies\n",
      "Reading facts notebook...\n",
      "Found 63 facts cells with CIDs\n",
      "Found 27 existing RDF signatures\n",
      "HTTP timeout: 60s per request\n",
      "Max iterations: 150 per cell\n",
      "--------------------------------------------------\n",
      "  Facts 1: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 2: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 3: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 4: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 5: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 6: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 7: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 8: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 9: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 10: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 11: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 12: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 13: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 14: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 15: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 16: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 17: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 18: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 19: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 20: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 21: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 22: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 23: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 24: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 25: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 26: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 27: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 28: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > World War II and the Manhattan Project\n",
      "    Input: • Albert Einstein called war a disease and advocated for resistance to war. • Al...\n",
      "    [Calling LLM with tools...] ✓ (10 triples, 10 iters, 19.8s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 29: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > US citizenship\n",
      "    Input: • Albert Einstein became a naturalized citizen of the United States in 1940. • A...\n",
      "    [Calling LLM with tools...] ✓ (14 triples, 8 iters, 30.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 30: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > US citizenship\n",
      "    Input: • Albert Einstein became an American citizen in 1940 • Albert Einstein settled i...\n",
      "    [Calling LLM with tools...] ✓ (33 triples, 21 iters, 42.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 31: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > US citizenship\n",
      "    Input: • In 1946, Albert Einstein visited Lincoln University in Pennsylvania, a histori...\n",
      "    [Calling LLM with tools...] ✓ (34 triples, 9 iters, 41.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 32: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views\n",
      "    Input: • Albert Einstein was one of the signatories of the founding proclamation of the...\n",
      "    [Calling LLM with tools...] ✓ (36 triples, 11 iters, 43.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 33: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Political views\n",
      "    Input: • Albert Einstein honored Vladimir Lenin as a man who sacrificed himself for soc...\n",
      "    [Calling LLM with tools...] ✓ (23 triples, 7 iters, 34.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 34: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Relationship with Zionism\n",
      "    Input: • Albert Einstein was a figurehead leader in the establishment of the Hebrew Uni...\n",
      "    [Calling LLM with tools...] ✓ (90 triples, 7 iters, 90.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 35: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Religious and philosophical views\n",
      "    Input: • Albert Einstein said he had sympathy for the impersonal pantheistic God of Bar...\n",
      "    [Calling LLM with tools...] ✓ (26 triples, 14 iters, 33.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 36: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Religious and philosophical views\n",
      "    Input: • Albert Einstein believed that the word God represented human weaknesses and th...\n",
      "    [Calling LLM with tools...] ✓ (52 triples, 9 iters, 59.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 37: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Love of music\n",
      "    Input: • Albert Einstein said \"If I were not a physicist, I would probably be a musicia...\n",
      "    [Calling LLM with tools...] ✗ ValidationError after 24.2s\n",
      "    Error: 1 validation error for emit_triple\n",
      "object_value\n",
      "  Input should be a valid string [type=string_type, \n",
      "    [Saving notebook...] saved\n",
      "  Facts 38: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Love of music\n",
      "    Input: • Music became a pivotal and permanent part of Albert Einstein's life. • Albert ...\n",
      "    [Calling LLM with tools...] ✓ (54 triples, 43 iters, 79.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 39: + Generating\n",
      "    Context: Albert Einstein > Life and career > Death\n",
      "    Input: • Albert Einstein died on 17 April 1955 at the age of 76. • Einstein died from i...\n",
      "    [Calling LLM with tools...] ✗ ValidationError after 28.5s\n",
      "    Error: 1 validation error for emit_triples\n",
      "triples\n",
      "  Input should be a valid list [type=list_type, input_va\n",
      "    [Saving notebook...] saved\n",
      "  Facts 40: + Generating\n",
      "    Context: Albert Einstein > Scientific career\n",
      "    Input: • Albert Einstein published more than 300 scientific papers and 150 non-scientif...\n",
      "    [Calling LLM with tools...] ✗ ValidationError after 100.9s\n",
      "    Error: 1 validation error for emit_triple\n",
      "object_value\n",
      "  Input should be a valid string [type=string_type, \n",
      "    [Saving notebook...] saved\n",
      "  Facts 41: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Statistical mechanics > Theory of critical opalescence\n",
      "    Input: • Albert Einstein returned to the problem of thermodynamic fluctuations and gave...\n",
      "    [Calling LLM with tools...] ✓ (64 triples, 11 iters, 64.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 42: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Special relativity\n",
      "    Input: • Albert Einstein's paper \"On the Electrodynamics of Moving Bodies\" was received...\n",
      "    [Calling LLM with tools...] ✓ (55 triples, 13 iters, 70.5s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 43: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity\n",
      "    Input: • General relativity is a theory of gravitation developed by Albert Einstein bet...\n",
      "    [Calling LLM with tools...] ✓ (87 triples, 17 iters, 61.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 44: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Gravitational waves\n",
      "    Input: • In 1916, Albert Einstein predicted gravitational waves as ripples in the curva...\n",
      "    [Calling LLM with tools...] ✓ (48 triples, 7 iters, 52.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 45: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Physical cosmology\n",
      "    Input: • In 1917, Albert Einstein applied the general theory of relativity to the struc...\n",
      "    [Calling LLM with tools...] ✓ (22 triples, 8 iters, 38.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 46: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Physical cosmology\n",
      "    Input: • In late 2013, a team led by the Irish physicist Cormac O'Raifeartaigh discover...\n",
      "    [Calling LLM with tools...] ✓ (32 triples, 11 iters, 43.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 47: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Energy momentum pseudotensor\n",
      "    Input: • Albert Einstein argued that energy and momentum in a gravitational field canno...\n",
      "    [Calling LLM with tools...] ✓ (26 triples, 14 iters, 35.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 48: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Einstein–Cartan theory\n",
      "    Input: • Albert Einstein and Cartan made a modification to general relativity in the 19...\n",
      "    [Calling LLM with tools...] ✓ (28 triples, 10 iters, 38.5s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 49: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Equations of motion\n",
      "    Input: • Albert Einstein proposed that the field equations of general relativity would ...\n",
      "    [Calling LLM with tools...] ✓ (47 triples, 18 iters, 44.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 50: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Old quantum theory\n",
      "    Input: • In 1905, Albert Einstein postulated that light consists of localized particles...\n",
      "    [Calling LLM with tools...] ✗ ValidationError after 71.7s\n",
      "    Error: 1 validation error for emit_triples\n",
      "triples\n",
      "  Input should be a valid list [type=list_type, input_va\n",
      "    [Saving notebook...] saved\n",
      "  Facts 51: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Old quantum theory > Bose–Einstein statistics\n",
      "    Input: - In 1924, Albert Einstein received a description of a statistical model from In...\n",
      "    [Calling LLM with tools...] ✓ (63 triples, 8 iters, 79.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 52: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Old quantum theory > Zero-point energy\n",
      "    Input: • Albert Einstein worked with Otto Stern on zero-point energy theory from 1911 t...\n",
      "    [Calling LLM with tools...] ✓ (26 triples, 6 iters, 35.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 53: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Quantum mechanics\n",
      "    Input: • Albert Einstein played a major role in developing quantum theory beginning wit...\n",
      "    [Calling LLM with tools...] ✓ (30 triples, 16 iters, 34.3s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 54: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Quantum mechanics > Einstein–Podolsky–Rosen paradox\n",
      "    Input: • Albert Einstein never fully accepted quantum mechanics. • Albert Einstein reco...\n",
      "    [Calling LLM with tools...] ✓ (43 triples, 13 iters, 39.8s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 55: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Quantum mechanics > Einstein–Podolsky–Rosen paradox\n",
      "    Input: • Albert Einstein published a paper in 1935 with Boris Podolsky and Nathan Rosen...\n",
      "    [Calling LLM with tools...] ✓ (23 triples, 6 iters, 31.3s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 56: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Quantum mechanics > Einstein–Podolsky–Rosen paradox\n",
      "    Input: • John Stewart Bell carried the analysis of quantum entanglement much further in...\n",
      "    [Calling LLM with tools...] ✓ (69 triples, 13 iters, 98.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 57: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Unified field theory\n",
      "    Input: • Albert Einstein sought an ambitious geometrical theory that would treat gravit...\n",
      "    [Calling LLM with tools...] ✓ (24 triples, 15 iters, 32.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 58: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Collaboration with other scientists > Einstein–de Haas experiment\n",
      "    Input: - Owen Willans Richardson predicted in 1908 that a change in the magnetic moment...\n",
      "    [Calling LLM with tools...] ⚠ (160 triples, 150 iters [MAX], 225.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 59: + Generating\n",
      "    Context: Albert Einstein > Legacy\n",
      "    Input: - Albert Einstein wrote daily letters to his wife Elsa and adopted stepdaughters...\n",
      "    [Calling LLM with tools...] ✓ (54 triples, 11 iters, 85.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 60: + Generating\n",
      "    Context: Albert Einstein > Legacy > Scientific\n",
      "    Input: • In 1999, a survey of top physicists voted Albert Einstein as the \"greatest phy...\n",
      "    [Calling LLM with tools...] ✓ (16 triples, 5 iters, 27.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 61: + Generating\n",
      "    Context: Albert Einstein > In popular culture\n",
      "    Input: • Albert Einstein became one of the most famous scientific celebrities after the...\n",
      "    [Calling LLM with tools...] ✓ (98 triples, 56 iters, 105.3s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 62: + Generating\n",
      "    Context: Albert Einstein > Publications\n",
      "    Input: - Albert Einstein wrote a letter to the editors of *The New York Times* on 4 Dec...\n",
      "    [Calling LLM with tools...] ✓ (52 triples, 18 iters, 46.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 63: + Generating\n",
      "    Context: Albert Einstein > External links > Archival materials collections\n",
      "    Input: • Albert Einstein's Historical Letters, Documents & Papers are held at the Shape...\n",
      "    [Calling LLM with tools...] ✓ (78 triples, 8 iters, 71.6s)\n",
      "    [Saving notebook...] saved\n",
      "--------------------------------------------------\n",
      "RDF generation complete:\n",
      "  - 32 cells processed\n",
      "  - 1517 total triples emitted\n",
      "  - 27 skipped (up-to-date)\n",
      "  - 4 errors/timeouts\n",
      "\n",
      "Iteration statistics (max=150):\n",
      "  - Min: 5\n",
      "  - Max: 150\n",
      "  - Mean: 17.9\n",
      "  - Hit limit: 1 cells\n",
      "  - Distribution: {'1-5': 1, '6-10': 13, '11-20': 14, '21-50': 2, '51-100': 1, '100+': 1}\n"
     ]
    }
   ],
   "source": [
    "# Load schema matcher with pre-built vocabulary index\n",
    "from schema_matcher import SchemaMatcher\n",
    "from langchain_core.tools import tool\n",
    "from typing import List\n",
    "\n",
    "VOCAB_CACHE_DIR = \"data/vocab_cache\"\n",
    "schema_matcher = SchemaMatcher.load(VOCAB_CACHE_DIR, embed_base_url=LLM_CONFIG[\"base_url\"])\n",
    "log_progress(f\"Loaded schema matcher with {len(schema_matcher.vocabularies)} vocabularies\")\n",
    "\n",
    "# Collector for emitted triples (reset per cell)\n",
    "emitted_triples: List[dict] = []\n",
    "\n",
    "# Iteration tracking for statistics\n",
    "iteration_counts: List[int] = []  # Track iterations per cell\n",
    "\n",
    "# === Schema lookup tools ===\n",
    "\n",
    "@tool\n",
    "def find_rdf_class(description: str) -> str:\n",
    "    \"\"\"Find the best schema.org class/type for an entity based on a natural language description.\n",
    "    \n",
    "    Use this when you need to determine the rdf:type of an entity.\n",
    "    Example: find_rdf_class(\"a person who does scientific research\")\n",
    "    \n",
    "    Args:\n",
    "        description: Natural language description of the entity type\n",
    "    \n",
    "    Returns:\n",
    "        Top matching schema.org classes with URIs and descriptions\n",
    "    \"\"\"\n",
    "    results = schema_matcher.find_class(description, top_k=5)\n",
    "    if not results:\n",
    "        return \"No matches found. Use a generic type like schema:Thing\"\n",
    "    \n",
    "    lines = [\"Top matching classes:\"]\n",
    "    for r in results:\n",
    "        lines.append(f\"  {r['prefix']} ({r['score']:.2f})\")\n",
    "        lines.append(f\"    URI: {r['uri']}\")\n",
    "        if r['description']:\n",
    "            lines.append(f\"    Description: {r['description'][:100]}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "@tool\n",
    "def find_rdf_property(description: str, subject_type: str = \"\", object_type: str = \"\") -> str:\n",
    "    \"\"\"Find the best schema.org property/predicate for a relationship.\n",
    "    \n",
    "    Use this when you need to find the right predicate for a triple.\n",
    "    Example: find_rdf_property(\"the date when someone was born\", subject_type=\"Person\")\n",
    "    \n",
    "    Args:\n",
    "        description: Natural language description of the relationship\n",
    "        subject_type: Optional - the type of the subject (e.g., \"Person\", \"Organization\")  \n",
    "        object_type: Optional - the type of the object/value (e.g., \"Date\", \"Place\")\n",
    "    \n",
    "    Returns:\n",
    "        Top matching schema.org properties with URIs, domains, and ranges\n",
    "    \"\"\"\n",
    "    results = schema_matcher.find_property(\n",
    "        description, \n",
    "        subject_type=subject_type or None,\n",
    "        object_type=object_type or None,\n",
    "        top_k=5\n",
    "    )\n",
    "    if not results:\n",
    "        return \"No matches found. Consider using rdfs:label or a descriptive URI fragment.\"\n",
    "    \n",
    "    lines = [\"Top matching properties:\"]\n",
    "    for r in results:\n",
    "        lines.append(f\"  {r['prefix']} ({r['score']:.2f})\")\n",
    "        lines.append(f\"    URI: {r['uri']}\")\n",
    "        if r['domain']:\n",
    "            lines.append(f\"    Domain: {r['domain']}\")\n",
    "        if r['range']:\n",
    "            lines.append(f\"    Range: {r['range']}\")\n",
    "        if r['description']:\n",
    "            lines.append(f\"    Description: {r['description'][:80]}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# === Triple output tools ===\n",
    "\n",
    "@tool\n",
    "def emit_triple(subject: str, predicate: str, object_value: str) -> str:\n",
    "    \"\"\"Emit a single RDF triple. Use this to output each triple you generate.\n",
    "    \n",
    "    Args:\n",
    "        subject: The subject URI (e.g., \"<https://example.org#entity>\") or prefixed (e.g., \"schema:Person\")\n",
    "        predicate: The predicate URI or prefixed term (e.g., \"schema:birthDate\", \"rdf:type\")\n",
    "        object_value: The object - a URI, prefixed term, or literal with datatype \n",
    "                      (e.g., '\"1879-03-14\"^^xsd:date', '\"Albert Einstein\"@en', '<https://...>')\n",
    "    \n",
    "    Returns:\n",
    "        Confirmation message\n",
    "    \"\"\"\n",
    "    emitted_triples.append({\n",
    "        \"subject\": subject,\n",
    "        \"predicate\": predicate,\n",
    "        \"object\": object_value,\n",
    "    })\n",
    "    return f\"Triple recorded: {subject} {predicate} {object_value}\"\n",
    "\n",
    "@tool  \n",
    "def emit_triples(triples: List[dict]) -> str:\n",
    "    \"\"\"Emit multiple RDF triples at once. More efficient than calling emit_triple repeatedly.\n",
    "    \n",
    "    Args:\n",
    "        triples: List of triple dictionaries, each with keys:\n",
    "                 - subject: The subject URI or prefixed term\n",
    "                 - predicate: The predicate URI or prefixed term  \n",
    "                 - object: The object URI, prefixed term, or literal\n",
    "                 \n",
    "    Example:\n",
    "        emit_triples([\n",
    "            {\"subject\": \"<#person_einstein>\", \"predicate\": \"rdf:type\", \"object\": \"schema:Person\"},\n",
    "            {\"subject\": \"<#person_einstein>\", \"predicate\": \"schema:name\", \"object\": '\"Albert Einstein\"'},\n",
    "            {\"subject\": \"<#person_einstein>\", \"predicate\": \"schema:birthDate\", \"object\": '\"1879-03-14\"^^xsd:date'}\n",
    "        ])\n",
    "    \n",
    "    Returns:\n",
    "        Confirmation with count of triples recorded\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for t in triples:\n",
    "        if isinstance(t, dict) and all(k in t for k in (\"subject\", \"predicate\", \"object\")):\n",
    "            emitted_triples.append({\n",
    "                \"subject\": t[\"subject\"],\n",
    "                \"predicate\": t[\"predicate\"],\n",
    "                \"object\": t[\"object\"],\n",
    "            })\n",
    "            count += 1\n",
    "    return f\"Recorded {count} triples\"\n",
    "\n",
    "# Create LLM client with timeout\n",
    "rdf_llm = ChatOpenAI(\n",
    "    model=LLM_CONFIG[\"model\"],\n",
    "    temperature=LLM_CONFIG[\"temperature\"],\n",
    "    base_url=LLM_CONFIG[\"base_url\"],\n",
    "    api_key=\"lm-studio\",  # type: ignore\n",
    "    timeout=CELL_TIMEOUT_SECONDS,\n",
    "    max_retries=0,\n",
    ")\n",
    "\n",
    "# Bind all tools to the LLM\n",
    "rdf_tools = [find_rdf_class, find_rdf_property, emit_triple, emit_triples]\n",
    "rdf_llm_with_tools = rdf_llm.bind_tools(rdf_tools)\n",
    "\n",
    "# Create RDF generation prompt\n",
    "rdf_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert at converting factual statements to RDF triples.\n",
    "\n",
    "You have access to these tools:\n",
    "\n",
    "LOOKUP TOOLS (use first to find correct vocabulary):\n",
    "- find_rdf_class: Find the best schema.org class/type for an entity\n",
    "- find_rdf_property: Find the best predicate for a relationship\n",
    "\n",
    "OUTPUT TOOLS (use to emit your triples):\n",
    "- emit_triple: Output a single triple (subject, predicate, object)\n",
    "- emit_triples: Output multiple triples at once (more efficient)\n",
    "\n",
    "WORKFLOW:\n",
    "1. First use find_rdf_class and find_rdf_property to look up appropriate schema.org terms\n",
    "2. Then use emit_triple or emit_triples to output each RDF triple\n",
    "3. Finally, provide a brief summary noting any quality concerns\n",
    "\n",
    "IMPORTANT:\n",
    "- Do NOT write Turtle syntax in your response - use the emit tools instead\n",
    "- Use schema.org terms you looked up (e.g., schema:Person, schema:birthDate)\n",
    "- For URIs, use angle brackets: <https://...> or fragment references: <#entity_id>\n",
    "- For literals, use quotes with optional datatype: \"value\"^^xsd:date or \"text\"@en\n",
    "- For prefixed terms as objects, just use the prefix: schema:Person\"\"\"),\n",
    "    (\"human\", \"\"\"Convert these factual statements to RDF triples.\n",
    "\n",
    "Source: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Entity Registry (use these URIs for known entities):\n",
    "{entity_registry}\n",
    "\n",
    "Factual statements to convert:\n",
    "{facts}\n",
    "\n",
    "Use the lookup tools to find schema.org terms, then emit your triples. End with a brief summary.\"\"\"),\n",
    "])\n",
    "\n",
    "def triples_to_turtle(triples: List[dict]) -> str:\n",
    "    \"\"\"Convert collected triples to Turtle format.\"\"\"\n",
    "    if not triples:\n",
    "        return \"# No triples emitted\"\n",
    "    \n",
    "    lines = []\n",
    "    for t in triples:\n",
    "        lines.append(f\"{t['subject']} {t['predicate']} {t['object']} .\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Tool-calling loop that collects emitted triples\n",
    "def call_llm_with_tools(prompt_vars: dict, max_iterations: int = MAX_ITERATIONS) -> tuple[str, List[dict], int]:\n",
    "    \"\"\"Call LLM with tool support. Returns (summary_response, collected_triples, iterations_used).\"\"\"\n",
    "    global emitted_triples\n",
    "    emitted_triples = []  # Reset collector\n",
    "    \n",
    "    messages = rdf_prompt.format_messages(**prompt_vars)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        response = rdf_llm_with_tools.invoke(messages)\n",
    "        \n",
    "        # Check if there are tool calls\n",
    "        if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "            # Add assistant message with tool calls\n",
    "            messages.append(response)\n",
    "            \n",
    "            # Execute each tool call\n",
    "            for tool_call in response.tool_calls:\n",
    "                tool_name = tool_call['name']\n",
    "                tool_args = tool_call['args']\n",
    "                \n",
    "                # Find and execute the tool\n",
    "                if tool_name == 'find_rdf_class':\n",
    "                    tool_result = find_rdf_class.invoke(tool_args)\n",
    "                elif tool_name == 'find_rdf_property':\n",
    "                    tool_result = find_rdf_property.invoke(tool_args)\n",
    "                elif tool_name == 'emit_triple':\n",
    "                    tool_result = emit_triple.invoke(tool_args)\n",
    "                elif tool_name == 'emit_triples':\n",
    "                    tool_result = emit_triples.invoke(tool_args)\n",
    "                else:\n",
    "                    tool_result = f\"Unknown tool: {tool_name}\"\n",
    "                \n",
    "                # Add tool result message\n",
    "                from langchain_core.messages import ToolMessage\n",
    "                messages.append(ToolMessage(content=str(tool_result), tool_call_id=tool_call['id']))\n",
    "        else:\n",
    "            # No tool calls, return the summary and collected triples\n",
    "            summary = response.content if hasattr(response, 'content') else str(response)\n",
    "            return summary, list(emitted_triples), iteration + 1\n",
    "    \n",
    "    # Max iterations reached\n",
    "    summary = response.content if hasattr(response, 'content') else \"Max iterations reached\"\n",
    "    return summary, list(emitted_triples), max_iterations\n",
    "\n",
    "def format_entity_registry_for_prompt(registry: EntityRegistry) -> str:\n",
    "    \"\"\"Format registry for RDF prompt.\"\"\"\n",
    "    lines = []\n",
    "    for entity in registry.entities.values():\n",
    "        lines.append(f\"<{entity['uri']}> # {entity['label']} ({entity['type']})\")\n",
    "    return \"\\n\".join(lines) if lines else \"# No entities registered yet\"\n",
    "\n",
    "# Read facts notebook to get source content and CIDs\n",
    "log_progress(\"Reading facts notebook...\")\n",
    "facts_nb = nbformat.read(facts_path, as_version=4)\n",
    "facts_signatures = extract_signatures(facts_nb)\n",
    "\n",
    "# Build list of facts content with CIDs\n",
    "facts_data = []\n",
    "cell_idx = 2  # Skip provenance and registry\n",
    "while cell_idx < len(facts_nb.cells):\n",
    "    cell = facts_nb.cells[cell_idx]\n",
    "    if cell.cell_type == 'markdown':\n",
    "        content = cell.source\n",
    "        # Get corresponding signature (next cell)\n",
    "        sig = None\n",
    "        if cell_idx + 1 < len(facts_nb.cells):\n",
    "            sig = parse_signature(facts_nb.cells[cell_idx + 1].source)\n",
    "        \n",
    "        # Extract breadcrumb\n",
    "        context_match = re.search(r'\\*\\*Context:\\*\\*\\s*(.+)', content)\n",
    "        breadcrumb = context_match.group(1) if context_match else \"Unknown\"\n",
    "        \n",
    "        # Extract facts (after ---)\n",
    "        parts = content.split(\"---\\n\", 1)\n",
    "        facts_text = parts[1].strip() if len(parts) > 1 else content\n",
    "        \n",
    "        facts_data.append({\n",
    "            \"cell_num\": sig[\"cell\"] if sig else len(facts_data) + 1,\n",
    "            \"content\": content,\n",
    "            \"facts_text\": facts_text,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"cid\": sig[\"cid\"] if sig else compute_cid(content),\n",
    "        })\n",
    "        cell_idx += 2  # Skip content and signature\n",
    "    else:\n",
    "        cell_idx += 1\n",
    "\n",
    "log_progress(f\"Found {len(facts_data)} facts cells with CIDs\")\n",
    "\n",
    "# Read existing RDF notebook and extract signatures\n",
    "rdf_nb = nbformat.read(rdf_path, as_version=4)\n",
    "rdf_signatures = extract_signatures(rdf_nb)\n",
    "\n",
    "log_progress(f\"Found {len(rdf_signatures)} existing RDF signatures\")\n",
    "log_progress(f\"HTTP timeout: {CELL_TIMEOUT_SECONDS}s per request\")\n",
    "log_progress(f\"Max iterations: {MAX_ITERATIONS} per cell\")\n",
    "log_progress(\"-\" * 50)\n",
    "\n",
    "# Process each facts cell\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "total_triples = 0\n",
    "iteration_counts = []  # Reset for this run\n",
    "max_iterations_hit = 0  # Count cells that hit the limit\n",
    "\n",
    "for facts_item in facts_data:\n",
    "    cell_num = facts_item[\"cell_num\"]\n",
    "    source_cid = facts_item[\"cid\"]\n",
    "    \n",
    "    # Skip cells that had errors in facts extraction\n",
    "    if facts_item[\"facts_text\"].startswith(\"# Error:\"):\n",
    "        log_progress(f\"  Facts {cell_num}: ⊘ Source had error, skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Check if we already have up-to-date RDF for these facts\n",
    "    existing_sig = rdf_signatures.get(cell_num)\n",
    "    if existing_sig and existing_sig[\"from_cid\"] == source_cid:\n",
    "        log_progress(f\"  Facts {cell_num}: ⊘ Up-to-date (CID match), skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Need to generate (or regenerate) this cell\n",
    "    status = \"↻ Regenerating\" if existing_sig else \"+ Generating\"\n",
    "    facts_preview = facts_item[\"facts_text\"][:80].replace('\\n', ' ')\n",
    "    log_progress(f\"  Facts {cell_num}: {status}\")\n",
    "    log_progress(f\"    Context: {facts_item['breadcrumb']}\")\n",
    "    log_progress(f\"    Input: {facts_preview}...\")\n",
    "    log_progress(f\"    [Calling LLM with tools...]\", end=\" \")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Call LLM with tool support - now returns (summary, triples, iterations)\n",
    "    try:\n",
    "        summary, triples, iterations_used = call_llm_with_tools({\n",
    "            \"source_url\": provenance[\"source_url\"],\n",
    "            \"breadcrumb\": facts_item[\"breadcrumb\"],\n",
    "            \"entity_registry\": format_entity_registry_for_prompt(registry),\n",
    "            \"facts\": facts_item[\"facts_text\"],\n",
    "        })\n",
    "        elapsed = time.time() - start_time\n",
    "        triple_count = len(triples)\n",
    "        total_triples += triple_count\n",
    "        iteration_counts.append(iterations_used)\n",
    "        \n",
    "        # Check if we hit the limit\n",
    "        if iterations_used >= MAX_ITERATIONS:\n",
    "            max_iterations_hit += 1\n",
    "            log_progress(f\"⚠ ({triple_count} triples, {iterations_used} iters [MAX], {elapsed:.1f}s)\")\n",
    "        else:\n",
    "            log_progress(f\"✓ ({triple_count} triples, {iterations_used} iters, {elapsed:.1f}s)\")\n",
    "        \n",
    "        # Convert triples to Turtle\n",
    "        rdf_content = triples_to_turtle(triples)\n",
    "        \n",
    "        # Include summary as comment if it has content\n",
    "        if summary and summary.strip():\n",
    "            # Clean up summary for comment\n",
    "            summary_lines = summary.strip().split('\\n')\n",
    "            summary_comment = '\\n'.join(f\"# {line}\" for line in summary_lines[:5])\n",
    "            rdf_content = f\"{summary_comment}\\n\\n{rdf_content}\"\n",
    "        \n",
    "        processed_count += 1\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)[:100]\n",
    "        rdf_content = f\"# Error: {error_type}: {e}\"\n",
    "        log_progress(f\"✗ {error_type} after {elapsed:.1f}s\")\n",
    "        log_progress(f\"    Error: {error_msg}\")\n",
    "        error_count += 1\n",
    "    \n",
    "    # Build the RDF cell content\n",
    "    rdf_cell_content = f\"\"\"# Context: {facts_item['breadcrumb']}\n",
    "# Cell: {cell_num} of {len(facts_data)}\n",
    "\n",
    "{rdf_content}\n",
    "\"\"\"\n",
    "    rdf_cid = compute_cid(rdf_cell_content)\n",
    "    signature = make_signature(cell_num, \"rdf\", rdf_cid, source_cid)\n",
    "    \n",
    "    # Find where to insert/update in the notebook\n",
    "    if existing_sig:\n",
    "        # Find and remove the old cells\n",
    "        new_cells = [rdf_nb.cells[0], rdf_nb.cells[1]]  # Keep header\n",
    "        i = 2\n",
    "        while i < len(rdf_nb.cells):\n",
    "            cell = rdf_nb.cells[i]\n",
    "            if cell.cell_type == 'raw':\n",
    "                sig = parse_signature(cell.source)\n",
    "                if sig and sig[\"cell\"] == cell_num:\n",
    "                    i += 1\n",
    "                    continue\n",
    "            if i > 0 and i + 1 < len(rdf_nb.cells):\n",
    "                next_sig = parse_signature(rdf_nb.cells[i + 1].source) if rdf_nb.cells[i + 1].cell_type == 'raw' else None\n",
    "                if next_sig and next_sig[\"cell\"] == cell_num:\n",
    "                    i += 2\n",
    "                    continue\n",
    "            new_cells.append(cell)\n",
    "            i += 1\n",
    "        rdf_nb.cells = new_cells\n",
    "    \n",
    "    # Append new content and signature\n",
    "    rdf_nb.cells.append(new_raw_cell(rdf_cell_content))\n",
    "    rdf_nb.cells.append(new_raw_cell(json.dumps(signature)))\n",
    "    \n",
    "    # Update signatures dict\n",
    "    rdf_signatures[cell_num] = signature\n",
    "    \n",
    "    # Save notebook after each cell\n",
    "    log_progress(f\"    [Saving notebook...]\", end=\" \")\n",
    "    with open(rdf_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(rdf_nb, f)\n",
    "    log_progress(\"saved\")\n",
    "\n",
    "log_progress(\"-\" * 50)\n",
    "log_progress(f\"RDF generation complete:\")\n",
    "log_progress(f\"  - {processed_count} cells processed\")\n",
    "log_progress(f\"  - {total_triples} total triples emitted\")\n",
    "log_progress(f\"  - {skipped_count} skipped (up-to-date)\")\n",
    "log_progress(f\"  - {error_count} errors/timeouts\")\n",
    "\n",
    "# Iteration statistics\n",
    "if iteration_counts:\n",
    "    log_progress(f\"\\nIteration statistics (max={MAX_ITERATIONS}):\")\n",
    "    log_progress(f\"  - Min: {min(iteration_counts)}\")\n",
    "    log_progress(f\"  - Max: {max(iteration_counts)}\")\n",
    "    log_progress(f\"  - Mean: {sum(iteration_counts)/len(iteration_counts):.1f}\")\n",
    "    log_progress(f\"  - Hit limit: {max_iterations_hit} cells\")\n",
    "    \n",
    "    # Distribution buckets\n",
    "    buckets = {\"1-5\": 0, \"6-10\": 0, \"11-20\": 0, \"21-50\": 0, \"51-100\": 0, \"100+\": 0}\n",
    "    for count in iteration_counts:\n",
    "        if count <= 5:\n",
    "            buckets[\"1-5\"] += 1\n",
    "        elif count <= 10:\n",
    "            buckets[\"6-10\"] += 1\n",
    "        elif count <= 20:\n",
    "            buckets[\"11-20\"] += 1\n",
    "        elif count <= 50:\n",
    "            buckets[\"21-50\"] += 1\n",
    "        elif count <= 100:\n",
    "            buckets[\"51-100\"] += 1\n",
    "        else:\n",
    "            buckets[\"100+\"] += 1\n",
    "    \n",
    "    log_progress(f\"  - Distribution: {buckets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de69a28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF notebook: data/albert_einstein/20251218_215312/rdf.ipynb\n",
      "  - Updated incrementally during processing\n",
      "  - Ready for review/editing before final export\n"
     ]
    }
   ],
   "source": [
    "# Summary of RDF generation (notebook already updated incrementally)\n",
    "print(f\"RDF notebook: {rdf_path}\")\n",
    "print(f\"  - Updated incrementally during processing\")\n",
    "print(f\"  - Ready for review/editing before final export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a59d9",
   "metadata": {},
   "source": [
    "## Export Combined RDF\n",
    "\n",
    "Combine all RDF cells into a single Turtle file with prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d061083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported RDF to: data/albert_einstein/20251218_215312/triples.ttl\n",
      "  - 62 chunks of triples\n",
      "  - 409229 characters total\n"
     ]
    }
   ],
   "source": [
    "# Combine all RDF into a single Turtle file\n",
    "all_triples = []\n",
    "\n",
    "# Read the updated RDF notebook\n",
    "rdf_nb = nbformat.read(rdf_path, as_version=4)\n",
    "\n",
    "# Collect RDF from all raw cells (skip provenance, registry, and signature cells)\n",
    "for cell in rdf_nb.cells[2:]:\n",
    "    if cell.cell_type == 'raw':\n",
    "        content = cell.source.strip()\n",
    "        \n",
    "        # Skip signature cells (JSON objects)\n",
    "        if content.startswith('{') and '\"cid\"' in content:\n",
    "            continue\n",
    "        \n",
    "        # Skip empty or error-only cells\n",
    "        if not content or content.startswith('# Error:'):\n",
    "            continue\n",
    "        \n",
    "        # Skip comment-only cells\n",
    "        lines = [line for line in content.split('\\n') \n",
    "                 if line.strip() and not line.strip().startswith('#')]\n",
    "        if lines:\n",
    "            all_triples.append(content)\n",
    "\n",
    "# Format prefixes for the Turtle file\n",
    "turtle_prefixes = RDF_PREFIXES.format(source_url=provenance['source_url'])\n",
    "\n",
    "# Build complete Turtle file\n",
    "turtle_output = f\"\"\"# RDF Knowledge Graph: {provenance['article_title']}\n",
    "# Source: {provenance['source_url']}\n",
    "# License: {provenance['license']}\n",
    "# Generated: {datetime.now().isoformat()}\n",
    "\n",
    "{turtle_prefixes}\n",
    "\n",
    "# === Triples ===\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "turtle_output += \"\\n\\n\".join(all_triples)\n",
    "\n",
    "# Save to file with simple filename (timestamp is in directory)\n",
    "turtle_path = os.path.join(RUN_OUTPUT_DIR, \"triples.ttl\")\n",
    "with open(turtle_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(turtle_output)\n",
    "\n",
    "print(f\"Exported RDF to: {turtle_path}\")\n",
    "print(f\"  - {len(all_triples)} chunks of triples\")\n",
    "print(f\"  - {len(turtle_output)} characters total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c4e37",
   "metadata": {},
   "source": [
    "## Save Initial Entity Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d416a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/albert_einstein/20251218_215312/registry.json\n",
      "\n",
      "Initial entities: 1\n",
      "  - Albert Einstein (Person): https://en.wikipedia.org/wiki/Albert_Einstein#person_albert_einstein\n"
     ]
    }
   ],
   "source": [
    "# Save entity registry to JSON file with simple filename (timestamp is in directory)\n",
    "registry_path = os.path.join(RUN_OUTPUT_DIR, \"registry.json\")\n",
    "with open(registry_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(registry.to_json())\n",
    "\n",
    "print(f\"Saved: {registry_path}\")\n",
    "print(f\"\\nInitial entities: {len(registry.entities)}\")\n",
    "for key, entity in registry.entities.items():\n",
    "    print(f\"  - {entity['label']} ({entity['type']}): {entity['uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2368a",
   "metadata": {},
   "source": [
    "## Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b31bf97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE COMPLETE\n",
      "============================================================\n",
      "\n",
      "Article: Albert Einstein\n",
      "Source: https://en.wikipedia.org/wiki/Albert_Einstein\n",
      "License: CC BY-SA 4.0\n",
      "Run timestamp: 20251218_215312\n",
      "\n",
      "Output directory: data/albert_einstein/20251218_215312\n",
      "\n",
      "Generated artifacts:\n",
      "  1. chunks.ipynb\n",
      "     - 63 chunks with breadcrumb context\n",
      "  2. facts.ipynb\n",
      "     - Facts extracted from chunks\n",
      "  3. rdf.ipynb\n",
      "     - RDF triples generated via tool calls\n",
      "  4. triples.ttl\n",
      "     - Combined Turtle file for import\n",
      "  5. registry.json\n",
      "     - Entity registry snapshot\n",
      "\n",
      "The intermediate notebooks can be reviewed and edited before re-export.\n",
      "Files can reference each other with relative paths within the run directory.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nArticle: {ARTICLE_TITLE}\")\n",
    "print(f\"Source: {source_url}\")\n",
    "print(f\"License: {provenance['license']}\")\n",
    "print(f\"Run timestamp: {RUN_TIMESTAMP}\")\n",
    "print(f\"\\nOutput directory: {RUN_OUTPUT_DIR}\")\n",
    "print(f\"\\nGenerated artifacts:\")\n",
    "print(f\"  1. chunks.ipynb\")\n",
    "print(f\"     - {len(contextual_chunks)} chunks with breadcrumb context\")\n",
    "print(f\"  2. facts.ipynb\")\n",
    "print(f\"     - Facts extracted from chunks\")\n",
    "print(f\"  3. rdf.ipynb\")\n",
    "print(f\"     - RDF triples generated via tool calls\")\n",
    "print(f\"  4. triples.ttl\")\n",
    "print(f\"     - Combined Turtle file for import\")\n",
    "print(f\"  5. registry.json\")\n",
    "print(f\"     - Entity registry snapshot\")\n",
    "print(f\"\\nThe intermediate notebooks can be reviewed and edited before re-export.\")\n",
    "print(f\"Files can reference each other with relative paths within the run directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef4bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
