{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236e29d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# mamba install -c conda-forge langchain langchain-text-splitters langchain-openai langchain-community wikipedia pydantic nbformat nbconvert\n",
    "%pip install -q langchain langchain-text-splitters langchain-openai langchain-community wikipedia pydantic nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833f695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded (with CID support)\n",
      "Cell timeout: 60s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "from urllib.parse import quote\n",
    "\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell, new_raw_cell\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# === Content ID (CID) Functions ===\n",
    "\n",
    "def compute_cid(content: str) -> str:\n",
    "    \"\"\"Compute SHA256 content ID for a string.\"\"\"\n",
    "    return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "def make_signature(cell_num: int, cell_type: str, cid: str, from_cid: str) -> dict:\n",
    "    \"\"\"Create a signature dict for a generated cell.\"\"\"\n",
    "    return {\n",
    "        \"cell\": cell_num,\n",
    "        \"type\": cell_type,\n",
    "        \"cid\": cid,\n",
    "        \"from_cid\": from_cid,\n",
    "    }\n",
    "\n",
    "def parse_signature(raw_content: str) -> Optional[dict]:\n",
    "    \"\"\"Parse a signature from raw cell content. Returns None if not a valid signature.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(raw_content.strip())\n",
    "        if all(k in data for k in (\"cell\", \"type\", \"cid\", \"from_cid\")):\n",
    "            return data\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def extract_signatures(notebook) -> dict:\n",
    "    \"\"\"Extract all signatures from a notebook, keyed by cell number.\"\"\"\n",
    "    signatures = {}\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == 'raw':\n",
    "            sig = parse_signature(cell.source)\n",
    "            if sig:\n",
    "                signatures[sig[\"cell\"]] = sig\n",
    "    return signatures\n",
    "\n",
    "# === Timeout Configuration ===\n",
    "# Note: SIGALRM doesn't work for blocking I/O (HTTP requests)\n",
    "# Instead, we use the timeout parameter on the ChatOpenAI client\n",
    "\n",
    "CELL_TIMEOUT_SECONDS = 60  # 1 minute per cell max\n",
    "\n",
    "def log_progress(msg: str, end=\"\\n\"):\n",
    "    \"\"\"Print with immediate flush for real-time progress.\"\"\"\n",
    "    print(msg, end=end, flush=True)\n",
    "\n",
    "print(\"Dependencies loaded (with CID support)\")\n",
    "print(f\"Cell timeout: {CELL_TIMEOUT_SECONDS}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252ef95",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f79c403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline configured for: Albert Einstein\n"
     ]
    }
   ],
   "source": [
    "# Pipeline configuration\n",
    "ARTICLE_TITLE = \"Albert Einstein\"\n",
    "OUTPUT_DIR = \"data\"\n",
    "CHUNK_SIZE = 2000\n",
    "CHUNK_OVERLAP = 128\n",
    "\n",
    "# LLM configuration (shared across stages)\n",
    "LLM_CONFIG = {\n",
    "    \"provider\": \"lm_studio\",  # or \"openai\"\n",
    "    \"model\": \"qwen/qwen3-coder-30b\",\n",
    "    \"temperature\": 1,\n",
    "    \"base_url\": os.environ.get(\"LM_STUDIO_BASE_URL\", \"http://host.docker.internal:1234/v1\"),\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Pipeline configured for: {ARTICLE_TITLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df90b5b",
   "metadata": {},
   "source": [
    "## Entity Registry\n",
    "\n",
    "Manages entity identity across chunks with stable URIs derived from source URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a66f9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntityRegistry class defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class EntityRegistry:\n",
    "    \"\"\"Tracks entities with stable IDs derived from source URL.\"\"\"\n",
    "    source_url: str\n",
    "    entities: dict = field(default_factory=dict)  # normalized_key -> entity\n",
    "    aliases: dict = field(default_factory=dict)   # alias -> canonical_key\n",
    "    \n",
    "    def normalize_key(self, label: str) -> str:\n",
    "        \"\"\"Create consistent key from entity label.\"\"\"\n",
    "        return re.sub(r'[^a-z0-9]+', '_', label.lower().strip()).strip('_')\n",
    "    \n",
    "    def generate_uri(self, entity_type: str, label: str) -> str:\n",
    "        \"\"\"Generate URI based on source URL with fragment identifier.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        # Use source URL as base, add fragment for entity\n",
    "        return f\"{self.source_url}#{entity_type.lower()}_{key}\"\n",
    "    \n",
    "    def generate_id(self, entity_type: str, label: str) -> str:\n",
    "        \"\"\"Generate local ID for internal reference.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        return f\"{entity_type.lower()}_{key}\"\n",
    "    \n",
    "    def register(self, label: str, entity_type: str, description: str = \"\",\n",
    "                 aliases: list = None, source_chunk: int = None) -> str:\n",
    "        \"\"\"Register or update an entity, return canonical ID.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        entity_id = self.generate_id(entity_type, label)\n",
    "        entity_uri = self.generate_uri(entity_type, label)\n",
    "        \n",
    "        if key not in self.entities:\n",
    "            self.entities[key] = {\n",
    "                \"id\": entity_id,\n",
    "                \"uri\": entity_uri,\n",
    "                \"label\": label,\n",
    "                \"type\": entity_type,\n",
    "                \"descriptions\": [description] if description else [],\n",
    "                \"source_chunks\": [source_chunk] if source_chunk is not None else [],\n",
    "                \"aliases\": list(aliases or []),\n",
    "            }\n",
    "        else:\n",
    "            existing = self.entities[key]\n",
    "            if description and description not in existing[\"descriptions\"]:\n",
    "                existing[\"descriptions\"].append(description)\n",
    "            if source_chunk is not None and source_chunk not in existing[\"source_chunks\"]:\n",
    "                existing[\"source_chunks\"].append(source_chunk)\n",
    "            if aliases:\n",
    "                existing[\"aliases\"] = list(set(existing[\"aliases\"]) | set(aliases))\n",
    "        \n",
    "        # Register aliases\n",
    "        for alias in (aliases or []):\n",
    "            self.aliases[self.normalize_key(alias)] = key\n",
    "        \n",
    "        return entity_id\n",
    "    \n",
    "    def lookup(self, label: str) -> Optional[dict]:\n",
    "        \"\"\"Find entity by label or alias.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        canonical_key = self.aliases.get(key, key)\n",
    "        return self.entities.get(canonical_key)\n",
    "    \n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Serialize registry to JSON.\"\"\"\n",
    "        return json.dumps({\n",
    "            \"source_url\": self.source_url,\n",
    "            \"entities\": self.entities,\n",
    "            \"aliases\": self.aliases,\n",
    "        }, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, json_str: str) -> 'EntityRegistry':\n",
    "        \"\"\"Deserialize registry from JSON.\"\"\"\n",
    "        data = json.loads(json_str)\n",
    "        registry = cls(source_url=data[\"source_url\"])\n",
    "        registry.entities = data[\"entities\"]\n",
    "        registry.aliases = data[\"aliases\"]\n",
    "        return registry\n",
    "\n",
    "print(\"EntityRegistry class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094a385",
   "metadata": {},
   "source": [
    "## Section Hierarchy Parser\n",
    "\n",
    "Extracts Wikipedia section structure for breadcrumb context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fe68df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section hierarchy parser defined\n"
     ]
    }
   ],
   "source": [
    "def extract_section_hierarchy(content: str) -> list[dict]:\n",
    "    \"\"\"Parse Wikipedia == headers == into hierarchical structure with positions.\"\"\"\n",
    "    header_pattern = re.compile(r'^(={2,6})\\s*(.+?)\\s*\\1\\s*$', re.MULTILINE)\n",
    "    \n",
    "    sections = []\n",
    "    current_path = []  # Stack of (level, title)\n",
    "    \n",
    "    for match in header_pattern.finditer(content):\n",
    "        level = len(match.group(1))  # Number of '=' signs\n",
    "        title = match.group(2).strip()\n",
    "        \n",
    "        # Pop stack until we're at parent level\n",
    "        while current_path and current_path[-1][0] >= level:\n",
    "            current_path.pop()\n",
    "        \n",
    "        current_path.append((level, title))\n",
    "        breadcrumb = \" > \".join(t for _, t in current_path)\n",
    "        \n",
    "        sections.append({\n",
    "            \"level\": level,\n",
    "            \"title\": title,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"start_pos\": match.start(),\n",
    "            \"end_pos\": match.end(),\n",
    "        })\n",
    "    \n",
    "    return sections\n",
    "\n",
    "\n",
    "def get_section_context(position: int, sections: list[dict], article_title: str) -> dict:\n",
    "    \"\"\"Find the section context for a given character position.\"\"\"\n",
    "    active_section = {\n",
    "        \"title\": \"Introduction\",\n",
    "        \"breadcrumb\": \"Introduction\",\n",
    "        \"level\": 1,\n",
    "    }\n",
    "    \n",
    "    for section in sections:\n",
    "        if section[\"start_pos\"] <= position:\n",
    "            active_section = section\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"section_title\": active_section[\"title\"],\n",
    "        \"breadcrumb\": f\"{article_title} > {active_section['breadcrumb']}\",\n",
    "    }\n",
    "\n",
    "print(\"Section hierarchy parser defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360c37f",
   "metadata": {},
   "source": [
    "## Fetch Wikipedia Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b91833b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched: Albert Einstein\n",
      "Source URL: https://en.wikipedia.org/wiki/Albert_Einstein\n",
      "Content length: 87959 characters\n",
      "License: CC BY-SA 4.0\n"
     ]
    }
   ],
   "source": [
    "# Fetch Wikipedia article\n",
    "loader = WikipediaLoader(query=ARTICLE_TITLE, load_max_docs=1, doc_content_chars_max=100000)\n",
    "docs = loader.load()\n",
    "\n",
    "if not docs:\n",
    "    raise ValueError(f\"Could not fetch article: {ARTICLE_TITLE}\")\n",
    "\n",
    "raw_content = docs[0].page_content\n",
    "metadata = docs[0].metadata\n",
    "\n",
    "# Construct source URL and provenance\n",
    "source_url = f\"https://en.wikipedia.org/wiki/{quote(ARTICLE_TITLE.replace(' ', '_'))}\"\n",
    "\n",
    "provenance = {\n",
    "    \"source_url\": source_url,\n",
    "    \"article_title\": ARTICLE_TITLE,\n",
    "    \"fetched_at\": datetime.now().isoformat(),\n",
    "    \"content_length\": len(raw_content),\n",
    "    # Wikipedia license - standard for all Wikipedia content\n",
    "    \"license\": \"CC BY-SA 4.0\",\n",
    "    \"license_url\": \"https://creativecommons.org/licenses/by-sa/4.0/\",\n",
    "    \"attribution\": \"Wikipedia contributors\",\n",
    "}\n",
    "\n",
    "print(f\"Fetched: {ARTICLE_TITLE}\")\n",
    "print(f\"Source URL: {source_url}\")\n",
    "print(f\"Content length: {len(raw_content)} characters\")\n",
    "print(f\"License: {provenance['license']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1276b5",
   "metadata": {},
   "source": [
    "## Create Contextual Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3146caad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 sections\n",
      "Split into 63 chunks\n",
      "\n",
      "Chunks with context:\n",
      "  Chunk 1: Albert Einstein > Introduction\n",
      "  Chunk 2: Albert Einstein > Introduction\n",
      "  Chunk 3: Albert Einstein > Life and career\n"
     ]
    }
   ],
   "source": [
    "# Parse section hierarchy\n",
    "sections = extract_section_hierarchy(raw_content)\n",
    "print(f\"Found {len(sections)} sections\")\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "raw_chunks = splitter.split_text(raw_content)\n",
    "print(f\"Split into {len(raw_chunks)} chunks\")\n",
    "\n",
    "# Add context to each chunk\n",
    "@dataclass\n",
    "class ContextualChunk:\n",
    "    content: str\n",
    "    chunk_index: int\n",
    "    total_chunks: int\n",
    "    breadcrumb: str\n",
    "    section_title: str\n",
    "    char_start: int\n",
    "    char_end: int\n",
    "\n",
    "contextual_chunks = []\n",
    "current_pos = 0\n",
    "\n",
    "for i, chunk_text in enumerate(raw_chunks):\n",
    "    # Find position in original content\n",
    "    chunk_start = raw_content.find(chunk_text, current_pos)\n",
    "    if chunk_start == -1:\n",
    "        chunk_start = current_pos  # Fallback\n",
    "    chunk_end = chunk_start + len(chunk_text)\n",
    "    \n",
    "    # Get section context\n",
    "    context = get_section_context(chunk_start, sections, ARTICLE_TITLE)\n",
    "    \n",
    "    contextual_chunks.append(ContextualChunk(\n",
    "        content=chunk_text,\n",
    "        chunk_index=i,\n",
    "        total_chunks=len(raw_chunks),\n",
    "        breadcrumb=context[\"breadcrumb\"],\n",
    "        section_title=context[\"section_title\"],\n",
    "        char_start=chunk_start,\n",
    "        char_end=chunk_end,\n",
    "    ))\n",
    "    \n",
    "    current_pos = chunk_start + 1\n",
    "\n",
    "print(f\"\\nChunks with context:\")\n",
    "for chunk in contextual_chunks[:3]:\n",
    "    print(f\"  Chunk {chunk.chunk_index + 1}: {chunk.breadcrumb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3af602",
   "metadata": {},
   "source": [
    "## Initialize Entity Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34b0e8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity registry initialized with subject: Albert Einstein\n",
      "Subject URI: https://en.wikipedia.org/wiki/Albert_Einstein#person_albert_einstein\n"
     ]
    }
   ],
   "source": [
    "# Initialize entity registry with article subject\n",
    "registry = EntityRegistry(source_url=source_url)\n",
    "\n",
    "# Pre-seed with the article subject\n",
    "registry.register(\n",
    "    label=ARTICLE_TITLE,\n",
    "    entity_type=\"Person\",  # Adjust based on article type\n",
    "    description=f\"Subject of Wikipedia article: {ARTICLE_TITLE}\",\n",
    "    aliases=[ARTICLE_TITLE.split()[-1]],  # Last name as alias\n",
    ")\n",
    "\n",
    "print(f\"Entity registry initialized with subject: {ARTICLE_TITLE}\")\n",
    "print(f\"Subject URI: {registry.entities[registry.normalize_key(ARTICLE_TITLE)]['uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd3cdd",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "These prompts are embedded in the generated notebooks for transparency and adjustability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3089027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates defined\n"
     ]
    }
   ],
   "source": [
    "FACTS_EXTRACTION_PROMPT = \"\"\"You are an expert at extracting factual information from text.\n",
    "\n",
    "Given text from a Wikipedia article, extract simple English statements that capture the consensus factual information. Each statement should:\n",
    "- Be a single, clear sentence\n",
    "- Contain one main fact or relationship\n",
    "- Use the full name of entities on first mention\n",
    "- Be verifiable from the source text\n",
    "- Avoid opinions, interpretations, or hedged language\n",
    "\n",
    "The text comes from: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Known entities (use consistent names):\n",
    "{known_entities}\n",
    "\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\n",
    "Extract factual statements as a bulleted list. Also identify any new entities (people, places, organizations, concepts, events, works) that should be added to the registry.\n",
    "\"\"\"\n",
    "\n",
    "RDF_GENERATION_PROMPT = \"\"\"You are an expert at converting factual statements to RDF triples in Turtle format.\n",
    "\n",
    "Convert the following factual statements to RDF using schema.org vocabulary where possible.\n",
    "\n",
    "Source: {source_url}\n",
    "Section: {breadcrumb}\n",
    "\n",
    "Use these prefixes:\n",
    "{prefixes}\n",
    "\n",
    "Entity registry (use these URIs):\n",
    "{entity_registry}\n",
    "\n",
    "Guidelines:\n",
    "- Use schema.org properties (schema:birthDate, schema:birthPlace, schema:worksFor, etc.)\n",
    "- For relationships not in schema.org, use wiki3: prefix\n",
    "- Include rdfs:label for entities\n",
    "- Use xsd datatypes for dates and numbers\n",
    "- Entity URIs should use the source URL as base with fragment identifiers\n",
    "\n",
    "---\n",
    "{facts}\n",
    "---\n",
    "\n",
    "Generate Turtle RDF:\n",
    "\"\"\"\n",
    "\n",
    "RDF_PREFIXES = \"\"\"@prefix schema: <https://schema.org/> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix wiki3: <https://wiki3.ai/vocab/> .\n",
    "@base <{source_url}> .\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prompt templates defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa35e61",
   "metadata": {},
   "source": [
    "## Generate Chunks Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d121080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: data/albert_einstein_chunks.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_chunks_notebook(chunks: list, provenance: dict, registry: EntityRegistry, \n",
    "                             llm_config: dict, output_path: str):\n",
    "    \"\"\"Generate a notebook with chunked source text and context metadata.\n",
    "    \n",
    "    Each chunk cell is followed by a signature raw cell with its CID.\n",
    "    \"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    # Cell 0: Provenance markdown\n",
    "    provenance_yaml = f\"\"\"# Chunked Text: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "fetched_at: {provenance['fetched_at']}\n",
    "content_length: {provenance['content_length']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "attribution: {provenance['attribution']}\n",
    "chunk_size: {CHUNK_SIZE}\n",
    "chunk_overlap: {CHUNK_OVERLAP}\n",
    "total_chunks: {len(chunks)}\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "Each chunk below contains source text with contextual metadata. The context line (before the separator) provides:\n",
    "- **Context**: Hierarchical breadcrumb showing article > section path\n",
    "- **Chunk**: Position in sequence\n",
    "\n",
    "The text below the `---` separator is the unchanged source content.\n",
    "Each chunk is followed by a signature cell containing its Content ID (CID).\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_yaml))\n",
    "    \n",
    "    # Cell 1: Entity registry (raw cell)\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    # Chunk cells with signatures\n",
    "    # Compute CID of raw source for provenance\n",
    "    source_cid = compute_cid(provenance['source_url'] + str(provenance['content_length']))\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Content cell\n",
    "        chunk_content = f\"\"\"**Context:** {chunk.breadcrumb}\n",
    "**Chunk:** {chunk.chunk_index + 1} of {chunk.total_chunks}\n",
    "\n",
    "---\n",
    "\n",
    "{chunk.content}\n",
    "\"\"\"\n",
    "        nb.cells.append(new_markdown_cell(chunk_content))\n",
    "        \n",
    "        # Signature cell\n",
    "        chunk_cid = compute_cid(chunk_content)\n",
    "        signature = make_signature(\n",
    "            cell_num=chunk.chunk_index + 1,\n",
    "            cell_type=\"chunk\",\n",
    "            cid=chunk_cid,\n",
    "            from_cid=source_cid\n",
    "        )\n",
    "        nb.cells.append(new_raw_cell(json.dumps(signature)))\n",
    "    \n",
    "    # Write notebook\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(nb, f)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate chunks notebook\n",
    "article_slug = ARTICLE_TITLE.lower().replace(' ', '_')\n",
    "chunks_path = os.path.join(OUTPUT_DIR, f\"{article_slug}_chunks.ipynb\")\n",
    "generate_chunks_notebook(contextual_chunks, provenance, registry, LLM_CONFIG, chunks_path)\n",
    "print(f\"Generated: {chunks_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c23dc4",
   "metadata": {},
   "source": [
    "## Generate Facts Notebook (Structure Only)\n",
    "\n",
    "Creates the facts notebook with placeholders. Actual content is generated in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd4acab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts notebook exists: data/albert_einstein_facts.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_facts_notebook_header(provenance: dict, registry: EntityRegistry,\n",
    "                                    llm_config: dict, prompt_template: str) -> nbformat.NotebookNode:\n",
    "    \"\"\"Generate just the header cells for facts notebook.\"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    provenance_md = f\"\"\"# Factual Statements: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "source_notebook: {article_slug}_chunks.ipynb\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "llm_provider: {llm_config['provider']}\n",
    "llm_model: {llm_config['model']}\n",
    "llm_temperature: {llm_config['temperature']}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "This notebook contains simple English factual statements extracted from source text chunks.\n",
    "Each content cell corresponds to one chunk from the source notebook.\n",
    "Each content cell is followed by a signature cell with CID provenance.\n",
    "\n",
    "To regenerate a specific cell: delete both the content cell and its signature, then re-run the pipeline.\n",
    "\n",
    "## Prompt Template\n",
    "\n",
    "```\n",
    "{prompt_template}\n",
    "```\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_md))\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    return nb\n",
    "\n",
    "# Create initial facts notebook with just header (no placeholders)\n",
    "facts_path = os.path.join(OUTPUT_DIR, f\"{article_slug}_facts.ipynb\")\n",
    "\n",
    "# Only create if doesn't exist\n",
    "if not os.path.exists(facts_path):\n",
    "    facts_nb = generate_facts_notebook_header(provenance, registry, LLM_CONFIG, FACTS_EXTRACTION_PROMPT)\n",
    "    with open(facts_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(facts_nb, f)\n",
    "    print(f\"Created facts notebook: {facts_path}\")\n",
    "else:\n",
    "    print(f\"Facts notebook exists: {facts_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256713fd",
   "metadata": {},
   "source": [
    "## Generate RDF Notebook (Structure Only)\n",
    "\n",
    "Creates the RDF notebook with placeholders. Actual content is generated after facts extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a7ae750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF notebook exists: data/albert_einstein_rdf.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_rdf_notebook_header(provenance: dict, registry: EntityRegistry,\n",
    "                                  llm_config: dict, prompt_template: str, prefixes: str) -> nbformat.NotebookNode:\n",
    "    \"\"\"Generate just the header cells for RDF notebook.\"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    formatted_prefixes = prefixes.format(source_url=provenance['source_url'])\n",
    "    provenance_md = f\"\"\"# RDF Triples: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "source_notebook: {article_slug}_facts.ipynb\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "llm_provider: {llm_config['provider']}\n",
    "llm_model: {llm_config['model']}\n",
    "llm_temperature: {llm_config['temperature']}\n",
    "rdf_format: Turtle\n",
    "```\n",
    "\n",
    "## RDF Prefixes\n",
    "\n",
    "```turtle\n",
    "{formatted_prefixes}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "This notebook contains RDF triples in Turtle format, one cell per source facts cell.\n",
    "Each content cell is followed by a signature cell with CID provenance.\n",
    "\n",
    "To regenerate a specific cell: delete both the content cell and its signature, then re-run the pipeline.\n",
    "\n",
    "## Prompt Template\n",
    "\n",
    "```\n",
    "{prompt_template}\n",
    "```\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_md))\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    return nb\n",
    "\n",
    "# Create initial RDF notebook with just header (no placeholders)\n",
    "rdf_path = os.path.join(OUTPUT_DIR, f\"{article_slug}_rdf.ipynb\")\n",
    "\n",
    "# Only create if doesn't exist\n",
    "if not os.path.exists(rdf_path):\n",
    "    rdf_nb = generate_rdf_notebook_header(provenance, registry, LLM_CONFIG, RDF_GENERATION_PROMPT, RDF_PREFIXES)\n",
    "    with open(rdf_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(rdf_nb, f)\n",
    "    print(f\"Created RDF notebook: {rdf_path}\")\n",
    "else:\n",
    "    print(f\"RDF notebook exists: {rdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696afdf",
   "metadata": {},
   "source": [
    "## Process Chunks → Extract Facts\n",
    "\n",
    "Run the LLM on each chunk to extract factual statements and update the facts notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2350ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks notebook...\n",
      "Found 63 chunks with CIDs\n",
      "Found 63 existing fact signatures\n",
      "HTTP timeout: 60s per request\n",
      "--------------------------------------------------\n",
      "  Chunk 1: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 2: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 3: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 4: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 5: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 6: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 7: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 8: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 9: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 10: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 11: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 12: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 13: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 14: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 15: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 16: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 17: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 18: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 19: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 20: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 21: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 22: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 23: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 24: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 25: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 26: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 27: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 28: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 29: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 30: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 31: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 32: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 33: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 34: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 35: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 36: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 37: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 38: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 39: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 40: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 41: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 42: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 43: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 44: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 45: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 46: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 47: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 48: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 49: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 50: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 51: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 52: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 53: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 54: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 55: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 56: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 57: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 58: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 59: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 60: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 61: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 62: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 63: ⊘ Up-to-date (CID match), skipping\n",
      "--------------------------------------------------\n",
      "Facts extraction complete:\n",
      "  - 0 generated\n",
      "  - 63 skipped (up-to-date)\n",
      "  - 0 errors/timeouts\n"
     ]
    }
   ],
   "source": [
    "# Create LLM client with timeout for facts extraction\n",
    "facts_llm = ChatOpenAI(\n",
    "    model=LLM_CONFIG[\"model\"],\n",
    "    temperature=LLM_CONFIG[\"temperature\"],\n",
    "    base_url=LLM_CONFIG[\"base_url\"],\n",
    "    api_key=\"lm-studio\",\n",
    "    timeout=CELL_TIMEOUT_SECONDS,  # HTTP timeout in seconds\n",
    "    max_retries=0,  # Don't retry on timeout\n",
    ")\n",
    "\n",
    "# Create facts extraction prompt\n",
    "facts_prompt = ChatPromptTemplate.from_template(\"\"\"You are an expert at extracting factual information from text.\n",
    "\n",
    "Given text from a Wikipedia article, extract simple English statements that capture the consensus factual information. Each statement should:\n",
    "- Be a single, clear sentence\n",
    "- Contain one main fact or relationship\n",
    "- Use the full name of entities on first mention\n",
    "- Be verifiable from the source text\n",
    "- Avoid opinions, interpretations, or hedged language\n",
    "\n",
    "Source: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Known entities (use consistent names):\n",
    "{known_entities}\n",
    "\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\n",
    "Extract factual statements as a bulleted list:\"\"\")\n",
    "\n",
    "facts_chain = facts_prompt | facts_llm\n",
    "\n",
    "def get_known_entities_text(registry: EntityRegistry) -> str:\n",
    "    \"\"\"Format known entities for prompt context.\"\"\"\n",
    "    lines = []\n",
    "    for entity in registry.entities.values():\n",
    "        lines.append(f\"- {entity['label']} ({entity['type']})\")\n",
    "    return \"\\n\".join(lines) if lines else \"None yet\"\n",
    "\n",
    "# Read chunks notebook to get source content and CIDs\n",
    "log_progress(\"Reading chunks notebook...\")\n",
    "chunks_nb = nbformat.read(chunks_path, as_version=4)\n",
    "chunk_signatures = extract_signatures(chunks_nb)\n",
    "\n",
    "# Build list of chunk content with CIDs\n",
    "# Chunks notebook structure: [provenance, registry, chunk1, sig1, chunk2, sig2, ...]\n",
    "chunk_data = []\n",
    "cell_idx = 2  # Skip provenance and registry\n",
    "while cell_idx < len(chunks_nb.cells):\n",
    "    cell = chunks_nb.cells[cell_idx]\n",
    "    if cell.cell_type == 'markdown':\n",
    "        content = cell.source\n",
    "        # Get corresponding signature (next cell)\n",
    "        sig = None\n",
    "        if cell_idx + 1 < len(chunks_nb.cells):\n",
    "            sig = parse_signature(chunks_nb.cells[cell_idx + 1].source)\n",
    "        \n",
    "        # Extract breadcrumb\n",
    "        context_match = re.search(r'\\*\\*Context:\\*\\*\\s*(.+)', content)\n",
    "        breadcrumb = context_match.group(1) if context_match else \"Unknown\"\n",
    "        \n",
    "        # Extract chunk text (after ---)\n",
    "        parts = content.split(\"---\\n\", 1)\n",
    "        chunk_text = parts[1].strip() if len(parts) > 1 else content\n",
    "        \n",
    "        chunk_data.append({\n",
    "            \"cell_num\": sig[\"cell\"] if sig else len(chunk_data) + 1,\n",
    "            \"content\": content,\n",
    "            \"chunk_text\": chunk_text,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"cid\": sig[\"cid\"] if sig else compute_cid(content),\n",
    "        })\n",
    "        cell_idx += 2  # Skip content and signature\n",
    "    else:\n",
    "        cell_idx += 1\n",
    "\n",
    "log_progress(f\"Found {len(chunk_data)} chunks with CIDs\")\n",
    "\n",
    "# Read existing facts notebook and extract signatures\n",
    "facts_nb = nbformat.read(facts_path, as_version=4)\n",
    "facts_signatures = extract_signatures(facts_nb)\n",
    "\n",
    "log_progress(f\"Found {len(facts_signatures)} existing fact signatures\")\n",
    "log_progress(f\"HTTP timeout: {CELL_TIMEOUT_SECONDS}s per request\")\n",
    "log_progress(\"-\" * 50)\n",
    "\n",
    "# Process each chunk\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for chunk in chunk_data:\n",
    "    cell_num = chunk[\"cell_num\"]\n",
    "    source_cid = chunk[\"cid\"]\n",
    "    \n",
    "    # Check if we already have up-to-date facts for this chunk\n",
    "    existing_sig = facts_signatures.get(cell_num)\n",
    "    if existing_sig and existing_sig[\"from_cid\"] == source_cid:\n",
    "        log_progress(f\"  Chunk {cell_num}: ⊘ Up-to-date (CID match), skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Need to generate (or regenerate) this cell\n",
    "    status = \"↻ Regenerating\" if existing_sig else \"+ Generating\"\n",
    "    chunk_preview = chunk[\"chunk_text\"][:80].replace('\\n', ' ')\n",
    "    log_progress(f\"  Chunk {cell_num}: {status}\")\n",
    "    log_progress(f\"    Context: {chunk['breadcrumb']}\")\n",
    "    log_progress(f\"    Input: {chunk_preview}...\")\n",
    "    log_progress(f\"    [Calling LLM...]\", end=\" \")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Call LLM to extract facts\n",
    "    try:\n",
    "        result = facts_chain.invoke({\n",
    "            \"source_url\": provenance[\"source_url\"],\n",
    "            \"breadcrumb\": chunk[\"breadcrumb\"],\n",
    "            \"known_entities\": get_known_entities_text(registry),\n",
    "            \"chunk_text\": chunk[\"chunk_text\"],\n",
    "        })\n",
    "        facts_content = result.content\n",
    "        elapsed = time.time() - start_time\n",
    "        log_progress(f\"✓ ({len(facts_content)} chars, {elapsed:.1f}s)\")\n",
    "        processed_count += 1\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)[:100]\n",
    "        facts_content = f\"# Error: {error_type}: {e}\"\n",
    "        log_progress(f\"✗ {error_type} after {elapsed:.1f}s\")\n",
    "        log_progress(f\"    Error: {error_msg}\")\n",
    "        error_count += 1\n",
    "    \n",
    "    # Build the facts cell content\n",
    "    facts_cell_content = f\"\"\"**Context:** {chunk['breadcrumb']}\n",
    "**Chunk:** {cell_num} of {len(chunk_data)}\n",
    "\n",
    "---\n",
    "\n",
    "{facts_content}\n",
    "\"\"\"\n",
    "    facts_cid = compute_cid(facts_cell_content)\n",
    "    signature = make_signature(cell_num, \"facts\", facts_cid, source_cid)\n",
    "    \n",
    "    # Find where to insert/update in the notebook\n",
    "    # If there's an existing signature for this cell, find and remove old content+sig\n",
    "    if existing_sig:\n",
    "        # Find and remove the old cells\n",
    "        new_cells = [facts_nb.cells[0], facts_nb.cells[1]]  # Keep header\n",
    "        i = 2\n",
    "        while i < len(facts_nb.cells):\n",
    "            cell = facts_nb.cells[i]\n",
    "            if cell.cell_type == 'raw':\n",
    "                sig = parse_signature(cell.source)\n",
    "                if sig and sig[\"cell\"] == cell_num:\n",
    "                    # Skip this signature and its preceding content cell\n",
    "                    i += 1\n",
    "                    continue\n",
    "            # Check if this is content for the cell we're replacing\n",
    "            if i > 0 and i + 1 < len(facts_nb.cells):\n",
    "                next_sig = parse_signature(facts_nb.cells[i + 1].source) if facts_nb.cells[i + 1].cell_type == 'raw' else None\n",
    "                if next_sig and next_sig[\"cell\"] == cell_num:\n",
    "                    i += 2  # Skip content and signature\n",
    "                    continue\n",
    "            new_cells.append(cell)\n",
    "            i += 1\n",
    "        facts_nb.cells = new_cells\n",
    "    \n",
    "    # Append new content and signature\n",
    "    facts_nb.cells.append(new_markdown_cell(facts_cell_content))\n",
    "    facts_nb.cells.append(new_raw_cell(json.dumps(signature)))\n",
    "    \n",
    "    # Update signatures dict\n",
    "    facts_signatures[cell_num] = signature\n",
    "    \n",
    "    # Save notebook after each cell\n",
    "    log_progress(f\"    [Saving notebook...]\", end=\" \")\n",
    "    with open(facts_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(facts_nb, f)\n",
    "    log_progress(\"saved\")\n",
    "\n",
    "log_progress(\"-\" * 50)\n",
    "log_progress(f\"Facts extraction complete:\")\n",
    "log_progress(f\"  - {processed_count} generated\")\n",
    "log_progress(f\"  - {skipped_count} skipped (up-to-date)\")\n",
    "log_progress(f\"  - {error_count} errors/timeouts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d29b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts notebook: data/albert_einstein_facts.ipynb\n",
      "  - Updated incrementally during processing\n",
      "  - Ready for review/editing before RDF generation\n"
     ]
    }
   ],
   "source": [
    "# Summary of facts extraction (notebook already updated incrementally)\n",
    "print(f\"Facts notebook: {facts_path}\")\n",
    "print(f\"  - Updated incrementally during processing\")\n",
    "print(f\"  - Ready for review/editing before RDF generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de7f1f",
   "metadata": {},
   "source": [
    "## Process Facts → Generate RDF\n",
    "\n",
    "Run the LLM on each facts cell to generate RDF triples and update the RDF notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afd0797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded schema matcher with 1 vocabularies\n",
      "Reading facts notebook...\n",
      "Found 63 facts cells with CIDs\n",
      "Found 63 existing RDF signatures\n",
      "HTTP timeout: 60s per request\n",
      "--------------------------------------------------\n",
      "  Facts 1: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 2: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 3: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 4: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 5: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 6: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 7: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 8: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 9: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 10: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 11: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 12: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 13: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 14: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 15: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 16: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 17: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 18: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 19: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 20: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 21: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 22: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 23: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 24: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 25: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 26: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 27: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 28: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 29: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 30: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 31: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 32: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 33: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 34: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 35: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 36: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 37: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 38: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 39: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 40: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 41: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 42: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 43: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 44: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 45: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 46: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 47: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 48: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 49: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 50: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 51: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 52: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 53: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 54: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 55: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 56: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 57: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 58: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 59: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 60: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 61: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 62: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 63: ⊘ Up-to-date (CID match), skipping\n",
      "--------------------------------------------------\n",
      "RDF generation complete:\n",
      "  - 0 generated\n",
      "  - 63 skipped (up-to-date)\n",
      "  - 0 errors/timeouts\n"
     ]
    }
   ],
   "source": [
    "# Load schema matcher with pre-built vocabulary index\n",
    "from schema_matcher import SchemaMatcher\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "VOCAB_CACHE_DIR = \"data/vocab_cache\"\n",
    "schema_matcher = SchemaMatcher.load(VOCAB_CACHE_DIR, embed_base_url=LLM_CONFIG[\"base_url\"])\n",
    "log_progress(f\"Loaded schema matcher with {len(schema_matcher.vocabularies)} vocabularies\")\n",
    "\n",
    "# Define tools for the LLM to query schema.org vocabulary\n",
    "\n",
    "@tool\n",
    "def find_rdf_class(description: str) -> str:\n",
    "    \"\"\"Find the best schema.org class/type for an entity based on a natural language description.\n",
    "    \n",
    "    Use this when you need to determine the rdf:type of an entity.\n",
    "    Example: find_rdf_class(\"a person who does scientific research\")\n",
    "    \n",
    "    Args:\n",
    "        description: Natural language description of the entity type\n",
    "    \n",
    "    Returns:\n",
    "        Top matching schema.org classes with URIs and descriptions\n",
    "    \"\"\"\n",
    "    results = schema_matcher.find_class(description, top_k=5)\n",
    "    if not results:\n",
    "        return \"No matches found. Use a generic type like schema:Thing\"\n",
    "    \n",
    "    lines = [\"Top matching classes:\"]\n",
    "    for r in results:\n",
    "        lines.append(f\"  {r['prefix']} ({r['score']:.2f})\")\n",
    "        lines.append(f\"    URI: {r['uri']}\")\n",
    "        if r['description']:\n",
    "            lines.append(f\"    Description: {r['description'][:100]}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "@tool\n",
    "def find_rdf_property(description: str, subject_type: str = \"\", object_type: str = \"\") -> str:\n",
    "    \"\"\"Find the best schema.org property/predicate for a relationship.\n",
    "    \n",
    "    Use this when you need to find the right predicate for a triple.\n",
    "    Example: find_rdf_property(\"the date when someone was born\", subject_type=\"Person\")\n",
    "    \n",
    "    Args:\n",
    "        description: Natural language description of the relationship\n",
    "        subject_type: Optional - the type of the subject (e.g., \"Person\", \"Organization\")  \n",
    "        object_type: Optional - the type of the object/value (e.g., \"Date\", \"Place\")\n",
    "    \n",
    "    Returns:\n",
    "        Top matching schema.org properties with URIs, domains, and ranges\n",
    "    \"\"\"\n",
    "    results = schema_matcher.find_property(\n",
    "        description, \n",
    "        subject_type=subject_type or None,\n",
    "        object_type=object_type or None,\n",
    "        top_k=5\n",
    "    )\n",
    "    if not results:\n",
    "        return \"No matches found. Consider using rdfs:label or a descriptive URI fragment.\"\n",
    "    \n",
    "    lines = [\"Top matching properties:\"]\n",
    "    for r in results:\n",
    "        lines.append(f\"  {r['prefix']} ({r['score']:.2f})\")\n",
    "        lines.append(f\"    URI: {r['uri']}\")\n",
    "        if r['domain']:\n",
    "            lines.append(f\"    Domain: {r['domain']}\")\n",
    "        if r['range']:\n",
    "            lines.append(f\"    Range: {r['range']}\")\n",
    "        if r['description']:\n",
    "            lines.append(f\"    Description: {r['description'][:80]}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Create LLM client with timeout\n",
    "rdf_llm = ChatOpenAI(\n",
    "    model=LLM_CONFIG[\"model\"],\n",
    "    temperature=LLM_CONFIG[\"temperature\"],\n",
    "    base_url=LLM_CONFIG[\"base_url\"],\n",
    "    api_key=\"lm-studio\",  # type: ignore\n",
    "    timeout=CELL_TIMEOUT_SECONDS,\n",
    "    max_retries=0,\n",
    ")\n",
    "\n",
    "# Bind tools to the LLM for tool calling\n",
    "rdf_tools = [find_rdf_class, find_rdf_property]\n",
    "rdf_llm_with_tools = rdf_llm.bind_tools(rdf_tools)\n",
    "\n",
    "# Create RDF generation prompt\n",
    "rdf_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert at converting factual statements to RDF triples in Turtle format.\n",
    "\n",
    "You have access to tools to find correct schema.org vocabulary:\n",
    "- find_rdf_class: Find the best class/type for an entity\n",
    "- find_rdf_property: Find the best predicate for a relationship\n",
    "\n",
    "IMPORTANT: Use these tools to look up appropriate schema.org terms. Do NOT invent predicates.\n",
    "\n",
    "Generate valid Turtle RDF triples. Use schema.org prefixed terms (e.g., schema:Person, schema:birthDate).\"\"\"),\n",
    "    (\"human\", \"\"\"Convert these factual statements to RDF triples.\n",
    "\n",
    "Source: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Entity Registry (use these URIs for known entities):\n",
    "{entity_registry}\n",
    "\n",
    "Factual statements to convert:\n",
    "{facts}\n",
    "\n",
    "Use the tools to find appropriate schema.org classes and properties, then generate the Turtle RDF.\"\"\"),\n",
    "])\n",
    "\n",
    "# Simple tool-calling loop instead of agent framework\n",
    "def call_llm_with_tools(prompt_vars: dict, max_iterations: int = 10) -> str:\n",
    "    \"\"\"Call LLM with tool support using a simple loop.\"\"\"\n",
    "    messages = rdf_prompt.format_messages(**prompt_vars)\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        response = rdf_llm_with_tools.invoke(messages)\n",
    "        \n",
    "        # Check if there are tool calls\n",
    "        if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "            # Add assistant message with tool calls\n",
    "            messages.append(response)\n",
    "            \n",
    "            # Execute each tool call\n",
    "            for tool_call in response.tool_calls:\n",
    "                tool_name = tool_call['name']\n",
    "                tool_args = tool_call['args']\n",
    "                \n",
    "                # Find and execute the tool\n",
    "                if tool_name == 'find_rdf_class':\n",
    "                    tool_result = find_rdf_class.invoke(tool_args)\n",
    "                elif tool_name == 'find_rdf_property':\n",
    "                    tool_result = find_rdf_property.invoke(tool_args)\n",
    "                else:\n",
    "                    tool_result = f\"Unknown tool: {tool_name}\"\n",
    "                \n",
    "                # Add tool result message\n",
    "                from langchain_core.messages import ToolMessage\n",
    "                messages.append(ToolMessage(content=str(tool_result), tool_call_id=tool_call['id']))\n",
    "        else:\n",
    "            # No tool calls, return the content\n",
    "            return response.content\n",
    "    \n",
    "    # Max iterations reached\n",
    "    return response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "def format_entity_registry_for_prompt(registry: EntityRegistry) -> str:\n",
    "    \"\"\"Format registry for RDF prompt.\"\"\"\n",
    "    lines = []\n",
    "    for entity in registry.entities.values():\n",
    "        lines.append(f\"<{entity['uri']}> # {entity['label']} ({entity['type']})\")\n",
    "    return \"\\n\".join(lines) if lines else \"# No entities registered yet\"\n",
    "\n",
    "# Read facts notebook to get source content and CIDs\n",
    "log_progress(\"Reading facts notebook...\")\n",
    "facts_nb = nbformat.read(facts_path, as_version=4)\n",
    "facts_signatures = extract_signatures(facts_nb)\n",
    "\n",
    "# Build list of facts content with CIDs\n",
    "facts_data = []\n",
    "cell_idx = 2  # Skip provenance and registry\n",
    "while cell_idx < len(facts_nb.cells):\n",
    "    cell = facts_nb.cells[cell_idx]\n",
    "    if cell.cell_type == 'markdown':\n",
    "        content = cell.source\n",
    "        # Get corresponding signature (next cell)\n",
    "        sig = None\n",
    "        if cell_idx + 1 < len(facts_nb.cells):\n",
    "            sig = parse_signature(facts_nb.cells[cell_idx + 1].source)\n",
    "        \n",
    "        # Extract breadcrumb\n",
    "        context_match = re.search(r'\\*\\*Context:\\*\\*\\s*(.+)', content)\n",
    "        breadcrumb = context_match.group(1) if context_match else \"Unknown\"\n",
    "        \n",
    "        # Extract facts (after ---)\n",
    "        parts = content.split(\"---\\n\", 1)\n",
    "        facts_text = parts[1].strip() if len(parts) > 1 else content\n",
    "        \n",
    "        facts_data.append({\n",
    "            \"cell_num\": sig[\"cell\"] if sig else len(facts_data) + 1,\n",
    "            \"content\": content,\n",
    "            \"facts_text\": facts_text,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"cid\": sig[\"cid\"] if sig else compute_cid(content),\n",
    "        })\n",
    "        cell_idx += 2  # Skip content and signature\n",
    "    else:\n",
    "        cell_idx += 1\n",
    "\n",
    "log_progress(f\"Found {len(facts_data)} facts cells with CIDs\")\n",
    "\n",
    "# Read existing RDF notebook and extract signatures\n",
    "rdf_nb = nbformat.read(rdf_path, as_version=4)\n",
    "rdf_signatures = extract_signatures(rdf_nb)\n",
    "\n",
    "log_progress(f\"Found {len(rdf_signatures)} existing RDF signatures\")\n",
    "log_progress(f\"HTTP timeout: {CELL_TIMEOUT_SECONDS}s per request\")\n",
    "log_progress(\"-\" * 50)\n",
    "\n",
    "# Process each facts cell\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for facts_item in facts_data:\n",
    "    cell_num = facts_item[\"cell_num\"]\n",
    "    source_cid = facts_item[\"cid\"]\n",
    "    \n",
    "    # Skip cells that had errors in facts extraction\n",
    "    if facts_item[\"facts_text\"].startswith(\"# Error:\"):\n",
    "        log_progress(f\"  Facts {cell_num}: ⊘ Source had error, skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Check if we already have up-to-date RDF for these facts\n",
    "    existing_sig = rdf_signatures.get(cell_num)\n",
    "    if existing_sig and existing_sig[\"from_cid\"] == source_cid:\n",
    "        log_progress(f\"  Facts {cell_num}: ⊘ Up-to-date (CID match), skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Need to generate (or regenerate) this cell\n",
    "    status = \"↻ Regenerating\" if existing_sig else \"+ Generating\"\n",
    "    facts_preview = facts_item[\"facts_text\"][:80].replace('\\n', ' ')\n",
    "    log_progress(f\"  Facts {cell_num}: {status}\")\n",
    "    log_progress(f\"    Context: {facts_item['breadcrumb']}\")\n",
    "    log_progress(f\"    Input: {facts_preview}...\")\n",
    "    log_progress(f\"    [Calling LLM with schema tools...]\", end=\" \")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Call LLM with tool support\n",
    "    try:\n",
    "        rdf_content = call_llm_with_tools({\n",
    "            \"source_url\": provenance[\"source_url\"],\n",
    "            \"breadcrumb\": facts_item[\"breadcrumb\"],\n",
    "            \"entity_registry\": format_entity_registry_for_prompt(registry),\n",
    "            \"facts\": facts_item[\"facts_text\"],\n",
    "        })\n",
    "        elapsed = time.time() - start_time\n",
    "        log_progress(f\"✓ ({len(rdf_content)} chars, {elapsed:.1f}s)\")\n",
    "        processed_count += 1\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)[:100]\n",
    "        rdf_content = f\"# Error: {error_type}: {e}\"\n",
    "        log_progress(f\"✗ {error_type} after {elapsed:.1f}s\")\n",
    "        log_progress(f\"    Error: {error_msg}\")\n",
    "        error_count += 1\n",
    "    \n",
    "    # Build the RDF cell content\n",
    "    rdf_cell_content = f\"\"\"# Context: {facts_item['breadcrumb']}\n",
    "# Cell: {cell_num} of {len(facts_data)}\n",
    "\n",
    "{rdf_content}\n",
    "\"\"\"\n",
    "    rdf_cid = compute_cid(rdf_cell_content)\n",
    "    signature = make_signature(cell_num, \"rdf\", rdf_cid, source_cid)\n",
    "    \n",
    "    # Find where to insert/update in the notebook\n",
    "    # If there's an existing signature for this cell, find and remove old content+sig\n",
    "    if existing_sig:\n",
    "        # Find and remove the old cells\n",
    "        new_cells = [rdf_nb.cells[0], rdf_nb.cells[1]]  # Keep header\n",
    "        i = 2\n",
    "        while i < len(rdf_nb.cells):\n",
    "            cell = rdf_nb.cells[i]\n",
    "            if cell.cell_type == 'raw':\n",
    "                sig = parse_signature(cell.source)\n",
    "                if sig and sig[\"cell\"] == cell_num:\n",
    "                    # Skip this signature and its preceding content cell\n",
    "                    i += 1\n",
    "                    continue\n",
    "            # Check if this is content for the cell we're replacing\n",
    "            if i > 0 and i + 1 < len(rdf_nb.cells):\n",
    "                next_sig = parse_signature(rdf_nb.cells[i + 1].source) if rdf_nb.cells[i + 1].cell_type == 'raw' else None\n",
    "                if next_sig and next_sig[\"cell\"] == cell_num:\n",
    "                    i += 2  # Skip content and signature\n",
    "                    continue\n",
    "            new_cells.append(cell)\n",
    "            i += 1\n",
    "        rdf_nb.cells = new_cells\n",
    "    \n",
    "    # Append new content and signature\n",
    "    rdf_nb.cells.append(new_raw_cell(rdf_cell_content))\n",
    "    rdf_nb.cells.append(new_raw_cell(json.dumps(signature)))\n",
    "    \n",
    "    # Update signatures dict\n",
    "    rdf_signatures[cell_num] = signature\n",
    "    \n",
    "    # Save notebook after each cell\n",
    "    log_progress(f\"    [Saving notebook...]\", end=\" \")\n",
    "    with open(rdf_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(rdf_nb, f)\n",
    "    log_progress(\"saved\")\n",
    "\n",
    "log_progress(\"-\" * 50)\n",
    "log_progress(f\"RDF generation complete:\")\n",
    "log_progress(f\"  - {processed_count} generated\")\n",
    "log_progress(f\"  - {skipped_count} skipped (up-to-date)\")\n",
    "log_progress(f\"  - {error_count} errors/timeouts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of RDF generation (notebook already updated incrementally)\n",
    "print(f\"RDF notebook: {rdf_path}\")\n",
    "print(f\"  - Updated incrementally during processing\")\n",
    "print(f\"  - Ready for review/editing before final export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a59d9",
   "metadata": {},
   "source": [
    "## Export Combined RDF\n",
    "\n",
    "Combine all RDF cells into a single Turtle file with prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d061083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported RDF to: data/albert_einstein.ttl\n",
      "  - 53 chunks of triples\n",
      "  - 199397 characters total\n"
     ]
    }
   ],
   "source": [
    "# Combine all RDF into a single Turtle file\n",
    "all_triples = []\n",
    "\n",
    "# Read the updated RDF notebook\n",
    "rdf_nb = nbformat.read(rdf_path, as_version=4)\n",
    "\n",
    "# Collect RDF from all raw cells (skip provenance, registry, and signature cells)\n",
    "for cell in rdf_nb.cells[2:]:\n",
    "    if cell.cell_type == 'raw':\n",
    "        content = cell.source.strip()\n",
    "        \n",
    "        # Skip signature cells (JSON objects)\n",
    "        if content.startswith('{') and '\"cid\"' in content:\n",
    "            continue\n",
    "        \n",
    "        # Skip empty or error-only cells\n",
    "        if not content or content.startswith('# Error:'):\n",
    "            continue\n",
    "        \n",
    "        # Skip comment-only cells\n",
    "        lines = [line for line in content.split('\\n') \n",
    "                 if line.strip() and not line.strip().startswith('#')]\n",
    "        if lines:\n",
    "            all_triples.append(content)\n",
    "\n",
    "# Format prefixes for the Turtle file\n",
    "turtle_prefixes = RDF_PREFIXES.format(source_url=provenance['source_url'])\n",
    "\n",
    "# Build complete Turtle file\n",
    "turtle_output = f\"\"\"# RDF Knowledge Graph: {provenance['article_title']}\n",
    "# Source: {provenance['source_url']}\n",
    "# License: {provenance['license']}\n",
    "# Generated: {datetime.now().isoformat()}\n",
    "\n",
    "{turtle_prefixes}\n",
    "\n",
    "# === Triples ===\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "turtle_output += \"\\n\\n\".join(all_triples)\n",
    "\n",
    "# Save to file\n",
    "turtle_path = os.path.join(OUTPUT_DIR, f\"{article_slug}.ttl\")\n",
    "with open(turtle_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(turtle_output)\n",
    "\n",
    "print(f\"Exported RDF to: {turtle_path}\")\n",
    "print(f\"  - {len(all_triples)} chunks of triples\")\n",
    "print(f\"  - {len(turtle_output)} characters total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c4e37",
   "metadata": {},
   "source": [
    "## Save Initial Entity Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d416a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/entity_registry.json\n",
      "\n",
      "Initial entities: 1\n",
      "  - Albert Einstein (Person): https://en.wikipedia.org/wiki/Albert_Einstein#person_albert_einstein\n"
     ]
    }
   ],
   "source": [
    "# Save entity registry to JSON file\n",
    "registry_path = os.path.join(OUTPUT_DIR, \"entity_registry.json\")\n",
    "with open(registry_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(registry.to_json())\n",
    "\n",
    "print(f\"Saved: {registry_path}\")\n",
    "print(f\"\\nInitial entities: {len(registry.entities)}\")\n",
    "for key, entity in registry.entities.items():\n",
    "    print(f\"  - {entity['label']} ({entity['type']}): {entity['uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2368a",
   "metadata": {},
   "source": [
    "## Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b31bf97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE COMPLETE\n",
      "============================================================\n",
      "\n",
      "Article: Albert Einstein\n",
      "Source: https://en.wikipedia.org/wiki/Albert_Einstein\n",
      "License: CC BY-SA 4.0\n",
      "\n",
      "Generated artifacts:\n",
      "  1. data/albert_einstein_chunks.ipynb\n",
      "     - 63 chunks with breadcrumb context\n",
      "  2. data/albert_einstein_facts.ipynb\n",
      "     - Facts extracted from chunks (see notebook for details)\n",
      "  3. data/albert_einstein_rdf.ipynb\n",
      "     - RDF triples generated from facts (see notebook for details)\n",
      "  4. data/albert_einstein.ttl\n",
      "     - Combined Turtle file for import\n",
      "\n",
      "Entity registry: data/entity_registry.json\n",
      "\n",
      "The intermediate notebooks can be reviewed and edited before re-export.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nArticle: {ARTICLE_TITLE}\")\n",
    "print(f\"Source: {source_url}\")\n",
    "print(f\"License: {provenance['license']}\")\n",
    "print(f\"\\nGenerated artifacts:\")\n",
    "print(f\"  1. {chunks_path}\")\n",
    "print(f\"     - {len(contextual_chunks)} chunks with breadcrumb context\")\n",
    "print(f\"  2. {facts_path}\")\n",
    "print(f\"     - Facts extracted from chunks (see notebook for details)\")\n",
    "print(f\"  3. {rdf_path}\")\n",
    "print(f\"     - RDF triples generated from facts (see notebook for details)\")\n",
    "print(f\"  4. {turtle_path}\")\n",
    "print(f\"     - Combined Turtle file for import\")\n",
    "print(f\"\\nEntity registry: {registry_path}\")\n",
    "print(f\"\\nThe intermediate notebooks can be reviewed and edited before re-export.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fef4bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
