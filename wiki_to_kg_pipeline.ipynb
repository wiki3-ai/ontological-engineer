{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236e29d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# mamba install -c conda-forge langchain langchain-text-splitters langchain-openai langchain-community wikipedia pydantic nbformat nbconvert\n",
    "%pip install -q langchain langchain-text-splitters langchain-openai langchain-community wikipedia pydantic nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833f695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded (with CID support)\n",
      "Cell timeout: 60s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "from urllib.parse import quote\n",
    "\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell, new_raw_cell\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# === Content ID (CID) Functions ===\n",
    "\n",
    "def compute_cid(content: str) -> str:\n",
    "    \"\"\"Compute SHA256 content ID for a string.\"\"\"\n",
    "    return hashlib.sha256(content.encode('utf-8')).hexdigest()\n",
    "\n",
    "def make_signature(cell_num: int, cell_type: str, cid: str, from_cid: str) -> dict:\n",
    "    \"\"\"Create a signature dict for a generated cell.\"\"\"\n",
    "    return {\n",
    "        \"cell\": cell_num,\n",
    "        \"type\": cell_type,\n",
    "        \"cid\": cid,\n",
    "        \"from_cid\": from_cid,\n",
    "    }\n",
    "\n",
    "def parse_signature(raw_content: str) -> Optional[dict]:\n",
    "    \"\"\"Parse a signature from raw cell content. Returns None if not a valid signature.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(raw_content.strip())\n",
    "        if all(k in data for k in (\"cell\", \"type\", \"cid\", \"from_cid\")):\n",
    "            return data\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def extract_signatures(notebook) -> dict:\n",
    "    \"\"\"Extract all signatures from a notebook, keyed by cell number.\"\"\"\n",
    "    signatures = {}\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == 'raw':\n",
    "            sig = parse_signature(cell.source)\n",
    "            if sig:\n",
    "                signatures[sig[\"cell\"]] = sig\n",
    "    return signatures\n",
    "\n",
    "# === Timeout Configuration ===\n",
    "# Note: SIGALRM doesn't work for blocking I/O (HTTP requests)\n",
    "# Instead, we use the timeout parameter on the ChatOpenAI client\n",
    "\n",
    "CELL_TIMEOUT_SECONDS = 60  # 1 minute per cell max\n",
    "\n",
    "def log_progress(msg: str, end=\"\\n\"):\n",
    "    \"\"\"Print with immediate flush for real-time progress.\"\"\"\n",
    "    print(msg, end=end, flush=True)\n",
    "\n",
    "print(\"Dependencies loaded (with CID support)\")\n",
    "print(f\"Cell timeout: {CELL_TIMEOUT_SECONDS}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252ef95",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f79c403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline configured for: Albert Einstein\n"
     ]
    }
   ],
   "source": [
    "# Pipeline configuration\n",
    "ARTICLE_TITLE = \"Albert Einstein\"\n",
    "OUTPUT_DIR = \"data\"\n",
    "CHUNK_SIZE = 2000\n",
    "CHUNK_OVERLAP = 128\n",
    "\n",
    "# LLM configuration (shared across stages)\n",
    "LLM_CONFIG = {\n",
    "    \"provider\": \"lm_studio\",  # or \"openai\"\n",
    "    \"model\": \"qwen/qwen3-coder-30b\",\n",
    "    \"temperature\": 1,\n",
    "    \"base_url\": os.environ.get(\"LM_STUDIO_BASE_URL\", \"http://host.docker.internal:1234/v1\"),\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Pipeline configured for: {ARTICLE_TITLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df90b5b",
   "metadata": {},
   "source": [
    "## Entity Registry\n",
    "\n",
    "Manages entity identity across chunks with stable URIs derived from source URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a66f9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntityRegistry class defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class EntityRegistry:\n",
    "    \"\"\"Tracks entities with stable IDs derived from source URL.\"\"\"\n",
    "    source_url: str\n",
    "    entities: dict = field(default_factory=dict)  # normalized_key -> entity\n",
    "    aliases: dict = field(default_factory=dict)   # alias -> canonical_key\n",
    "    \n",
    "    def normalize_key(self, label: str) -> str:\n",
    "        \"\"\"Create consistent key from entity label.\"\"\"\n",
    "        return re.sub(r'[^a-z0-9]+', '_', label.lower().strip()).strip('_')\n",
    "    \n",
    "    def generate_uri(self, entity_type: str, label: str) -> str:\n",
    "        \"\"\"Generate URI based on source URL with fragment identifier.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        # Use source URL as base, add fragment for entity\n",
    "        return f\"{self.source_url}#{entity_type.lower()}_{key}\"\n",
    "    \n",
    "    def generate_id(self, entity_type: str, label: str) -> str:\n",
    "        \"\"\"Generate local ID for internal reference.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        return f\"{entity_type.lower()}_{key}\"\n",
    "    \n",
    "    def register(self, label: str, entity_type: str, description: str = \"\",\n",
    "                 aliases: list = None, source_chunk: int = None) -> str:\n",
    "        \"\"\"Register or update an entity, return canonical ID.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        entity_id = self.generate_id(entity_type, label)\n",
    "        entity_uri = self.generate_uri(entity_type, label)\n",
    "        \n",
    "        if key not in self.entities:\n",
    "            self.entities[key] = {\n",
    "                \"id\": entity_id,\n",
    "                \"uri\": entity_uri,\n",
    "                \"label\": label,\n",
    "                \"type\": entity_type,\n",
    "                \"descriptions\": [description] if description else [],\n",
    "                \"source_chunks\": [source_chunk] if source_chunk is not None else [],\n",
    "                \"aliases\": list(aliases or []),\n",
    "            }\n",
    "        else:\n",
    "            existing = self.entities[key]\n",
    "            if description and description not in existing[\"descriptions\"]:\n",
    "                existing[\"descriptions\"].append(description)\n",
    "            if source_chunk is not None and source_chunk not in existing[\"source_chunks\"]:\n",
    "                existing[\"source_chunks\"].append(source_chunk)\n",
    "            if aliases:\n",
    "                existing[\"aliases\"] = list(set(existing[\"aliases\"]) | set(aliases))\n",
    "        \n",
    "        # Register aliases\n",
    "        for alias in (aliases or []):\n",
    "            self.aliases[self.normalize_key(alias)] = key\n",
    "        \n",
    "        return entity_id\n",
    "    \n",
    "    def lookup(self, label: str) -> Optional[dict]:\n",
    "        \"\"\"Find entity by label or alias.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        canonical_key = self.aliases.get(key, key)\n",
    "        return self.entities.get(canonical_key)\n",
    "    \n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Serialize registry to JSON.\"\"\"\n",
    "        return json.dumps({\n",
    "            \"source_url\": self.source_url,\n",
    "            \"entities\": self.entities,\n",
    "            \"aliases\": self.aliases,\n",
    "        }, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, json_str: str) -> 'EntityRegistry':\n",
    "        \"\"\"Deserialize registry from JSON.\"\"\"\n",
    "        data = json.loads(json_str)\n",
    "        registry = cls(source_url=data[\"source_url\"])\n",
    "        registry.entities = data[\"entities\"]\n",
    "        registry.aliases = data[\"aliases\"]\n",
    "        return registry\n",
    "\n",
    "print(\"EntityRegistry class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094a385",
   "metadata": {},
   "source": [
    "## Section Hierarchy Parser\n",
    "\n",
    "Extracts Wikipedia section structure for breadcrumb context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fe68df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section hierarchy parser defined\n"
     ]
    }
   ],
   "source": [
    "def extract_section_hierarchy(content: str) -> list[dict]:\n",
    "    \"\"\"Parse Wikipedia == headers == into hierarchical structure with positions.\"\"\"\n",
    "    header_pattern = re.compile(r'^(={2,6})\\s*(.+?)\\s*\\1\\s*$', re.MULTILINE)\n",
    "    \n",
    "    sections = []\n",
    "    current_path = []  # Stack of (level, title)\n",
    "    \n",
    "    for match in header_pattern.finditer(content):\n",
    "        level = len(match.group(1))  # Number of '=' signs\n",
    "        title = match.group(2).strip()\n",
    "        \n",
    "        # Pop stack until we're at parent level\n",
    "        while current_path and current_path[-1][0] >= level:\n",
    "            current_path.pop()\n",
    "        \n",
    "        current_path.append((level, title))\n",
    "        breadcrumb = \" > \".join(t for _, t in current_path)\n",
    "        \n",
    "        sections.append({\n",
    "            \"level\": level,\n",
    "            \"title\": title,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"start_pos\": match.start(),\n",
    "            \"end_pos\": match.end(),\n",
    "        })\n",
    "    \n",
    "    return sections\n",
    "\n",
    "\n",
    "def get_section_context(position: int, sections: list[dict], article_title: str) -> dict:\n",
    "    \"\"\"Find the section context for a given character position.\"\"\"\n",
    "    active_section = {\n",
    "        \"title\": \"Introduction\",\n",
    "        \"breadcrumb\": \"Introduction\",\n",
    "        \"level\": 1,\n",
    "    }\n",
    "    \n",
    "    for section in sections:\n",
    "        if section[\"start_pos\"] <= position:\n",
    "            active_section = section\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"section_title\": active_section[\"title\"],\n",
    "        \"breadcrumb\": f\"{article_title} > {active_section['breadcrumb']}\",\n",
    "    }\n",
    "\n",
    "print(\"Section hierarchy parser defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360c37f",
   "metadata": {},
   "source": [
    "## Fetch Wikipedia Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b91833b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched: Albert Einstein\n",
      "Source URL: https://en.wikipedia.org/wiki/Albert_Einstein\n",
      "Content length: 87959 characters\n",
      "License: CC BY-SA 4.0\n"
     ]
    }
   ],
   "source": [
    "# Fetch Wikipedia article\n",
    "loader = WikipediaLoader(query=ARTICLE_TITLE, load_max_docs=1, doc_content_chars_max=100000)\n",
    "docs = loader.load()\n",
    "\n",
    "if not docs:\n",
    "    raise ValueError(f\"Could not fetch article: {ARTICLE_TITLE}\")\n",
    "\n",
    "raw_content = docs[0].page_content\n",
    "metadata = docs[0].metadata\n",
    "\n",
    "# Construct source URL and provenance\n",
    "source_url = f\"https://en.wikipedia.org/wiki/{quote(ARTICLE_TITLE.replace(' ', '_'))}\"\n",
    "\n",
    "provenance = {\n",
    "    \"source_url\": source_url,\n",
    "    \"article_title\": ARTICLE_TITLE,\n",
    "    \"fetched_at\": datetime.now().isoformat(),\n",
    "    \"content_length\": len(raw_content),\n",
    "    # Wikipedia license - standard for all Wikipedia content\n",
    "    \"license\": \"CC BY-SA 4.0\",\n",
    "    \"license_url\": \"https://creativecommons.org/licenses/by-sa/4.0/\",\n",
    "    \"attribution\": \"Wikipedia contributors\",\n",
    "}\n",
    "\n",
    "print(f\"Fetched: {ARTICLE_TITLE}\")\n",
    "print(f\"Source URL: {source_url}\")\n",
    "print(f\"Content length: {len(raw_content)} characters\")\n",
    "print(f\"License: {provenance['license']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1276b5",
   "metadata": {},
   "source": [
    "## Create Contextual Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3146caad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 sections\n",
      "Split into 63 chunks\n",
      "\n",
      "Chunks with context:\n",
      "  Chunk 1: Albert Einstein > Introduction\n",
      "  Chunk 2: Albert Einstein > Introduction\n",
      "  Chunk 3: Albert Einstein > Life and career\n"
     ]
    }
   ],
   "source": [
    "# Parse section hierarchy\n",
    "sections = extract_section_hierarchy(raw_content)\n",
    "print(f\"Found {len(sections)} sections\")\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "raw_chunks = splitter.split_text(raw_content)\n",
    "print(f\"Split into {len(raw_chunks)} chunks\")\n",
    "\n",
    "# Add context to each chunk\n",
    "@dataclass\n",
    "class ContextualChunk:\n",
    "    content: str\n",
    "    chunk_index: int\n",
    "    total_chunks: int\n",
    "    breadcrumb: str\n",
    "    section_title: str\n",
    "    char_start: int\n",
    "    char_end: int\n",
    "\n",
    "contextual_chunks = []\n",
    "current_pos = 0\n",
    "\n",
    "for i, chunk_text in enumerate(raw_chunks):\n",
    "    # Find position in original content\n",
    "    chunk_start = raw_content.find(chunk_text, current_pos)\n",
    "    if chunk_start == -1:\n",
    "        chunk_start = current_pos  # Fallback\n",
    "    chunk_end = chunk_start + len(chunk_text)\n",
    "    \n",
    "    # Get section context\n",
    "    context = get_section_context(chunk_start, sections, ARTICLE_TITLE)\n",
    "    \n",
    "    contextual_chunks.append(ContextualChunk(\n",
    "        content=chunk_text,\n",
    "        chunk_index=i,\n",
    "        total_chunks=len(raw_chunks),\n",
    "        breadcrumb=context[\"breadcrumb\"],\n",
    "        section_title=context[\"section_title\"],\n",
    "        char_start=chunk_start,\n",
    "        char_end=chunk_end,\n",
    "    ))\n",
    "    \n",
    "    current_pos = chunk_start + 1\n",
    "\n",
    "print(f\"\\nChunks with context:\")\n",
    "for chunk in contextual_chunks[:3]:\n",
    "    print(f\"  Chunk {chunk.chunk_index + 1}: {chunk.breadcrumb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3af602",
   "metadata": {},
   "source": [
    "## Initialize Entity Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34b0e8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity registry initialized with subject: Albert Einstein\n",
      "Subject URI: https://en.wikipedia.org/wiki/Albert_Einstein#person_albert_einstein\n"
     ]
    }
   ],
   "source": [
    "# Initialize entity registry with article subject\n",
    "registry = EntityRegistry(source_url=source_url)\n",
    "\n",
    "# Pre-seed with the article subject\n",
    "registry.register(\n",
    "    label=ARTICLE_TITLE,\n",
    "    entity_type=\"Person\",  # Adjust based on article type\n",
    "    description=f\"Subject of Wikipedia article: {ARTICLE_TITLE}\",\n",
    "    aliases=[ARTICLE_TITLE.split()[-1]],  # Last name as alias\n",
    ")\n",
    "\n",
    "print(f\"Entity registry initialized with subject: {ARTICLE_TITLE}\")\n",
    "print(f\"Subject URI: {registry.entities[registry.normalize_key(ARTICLE_TITLE)]['uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd3cdd",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "These prompts are embedded in the generated notebooks for transparency and adjustability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3089027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates defined\n"
     ]
    }
   ],
   "source": [
    "FACTS_EXTRACTION_PROMPT = \"\"\"You are an expert at extracting factual information from text.\n",
    "\n",
    "Given text from a Wikipedia article, extract simple English statements that capture the consensus factual information. Each statement should:\n",
    "- Be a single, clear sentence\n",
    "- Contain one main fact or relationship\n",
    "- Use the full name of entities on first mention\n",
    "- Be verifiable from the source text\n",
    "- Avoid opinions, interpretations, or hedged language\n",
    "\n",
    "The text comes from: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Known entities (use consistent names):\n",
    "{known_entities}\n",
    "\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\n",
    "Extract factual statements as a bulleted list. Also identify any new entities (people, places, organizations, concepts, events, works) that should be added to the registry.\n",
    "\"\"\"\n",
    "\n",
    "RDF_GENERATION_PROMPT = \"\"\"You are an expert at converting factual statements to RDF triples in Turtle format.\n",
    "\n",
    "Convert the following factual statements to RDF using schema.org vocabulary where possible.\n",
    "\n",
    "Source: {source_url}\n",
    "Section: {breadcrumb}\n",
    "\n",
    "Use these prefixes:\n",
    "{prefixes}\n",
    "\n",
    "Entity registry (use these URIs):\n",
    "{entity_registry}\n",
    "\n",
    "Guidelines:\n",
    "- Use schema.org properties (schema:birthDate, schema:birthPlace, schema:worksFor, etc.)\n",
    "- For relationships not in schema.org, use wiki3: prefix\n",
    "- Include rdfs:label for entities\n",
    "- Use xsd datatypes for dates and numbers\n",
    "- Entity URIs should use the source URL as base with fragment identifiers\n",
    "\n",
    "---\n",
    "{facts}\n",
    "---\n",
    "\n",
    "Generate Turtle RDF:\n",
    "\"\"\"\n",
    "\n",
    "RDF_PREFIXES = \"\"\"@prefix schema: <https://schema.org/> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix wiki3: <https://wiki3.ai/vocab/> .\n",
    "@base <{source_url}> .\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prompt templates defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa35e61",
   "metadata": {},
   "source": [
    "## Generate Chunks Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d121080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: data/albert_einstein_chunks.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_chunks_notebook(chunks: list, provenance: dict, registry: EntityRegistry, \n",
    "                             llm_config: dict, output_path: str):\n",
    "    \"\"\"Generate a notebook with chunked source text and context metadata.\n",
    "    \n",
    "    Each chunk cell is followed by a signature raw cell with its CID.\n",
    "    \"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    # Cell 0: Provenance markdown\n",
    "    provenance_yaml = f\"\"\"# Chunked Text: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "fetched_at: {provenance['fetched_at']}\n",
    "content_length: {provenance['content_length']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "attribution: {provenance['attribution']}\n",
    "chunk_size: {CHUNK_SIZE}\n",
    "chunk_overlap: {CHUNK_OVERLAP}\n",
    "total_chunks: {len(chunks)}\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "Each chunk below contains source text with contextual metadata. The context line (before the separator) provides:\n",
    "- **Context**: Hierarchical breadcrumb showing article > section path\n",
    "- **Chunk**: Position in sequence\n",
    "\n",
    "The text below the `---` separator is the unchanged source content.\n",
    "Each chunk is followed by a signature cell containing its Content ID (CID).\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_yaml))\n",
    "    \n",
    "    # Cell 1: Entity registry (raw cell)\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    # Chunk cells with signatures\n",
    "    # Compute CID of raw source for provenance\n",
    "    source_cid = compute_cid(provenance['source_url'] + str(provenance['content_length']))\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Content cell\n",
    "        chunk_content = f\"\"\"**Context:** {chunk.breadcrumb}\n",
    "**Chunk:** {chunk.chunk_index + 1} of {chunk.total_chunks}\n",
    "\n",
    "---\n",
    "\n",
    "{chunk.content}\n",
    "\"\"\"\n",
    "        nb.cells.append(new_markdown_cell(chunk_content))\n",
    "        \n",
    "        # Signature cell\n",
    "        chunk_cid = compute_cid(chunk_content)\n",
    "        signature = make_signature(\n",
    "            cell_num=chunk.chunk_index + 1,\n",
    "            cell_type=\"chunk\",\n",
    "            cid=chunk_cid,\n",
    "            from_cid=source_cid\n",
    "        )\n",
    "        nb.cells.append(new_raw_cell(json.dumps(signature)))\n",
    "    \n",
    "    # Write notebook\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(nb, f)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate chunks notebook\n",
    "article_slug = ARTICLE_TITLE.lower().replace(' ', '_')\n",
    "chunks_path = os.path.join(OUTPUT_DIR, f\"{article_slug}_chunks.ipynb\")\n",
    "generate_chunks_notebook(contextual_chunks, provenance, registry, LLM_CONFIG, chunks_path)\n",
    "print(f\"Generated: {chunks_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c23dc4",
   "metadata": {},
   "source": [
    "## Generate Facts Notebook (Structure Only)\n",
    "\n",
    "Creates the facts notebook with placeholders. Actual content is generated in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd4acab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts notebook exists: data/albert_einstein_facts.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_facts_notebook_header(provenance: dict, registry: EntityRegistry,\n",
    "                                    llm_config: dict, prompt_template: str) -> nbformat.NotebookNode:\n",
    "    \"\"\"Generate just the header cells for facts notebook.\"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    provenance_md = f\"\"\"# Factual Statements: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "source_notebook: {article_slug}_chunks.ipynb\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "llm_provider: {llm_config['provider']}\n",
    "llm_model: {llm_config['model']}\n",
    "llm_temperature: {llm_config['temperature']}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "This notebook contains simple English factual statements extracted from source text chunks.\n",
    "Each content cell corresponds to one chunk from the source notebook.\n",
    "Each content cell is followed by a signature cell with CID provenance.\n",
    "\n",
    "To regenerate a specific cell: delete both the content cell and its signature, then re-run the pipeline.\n",
    "\n",
    "## Prompt Template\n",
    "\n",
    "```\n",
    "{prompt_template}\n",
    "```\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_md))\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    return nb\n",
    "\n",
    "# Create initial facts notebook with just header (no placeholders)\n",
    "facts_path = os.path.join(OUTPUT_DIR, f\"{article_slug}_facts.ipynb\")\n",
    "\n",
    "# Only create if doesn't exist\n",
    "if not os.path.exists(facts_path):\n",
    "    facts_nb = generate_facts_notebook_header(provenance, registry, LLM_CONFIG, FACTS_EXTRACTION_PROMPT)\n",
    "    with open(facts_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(facts_nb, f)\n",
    "    print(f\"Created facts notebook: {facts_path}\")\n",
    "else:\n",
    "    print(f\"Facts notebook exists: {facts_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256713fd",
   "metadata": {},
   "source": [
    "## Generate RDF Notebook (Structure Only)\n",
    "\n",
    "Creates the RDF notebook with placeholders. Actual content is generated after facts extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a7ae750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF notebook exists: data/albert_einstein_rdf.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_rdf_notebook_header(provenance: dict, registry: EntityRegistry,\n",
    "                                  llm_config: dict, prompt_template: str, prefixes: str) -> nbformat.NotebookNode:\n",
    "    \"\"\"Generate just the header cells for RDF notebook.\"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    formatted_prefixes = prefixes.format(source_url=provenance['source_url'])\n",
    "    provenance_md = f\"\"\"# RDF Triples: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "source_notebook: {article_slug}_facts.ipynb\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "llm_provider: {llm_config['provider']}\n",
    "llm_model: {llm_config['model']}\n",
    "llm_temperature: {llm_config['temperature']}\n",
    "rdf_format: Turtle\n",
    "```\n",
    "\n",
    "## RDF Prefixes\n",
    "\n",
    "```turtle\n",
    "{formatted_prefixes}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "This notebook contains RDF triples in Turtle format, one cell per source facts cell.\n",
    "Each content cell is followed by a signature cell with CID provenance.\n",
    "\n",
    "To regenerate a specific cell: delete both the content cell and its signature, then re-run the pipeline.\n",
    "\n",
    "## Prompt Template\n",
    "\n",
    "```\n",
    "{prompt_template}\n",
    "```\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_md))\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    return nb\n",
    "\n",
    "# Create initial RDF notebook with just header (no placeholders)\n",
    "rdf_path = os.path.join(OUTPUT_DIR, f\"{article_slug}_rdf.ipynb\")\n",
    "\n",
    "# Only create if doesn't exist\n",
    "if not os.path.exists(rdf_path):\n",
    "    rdf_nb = generate_rdf_notebook_header(provenance, registry, LLM_CONFIG, RDF_GENERATION_PROMPT, RDF_PREFIXES)\n",
    "    with open(rdf_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(rdf_nb, f)\n",
    "    print(f\"Created RDF notebook: {rdf_path}\")\n",
    "else:\n",
    "    print(f\"RDF notebook exists: {rdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696afdf",
   "metadata": {},
   "source": [
    "## Process Chunks → Extract Facts\n",
    "\n",
    "Run the LLM on each chunk to extract factual statements and update the facts notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2350ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading chunks notebook...\n",
      "Found 63 chunks with CIDs\n",
      "Found 63 existing fact signatures\n",
      "HTTP timeout: 60s per request\n",
      "--------------------------------------------------\n",
      "  Chunk 1: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 2: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 3: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 4: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 5: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 6: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 7: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 8: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 9: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 10: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 11: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 12: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 13: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 14: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 15: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 16: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 17: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 18: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 19: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 20: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 21: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 22: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 23: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 24: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 25: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 26: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 27: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 28: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 29: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 30: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 31: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 32: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 33: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 34: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 35: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 36: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 37: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 38: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 39: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 40: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 41: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 42: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 43: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 44: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 45: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 46: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 47: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 48: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 49: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 50: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 51: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 52: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 53: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 54: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 55: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 56: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 57: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 58: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 59: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 60: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 61: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 62: ⊘ Up-to-date (CID match), skipping\n",
      "  Chunk 63: ⊘ Up-to-date (CID match), skipping\n",
      "--------------------------------------------------\n",
      "Facts extraction complete:\n",
      "  - 0 generated\n",
      "  - 63 skipped (up-to-date)\n",
      "  - 0 errors/timeouts\n"
     ]
    }
   ],
   "source": [
    "# Create LLM client with timeout for facts extraction\n",
    "facts_llm = ChatOpenAI(\n",
    "    model=LLM_CONFIG[\"model\"],\n",
    "    temperature=LLM_CONFIG[\"temperature\"],\n",
    "    base_url=LLM_CONFIG[\"base_url\"],\n",
    "    api_key=\"lm-studio\",\n",
    "    timeout=CELL_TIMEOUT_SECONDS,  # HTTP timeout in seconds\n",
    "    max_retries=0,  # Don't retry on timeout\n",
    ")\n",
    "\n",
    "# Create facts extraction prompt\n",
    "facts_prompt = ChatPromptTemplate.from_template(\"\"\"You are an expert at extracting factual information from text.\n",
    "\n",
    "Given text from a Wikipedia article, extract simple English statements that capture the consensus factual information. Each statement should:\n",
    "- Be a single, clear sentence\n",
    "- Contain one main fact or relationship\n",
    "- Use the full name of entities on first mention\n",
    "- Be verifiable from the source text\n",
    "- Avoid opinions, interpretations, or hedged language\n",
    "\n",
    "Source: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Known entities (use consistent names):\n",
    "{known_entities}\n",
    "\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\n",
    "Extract factual statements as a bulleted list:\"\"\")\n",
    "\n",
    "facts_chain = facts_prompt | facts_llm\n",
    "\n",
    "def get_known_entities_text(registry: EntityRegistry) -> str:\n",
    "    \"\"\"Format known entities for prompt context.\"\"\"\n",
    "    lines = []\n",
    "    for entity in registry.entities.values():\n",
    "        lines.append(f\"- {entity['label']} ({entity['type']})\")\n",
    "    return \"\\n\".join(lines) if lines else \"None yet\"\n",
    "\n",
    "# Read chunks notebook to get source content and CIDs\n",
    "log_progress(\"Reading chunks notebook...\")\n",
    "chunks_nb = nbformat.read(chunks_path, as_version=4)\n",
    "chunk_signatures = extract_signatures(chunks_nb)\n",
    "\n",
    "# Build list of chunk content with CIDs\n",
    "# Chunks notebook structure: [provenance, registry, chunk1, sig1, chunk2, sig2, ...]\n",
    "chunk_data = []\n",
    "cell_idx = 2  # Skip provenance and registry\n",
    "while cell_idx < len(chunks_nb.cells):\n",
    "    cell = chunks_nb.cells[cell_idx]\n",
    "    if cell.cell_type == 'markdown':\n",
    "        content = cell.source\n",
    "        # Get corresponding signature (next cell)\n",
    "        sig = None\n",
    "        if cell_idx + 1 < len(chunks_nb.cells):\n",
    "            sig = parse_signature(chunks_nb.cells[cell_idx + 1].source)\n",
    "        \n",
    "        # Extract breadcrumb\n",
    "        context_match = re.search(r'\\*\\*Context:\\*\\*\\s*(.+)', content)\n",
    "        breadcrumb = context_match.group(1) if context_match else \"Unknown\"\n",
    "        \n",
    "        # Extract chunk text (after ---)\n",
    "        parts = content.split(\"---\\n\", 1)\n",
    "        chunk_text = parts[1].strip() if len(parts) > 1 else content\n",
    "        \n",
    "        chunk_data.append({\n",
    "            \"cell_num\": sig[\"cell\"] if sig else len(chunk_data) + 1,\n",
    "            \"content\": content,\n",
    "            \"chunk_text\": chunk_text,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"cid\": sig[\"cid\"] if sig else compute_cid(content),\n",
    "        })\n",
    "        cell_idx += 2  # Skip content and signature\n",
    "    else:\n",
    "        cell_idx += 1\n",
    "\n",
    "log_progress(f\"Found {len(chunk_data)} chunks with CIDs\")\n",
    "\n",
    "# Read existing facts notebook and extract signatures\n",
    "facts_nb = nbformat.read(facts_path, as_version=4)\n",
    "facts_signatures = extract_signatures(facts_nb)\n",
    "\n",
    "log_progress(f\"Found {len(facts_signatures)} existing fact signatures\")\n",
    "log_progress(f\"HTTP timeout: {CELL_TIMEOUT_SECONDS}s per request\")\n",
    "log_progress(\"-\" * 50)\n",
    "\n",
    "# Process each chunk\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for chunk in chunk_data:\n",
    "    cell_num = chunk[\"cell_num\"]\n",
    "    source_cid = chunk[\"cid\"]\n",
    "    \n",
    "    # Check if we already have up-to-date facts for this chunk\n",
    "    existing_sig = facts_signatures.get(cell_num)\n",
    "    if existing_sig and existing_sig[\"from_cid\"] == source_cid:\n",
    "        log_progress(f\"  Chunk {cell_num}: ⊘ Up-to-date (CID match), skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Need to generate (or regenerate) this cell\n",
    "    status = \"↻ Regenerating\" if existing_sig else \"+ Generating\"\n",
    "    chunk_preview = chunk[\"chunk_text\"][:80].replace('\\n', ' ')\n",
    "    log_progress(f\"  Chunk {cell_num}: {status}\")\n",
    "    log_progress(f\"    Context: {chunk['breadcrumb']}\")\n",
    "    log_progress(f\"    Input: {chunk_preview}...\")\n",
    "    log_progress(f\"    [Calling LLM...]\", end=\" \")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Call LLM to extract facts\n",
    "    try:\n",
    "        result = facts_chain.invoke({\n",
    "            \"source_url\": provenance[\"source_url\"],\n",
    "            \"breadcrumb\": chunk[\"breadcrumb\"],\n",
    "            \"known_entities\": get_known_entities_text(registry),\n",
    "            \"chunk_text\": chunk[\"chunk_text\"],\n",
    "        })\n",
    "        facts_content = result.content\n",
    "        elapsed = time.time() - start_time\n",
    "        log_progress(f\"✓ ({len(facts_content)} chars, {elapsed:.1f}s)\")\n",
    "        processed_count += 1\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)[:100]\n",
    "        facts_content = f\"# Error: {error_type}: {e}\"\n",
    "        log_progress(f\"✗ {error_type} after {elapsed:.1f}s\")\n",
    "        log_progress(f\"    Error: {error_msg}\")\n",
    "        error_count += 1\n",
    "    \n",
    "    # Build the facts cell content\n",
    "    facts_cell_content = f\"\"\"**Context:** {chunk['breadcrumb']}\n",
    "**Chunk:** {cell_num} of {len(chunk_data)}\n",
    "\n",
    "---\n",
    "\n",
    "{facts_content}\n",
    "\"\"\"\n",
    "    facts_cid = compute_cid(facts_cell_content)\n",
    "    signature = make_signature(cell_num, \"facts\", facts_cid, source_cid)\n",
    "    \n",
    "    # Find where to insert/update in the notebook\n",
    "    # If there's an existing signature for this cell, find and remove old content+sig\n",
    "    if existing_sig:\n",
    "        # Find and remove the old cells\n",
    "        new_cells = [facts_nb.cells[0], facts_nb.cells[1]]  # Keep header\n",
    "        i = 2\n",
    "        while i < len(facts_nb.cells):\n",
    "            cell = facts_nb.cells[i]\n",
    "            if cell.cell_type == 'raw':\n",
    "                sig = parse_signature(cell.source)\n",
    "                if sig and sig[\"cell\"] == cell_num:\n",
    "                    # Skip this signature and its preceding content cell\n",
    "                    i += 1\n",
    "                    continue\n",
    "            # Check if this is content for the cell we're replacing\n",
    "            if i > 0 and i + 1 < len(facts_nb.cells):\n",
    "                next_sig = parse_signature(facts_nb.cells[i + 1].source) if facts_nb.cells[i + 1].cell_type == 'raw' else None\n",
    "                if next_sig and next_sig[\"cell\"] == cell_num:\n",
    "                    i += 2  # Skip content and signature\n",
    "                    continue\n",
    "            new_cells.append(cell)\n",
    "            i += 1\n",
    "        facts_nb.cells = new_cells\n",
    "    \n",
    "    # Append new content and signature\n",
    "    facts_nb.cells.append(new_markdown_cell(facts_cell_content))\n",
    "    facts_nb.cells.append(new_raw_cell(json.dumps(signature)))\n",
    "    \n",
    "    # Update signatures dict\n",
    "    facts_signatures[cell_num] = signature\n",
    "    \n",
    "    # Save notebook after each cell\n",
    "    log_progress(f\"    [Saving notebook...]\", end=\" \")\n",
    "    with open(facts_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(facts_nb, f)\n",
    "    log_progress(\"saved\")\n",
    "\n",
    "log_progress(\"-\" * 50)\n",
    "log_progress(f\"Facts extraction complete:\")\n",
    "log_progress(f\"  - {processed_count} generated\")\n",
    "log_progress(f\"  - {skipped_count} skipped (up-to-date)\")\n",
    "log_progress(f\"  - {error_count} errors/timeouts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d29b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts notebook: data/albert_einstein_facts.ipynb\n",
      "  - Updated incrementally during processing\n",
      "  - Ready for review/editing before RDF generation\n"
     ]
    }
   ],
   "source": [
    "# Summary of facts extraction (notebook already updated incrementally)\n",
    "print(f\"Facts notebook: {facts_path}\")\n",
    "print(f\"  - Updated incrementally during processing\")\n",
    "print(f\"  - Ready for review/editing before RDF generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de7f1f",
   "metadata": {},
   "source": [
    "## Process Facts → Generate RDF\n",
    "\n",
    "Run the LLM on each facts cell to generate RDF triples and update the RDF notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afd0797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading facts notebook...\n",
      "Found 63 facts cells with CIDs\n",
      "Found 3 existing RDF signatures\n",
      "HTTP timeout: 60s per request\n",
      "--------------------------------------------------\n",
      "  Facts 1: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 2: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 3: ⊘ Up-to-date (CID match), skipping\n",
      "  Facts 4: + Generating\n",
      "    Context: Albert Einstein > Life and career > Childhood, youth and education\n",
      "    Input: • In 1894, Hermann and Jakob's company tendered for a contract to install electr...\n",
      "    [Calling LLM...] ✗ APITimeoutError after 60.1s\n",
      "    Error: Request timed out.\n",
      "    [Saving notebook...] saved\n",
      "  Facts 5: + Generating\n",
      "    Context: Albert Einstein > Life and career > Childhood, youth and education\n",
      "    Input: • Albert Einstein excelled at physics and mathematics from an early age. • Alber...\n",
      "    [Calling LLM...] ✗ APITimeoutError after 60.1s\n",
      "    Error: Request timed out.\n",
      "    [Saving notebook...] saved\n",
      "  Facts 6: + Generating\n",
      "    Context: Albert Einstein > Life and career > Childhood, youth and education\n",
      "    Input: • Albert Einstein was thirteen years old when he was introduced to Kant's Critiq...\n",
      "    [Calling LLM...] ✓ (2212 chars, 8.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 7: + Generating\n",
      "    Context: Albert Einstein > Life and career > Childhood, youth and education\n",
      "    Input: • In January 1896, Albert Einstein renounced his citizenship of the German Kingd...\n",
      "    [Calling LLM...] ✓ (4155 chars, 15.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 8: + Generating\n",
      "    Context: Albert Einstein > Life and career > Marriages, relationships and children\n",
      "    Input: - Albert Einstein married Mileva Marić in January 1903. - In early 1902, Mileva ...\n",
      "    [Calling LLM...] ✓ (7453 chars, 30.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 9: + Generating\n",
      "    Context: Albert Einstein > Life and career > Marriages, relationships and children\n",
      "    Input: • Albert Einstein was married to his first wife, Mileva Marić. • Albert Einstein...\n",
      "    [Calling LLM...] ✓ (2219 chars, 8.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 10: + Generating\n",
      "    Context: Albert Einstein > Life and career > Assistant at the Swiss Patent Office (1902–1909)\n",
      "    Input: • Albert Einstein graduated from the federal polytechnic school in 1900 and was ...\n",
      "    [Calling LLM...] ✗ APITimeoutError after 60.1s\n",
      "    Error: Request timed out.\n",
      "    [Saving notebook...] saved\n",
      "  Facts 11: + Generating\n",
      "    Context: Albert Einstein > Life and career > First scientific papers (1900–1905)\n",
      "    Input: - Albert Einstein's first scientific paper, titled \"Folgerungen aus den Capillar...\n",
      "    [Calling LLM...] ✓ (4811 chars, 18.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 12: + Generating\n",
      "    Context: Albert Einstein > Life and career > Academic career in Europe (1908–1933)\n",
      "    Input: • Albert Einstein attended the first Solvay Conference on Physics from 30 Octobe...\n",
      "    [Calling LLM...] ✓ (4387 chars, 16.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 13: + Generating\n",
      "    Context: Albert Einstein > Life and career > Academic career in Europe (1908–1933)\n",
      "    Input: - Albert Einstein was elected president of the German Physical Society in 1916. ...\n",
      "    [Calling LLM...] ✓ (3403 chars, 12.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 14: + Generating\n",
      "    Context: Albert Einstein > Life and career > Academic career in Europe (1908–1933)\n",
      "    Input: • Albert Einstein resigned from the Prussian Academy in March 1933. • Albert Ein...\n",
      "    [Calling LLM...] ✓ (1984 chars, 7.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 15: + Generating\n",
      "    Context: Albert Einstein > Life and career > Putting general relativity to the test (1919)\n",
      "    Input: • In 1907, Albert Einstein formulated the equivalence principle, which states th...\n",
      "    [Calling LLM...] ✓ (4874 chars, 17.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 16: + Generating\n",
      "    Context: Albert Einstein > Life and career > Coming to terms with fame (1921–1923)\n",
      "    Input: • Albert Einstein became \"perhaps the world's first celebrity scientist\" after E...\n",
      "    [Calling LLM...] ✓ (3126 chars, 11.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 17: + Generating\n",
      "    Context: Albert Einstein > Life and career > Coming to terms with fame (1921–1923)\n",
      "    Input: • In 1922, Albert Einstein undertook a six-month tour of Asia. • During his Asia...\n",
      "    [Calling LLM...] ✓ (3241 chars, 12.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 18: + Generating\n",
      "    Context: Albert Einstein > Life and career > Coming to terms with fame (1921–1923)\n",
      "    Input: • Albert Einstein decided to tour the eastern hemisphere in 1922. • Einstein was...\n",
      "    [Calling LLM...] ✓ (6213 chars, 24.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 19: + Generating\n",
      "    Context: Albert Einstein > Life and career > Serving the League of Nations (1922–1932)\n",
      "    Input: • Albert Einstein was a member of the International Committee on Intellectual Co...\n",
      "    [Calling LLM...] ✓ (3588 chars, 13.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 20: + Generating\n",
      "    Context: Albert Einstein > Life and career > Touring the US (1930–1931)\n",
      "    Input: • Albert Einstein began a sojourn in the United States in December 1930 • Einste...\n",
      "    [Calling LLM...] ✓ (2221 chars, 8.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 21: + Generating\n",
      "    Context: Albert Einstein > Life and career > Touring the US (1930–1931)\n",
      "    Input: • Albert Einstein traveled to California in 1930-1931. • In California, Albert E...\n",
      "    [Calling LLM...] ✓ (4936 chars, 18.8s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 22: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933)\n",
      "    Input: • Albert Einstein was in the United States in February 1933 while on a visit. • ...\n",
      "    [Calling LLM...] ✓ (2253 chars, 9.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 23: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > Refugee status\n",
      "    Input: • In April 1933, Albert Einstein discovered that the new German government had p...\n",
      "    [Calling LLM...] ✗ APITimeoutError after 60.1s\n",
      "    Error: Request timed out.\n",
      "    [Saving notebook...] saved\n",
      "  Facts 24: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > Refugee status\n",
      "    Input: • Albert Einstein was without a permanent home and unsure where he would live an...\n",
      "    [Calling LLM...] ✓ (4572 chars, 17.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 25: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > Refugee status\n",
      "    Input: • Albert Einstein contacted British leaders including Winston Churchill, Austen ...\n",
      "    [Calling LLM...] ✗ APITimeoutError after 60.1s\n",
      "    Error: Request timed out.\n",
      "    [Saving notebook...] saved\n",
      "  Facts 26: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > Resident scholar at the Institute for Advanced Study\n",
      "    Input: • Albert Einstein was a resident scholar at the Institute for Advanced Study in ...\n",
      "    [Calling LLM...] ✗ APITimeoutError after 60.1s\n",
      "    Error: Request timed out.\n",
      "    [Saving notebook...] saved\n",
      "  Facts 27: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > World War II and the Manhattan Project\n",
      "    Input: • In 1939, a group of Hungarian scientists including émigré physicist Leó Szilár...\n",
      "    [Calling LLM...] ✗ APITimeoutError after 60.1s\n",
      "    Error: Request timed out.\n",
      "    [Saving notebook...] saved\n",
      "  Facts 28: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > World War II and the Manhattan Project\n",
      "    Input: • Albert Einstein called war a disease and advocated for resistance to war. • Al...\n",
      "    [Calling LLM...] ✓ (3515 chars, 12.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 29: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > US citizenship\n",
      "    Input: • Albert Einstein became a naturalized citizen of the United States in 1940. • A...\n",
      "    [Calling LLM...] ✓ (1229 chars, 4.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 30: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > US citizenship\n",
      "    Input: • Albert Einstein became an American citizen in 1940 • Albert Einstein settled i...\n",
      "    [Calling LLM...] ✓ (2441 chars, 9.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 31: + Generating\n",
      "    Context: Albert Einstein > Life and career > Emigration to the US (1933) > US citizenship\n",
      "    Input: • In 1946, Albert Einstein visited Lincoln University in Pennsylvania, a histori...\n",
      "    [Calling LLM...] ✓ (4838 chars, 18.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 32: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views\n",
      "    Input: • Albert Einstein was one of the signatories of the founding proclamation of the...\n",
      "    [Calling LLM...] ✓ (2075 chars, 9.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 33: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Political views\n",
      "    Input: • Albert Einstein honored Lenin as a man who sacrificed himself to realize socia...\n",
      "    [Calling LLM...] ✓ (3999 chars, 16.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 34: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Relationship with Zionism\n",
      "    Input: • Albert Einstein was a figurehead leader in the establishment of the Hebrew Uni...\n",
      "    [Calling LLM...] ✗ APITimeoutError after 60.1s\n",
      "    Error: Request timed out.\n",
      "    [Saving notebook...] saved\n",
      "  Facts 35: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Religious and philosophical views\n",
      "    Input: • Albert Einstein said he had sympathy for the impersonal pantheistic God of Bar...\n",
      "    [Calling LLM...] ✓ (4736 chars, 16.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 36: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Religious and philosophical views\n",
      "    Input: • Albert Einstein believed that the word God represents human weaknesses and tha...\n",
      "    [Calling LLM...] ✓ (3041 chars, 10.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 37: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Love of music\n",
      "    Input: • Albert Einstein said \"If I were not a physicist, I would probably be a musicia...\n",
      "    [Calling LLM...] ✓ (2984 chars, 11.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 38: + Generating\n",
      "    Context: Albert Einstein > Life and career > Personal views > Love of music\n",
      "    Input: • Music became a pivotal and permanent part of Albert Einstein's life. • Albert ...\n",
      "    [Calling LLM...] ✓ (3552 chars, 13.2s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 39: + Generating\n",
      "    Context: Albert Einstein > Life and career > Death\n",
      "    Input: • Albert Einstein died on 17 April 1955 at the age of 76 • Einstein died from in...\n",
      "    [Calling LLM...] ✓ (3143 chars, 12.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 40: + Generating\n",
      "    Context: Albert Einstein > Scientific career\n",
      "    Input: • Albert Einstein published more than 300 scientific papers and 150 non-scientif...\n",
      "    [Calling LLM...] ✓ (3040 chars, 11.8s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 41: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Statistical mechanics > Theory of critical opalescence\n",
      "    Input: • Albert Einstein returned to the problem of thermodynamic fluctuations and gave...\n",
      "    [Calling LLM...] ✓ (6580 chars, 24.0s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 42: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Special relativity\n",
      "    Input: • Albert Einstein's paper \"On the Electrodynamics of Moving Bodies\" was received...\n",
      "    [Calling LLM...] ✓ (3720 chars, 13.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 43: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity\n",
      "    Input: • Albert Einstein developed the theory of general relativity between 1907 and 19...\n",
      "    [Calling LLM...] ✓ (3567 chars, 13.6s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 44: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Gravitational waves\n",
      "    Input: • In 1916, Albert Einstein predicted gravitational waves, which are ripples in t...\n",
      "    [Calling LLM...] ✓ (3521 chars, 13.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 45: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Physical cosmology\n",
      "    Input: • In 1917, Albert Einstein applied the general theory of relativity to the struc...\n",
      "    [Calling LLM...] ✓ (3836 chars, 13.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 46: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Physical cosmology\n",
      "    Input: • In late 2013, a team led by the Irish physicist Cormac O'Raifeartaigh discover...\n",
      "    [Calling LLM...] ✓ (2517 chars, 8.3s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 47: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Energy momentum pseudotensor\n",
      "    Input: • Albert Einstein argued that energy and momentum in a gravitational field canno...\n",
      "    [Calling LLM...] ✓ (3872 chars, 13.5s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 48: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Einstein–Cartan theory\n",
      "    Input: • Albert Einstein and Élie Cartan made a modification to general relativity in t...\n",
      "    [Calling LLM...] ✓ (1284 chars, 4.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 49: + Generating\n",
      "    Context: Albert Einstein > Scientific career > General relativity > Equations of motion\n",
      "    Input: • Albert Einstein proposed that the equations of general relativity would determ...\n",
      "    [Calling LLM...] ✓ (5672 chars, 19.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 50: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Old quantum theory\n",
      "    Input: • In 1905, Albert Einstein postulated that light itself consists of localized pa...\n",
      "    [Calling LLM...] ✓ (4338 chars, 14.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 51: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Old quantum theory > Bose–Einstein statistics\n",
      "    Input: - In 1924, Albert Einstein received a description of a statistical model from In...\n",
      "    [Calling LLM...] ✗ APITimeoutError after 60.1s\n",
      "    Error: Request timed out.\n",
      "    [Saving notebook...] saved\n",
      "  Facts 52: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Old quantum theory > Zero-point energy\n",
      "    Input: • Albert Einstein worked with Otto Stern on zero-point energy theory from 1911 t...\n",
      "    [Calling LLM...] ✓ (4088 chars, 15.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 53: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Quantum mechanics\n",
      "    Input: • Albert Einstein played a major role in developing quantum theory beginning wit...\n",
      "    [Calling LLM...] ✓ (3079 chars, 13.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 54: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Quantum mechanics > Einstein–Podolsky–Rosen paradox\n",
      "    Input: • Albert Einstein never fully accepted quantum mechanics. • Einstein recognized ...\n",
      "    [Calling LLM...] ✓ (3559 chars, 13.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 55: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Quantum mechanics > Einstein–Podolsky–Rosen paradox\n",
      "    Input: • Albert Einstein published a paper with Boris Podolsky and Nathan Rosen in 1935...\n",
      "    [Calling LLM...] ✓ (3224 chars, 15.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 56: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Quantum mechanics > Einstein–Podolsky–Rosen paradox\n",
      "    Input: • John Stewart Bell carried the analysis of quantum entanglement much further in...\n",
      "    [Calling LLM...] ✗ APITimeoutError after 60.0s\n",
      "    Error: Request timed out.\n",
      "    [Saving notebook...] saved\n",
      "  Facts 57: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Unified field theory\n",
      "    Input: • Albert Einstein sought an ambitious geometrical theory that would treat gravit...\n",
      "    [Calling LLM...] ✓ (1805 chars, 5.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 58: + Generating\n",
      "    Context: Albert Einstein > Scientific career > Collaboration with other scientists > Einstein–de Haas experiment\n",
      "    Input: - Owen Willans Richardson predicted in 1908 that a change in the magnetic moment...\n",
      "    [Calling LLM...] ✓ (5789 chars, 21.7s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 59: + Generating\n",
      "    Context: Albert Einstein > Legacy\n",
      "    Input: - Albert Einstein wrote daily letters to his wife Elsa and adopted stepdaughters...\n",
      "    [Calling LLM...] ✓ (3572 chars, 14.9s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 60: + Generating\n",
      "    Context: Albert Einstein > Legacy > Scientific\n",
      "    Input: • In 1999, a survey of top physicists voted Albert Einstein as the \"greatest phy...\n",
      "    [Calling LLM...] ✓ (3309 chars, 14.5s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 61: + Generating\n",
      "    Context: Albert Einstein > In popular culture\n",
      "    Input: • Albert Einstein became one of the most famous scientific celebrities after the...\n",
      "    [Calling LLM...] ✓ (4094 chars, 15.1s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 62: + Generating\n",
      "    Context: Albert Einstein > Publications\n",
      "    Input: - Albert Einstein published an article titled \"To the editors of The New York Ti...\n",
      "    [Calling LLM...] ✓ (2736 chars, 11.4s)\n",
      "    [Saving notebook...] saved\n",
      "  Facts 63: + Generating\n",
      "    Context: Albert Einstein > External links > Archival materials collections\n",
      "    Input: • Albert Einstein Historical Letters, Documents & Papers are held at the Shapell...\n",
      "    [Calling LLM...] ✓ (1724 chars, 6.5s)\n",
      "    [Saving notebook...] saved\n",
      "--------------------------------------------------\n",
      "RDF generation complete:\n",
      "  - 50 generated\n",
      "  - 3 skipped (up-to-date)\n",
      "  - 10 errors/timeouts\n"
     ]
    }
   ],
   "source": [
    "# Create LLM client with timeout for RDF generation\n",
    "# Using request_timeout to enforce HTTP-level timeout\n",
    "rdf_llm = ChatOpenAI(\n",
    "    model=LLM_CONFIG[\"model\"],\n",
    "    temperature=LLM_CONFIG[\"temperature\"],\n",
    "    base_url=LLM_CONFIG[\"base_url\"],\n",
    "    api_key=\"lm-studio\",\n",
    "    timeout=CELL_TIMEOUT_SECONDS,  # HTTP timeout in seconds\n",
    "    max_retries=0,  # Don't retry on timeout\n",
    ")\n",
    "\n",
    "# Create RDF generation prompt\n",
    "rdf_prompt = ChatPromptTemplate.from_template(\"\"\"You are an expert at converting factual statements to RDF triples in Turtle format.\n",
    "\n",
    "Convert the following factual statements to RDF triples. Use:\n",
    "- schema.org vocabulary for common predicates (schema:name, schema:birthDate, etc.)\n",
    "- wiki3:vocab for domain-specific predicates\n",
    "- Entity URIs from the registry when available, otherwise create from source URL\n",
    "\n",
    "Source: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Entity Registry (use these URIs):\n",
    "{entity_registry}\n",
    "\n",
    "Factual statements:\n",
    "{facts}\n",
    "\n",
    "Generate Turtle RDF triples (no prefixes, use full URIs):\"\"\")\n",
    "\n",
    "rdf_chain = rdf_prompt | rdf_llm\n",
    "\n",
    "def format_entity_registry_for_prompt(registry: EntityRegistry) -> str:\n",
    "    \"\"\"Format registry for RDF prompt.\"\"\"\n",
    "    lines = []\n",
    "    for entity in registry.entities.values():\n",
    "        lines.append(f\"<{entity['uri']}> # {entity['label']} ({entity['type']})\")\n",
    "    return \"\\n\".join(lines) if lines else \"# No entities registered yet\"\n",
    "\n",
    "# Read facts notebook to get source content and CIDs\n",
    "log_progress(\"Reading facts notebook...\")\n",
    "facts_nb = nbformat.read(facts_path, as_version=4)\n",
    "facts_signatures = extract_signatures(facts_nb)\n",
    "\n",
    "# Build list of facts content with CIDs\n",
    "facts_data = []\n",
    "cell_idx = 2  # Skip provenance and registry\n",
    "while cell_idx < len(facts_nb.cells):\n",
    "    cell = facts_nb.cells[cell_idx]\n",
    "    if cell.cell_type == 'markdown':\n",
    "        content = cell.source\n",
    "        # Get corresponding signature (next cell)\n",
    "        sig = None\n",
    "        if cell_idx + 1 < len(facts_nb.cells):\n",
    "            sig = parse_signature(facts_nb.cells[cell_idx + 1].source)\n",
    "        \n",
    "        # Extract breadcrumb\n",
    "        context_match = re.search(r'\\*\\*Context:\\*\\*\\s*(.+)', content)\n",
    "        breadcrumb = context_match.group(1) if context_match else \"Unknown\"\n",
    "        \n",
    "        # Extract facts (after ---)\n",
    "        parts = content.split(\"---\\n\", 1)\n",
    "        facts_text = parts[1].strip() if len(parts) > 1 else content\n",
    "        \n",
    "        facts_data.append({\n",
    "            \"cell_num\": sig[\"cell\"] if sig else len(facts_data) + 1,\n",
    "            \"content\": content,\n",
    "            \"facts_text\": facts_text,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"cid\": sig[\"cid\"] if sig else compute_cid(content),\n",
    "        })\n",
    "        cell_idx += 2  # Skip content and signature\n",
    "    else:\n",
    "        cell_idx += 1\n",
    "\n",
    "log_progress(f\"Found {len(facts_data)} facts cells with CIDs\")\n",
    "\n",
    "# Read existing RDF notebook and extract signatures\n",
    "rdf_nb = nbformat.read(rdf_path, as_version=4)\n",
    "rdf_signatures = extract_signatures(rdf_nb)\n",
    "\n",
    "log_progress(f\"Found {len(rdf_signatures)} existing RDF signatures\")\n",
    "log_progress(f\"HTTP timeout: {CELL_TIMEOUT_SECONDS}s per request\")\n",
    "log_progress(\"-\" * 50)\n",
    "\n",
    "# Process each facts cell\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for facts_item in facts_data:\n",
    "    cell_num = facts_item[\"cell_num\"]\n",
    "    source_cid = facts_item[\"cid\"]\n",
    "    \n",
    "    # Skip cells that had errors in facts extraction\n",
    "    if facts_item[\"facts_text\"].startswith(\"# Error:\"):\n",
    "        log_progress(f\"  Facts {cell_num}: ⊘ Source had error, skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Check if we already have up-to-date RDF for these facts\n",
    "    existing_sig = rdf_signatures.get(cell_num)\n",
    "    if existing_sig and existing_sig[\"from_cid\"] == source_cid:\n",
    "        log_progress(f\"  Facts {cell_num}: ⊘ Up-to-date (CID match), skipping\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    # Need to generate (or regenerate) this cell\n",
    "    status = \"↻ Regenerating\" if existing_sig else \"+ Generating\"\n",
    "    facts_preview = facts_item[\"facts_text\"][:80].replace('\\n', ' ')\n",
    "    log_progress(f\"  Facts {cell_num}: {status}\")\n",
    "    log_progress(f\"    Context: {facts_item['breadcrumb']}\")\n",
    "    log_progress(f\"    Input: {facts_preview}...\")\n",
    "    log_progress(f\"    [Calling LLM...]\", end=\" \")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Call LLM to generate RDF\n",
    "    try:\n",
    "        result = rdf_chain.invoke({\n",
    "            \"source_url\": provenance[\"source_url\"],\n",
    "            \"breadcrumb\": facts_item[\"breadcrumb\"],\n",
    "            \"entity_registry\": format_entity_registry_for_prompt(registry),\n",
    "            \"facts\": facts_item[\"facts_text\"],\n",
    "        })\n",
    "        rdf_content = result.content\n",
    "        elapsed = time.time() - start_time\n",
    "        log_progress(f\"✓ ({len(rdf_content)} chars, {elapsed:.1f}s)\")\n",
    "        processed_count += 1\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        error_type = type(e).__name__\n",
    "        error_msg = str(e)[:100]\n",
    "        rdf_content = f\"# Error: {error_type}: {e}\"\n",
    "        log_progress(f\"✗ {error_type} after {elapsed:.1f}s\")\n",
    "        log_progress(f\"    Error: {error_msg}\")\n",
    "        error_count += 1\n",
    "    \n",
    "    # Build the RDF cell content\n",
    "    rdf_cell_content = f\"\"\"# Context: {facts_item['breadcrumb']}\n",
    "# Cell: {cell_num} of {len(facts_data)}\n",
    "\n",
    "{rdf_content}\n",
    "\"\"\"\n",
    "    rdf_cid = compute_cid(rdf_cell_content)\n",
    "    signature = make_signature(cell_num, \"rdf\", rdf_cid, source_cid)\n",
    "    \n",
    "    # Find where to insert/update in the notebook\n",
    "    # If there's an existing signature for this cell, find and remove old content+sig\n",
    "    if existing_sig:\n",
    "        # Find and remove the old cells\n",
    "        new_cells = [rdf_nb.cells[0], rdf_nb.cells[1]]  # Keep header\n",
    "        i = 2\n",
    "        while i < len(rdf_nb.cells):\n",
    "            cell = rdf_nb.cells[i]\n",
    "            if cell.cell_type == 'raw':\n",
    "                sig = parse_signature(cell.source)\n",
    "                if sig and sig[\"cell\"] == cell_num:\n",
    "                    # Skip this signature and its preceding content cell\n",
    "                    i += 1\n",
    "                    continue\n",
    "            # Check if this is content for the cell we're replacing\n",
    "            if i > 0 and i + 1 < len(rdf_nb.cells):\n",
    "                next_sig = parse_signature(rdf_nb.cells[i + 1].source) if rdf_nb.cells[i + 1].cell_type == 'raw' else None\n",
    "                if next_sig and next_sig[\"cell\"] == cell_num:\n",
    "                    i += 2  # Skip content and signature\n",
    "                    continue\n",
    "            new_cells.append(cell)\n",
    "            i += 1\n",
    "        rdf_nb.cells = new_cells\n",
    "    \n",
    "    # Append new content and signature\n",
    "    rdf_nb.cells.append(new_raw_cell(rdf_cell_content))\n",
    "    rdf_nb.cells.append(new_raw_cell(json.dumps(signature)))\n",
    "    \n",
    "    # Update signatures dict\n",
    "    rdf_signatures[cell_num] = signature\n",
    "    \n",
    "    # Save notebook after each cell\n",
    "    log_progress(f\"    [Saving notebook...]\", end=\" \")\n",
    "    with open(rdf_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(rdf_nb, f)\n",
    "    log_progress(\"saved\")\n",
    "\n",
    "log_progress(\"-\" * 50)\n",
    "log_progress(f\"RDF generation complete:\")\n",
    "log_progress(f\"  - {processed_count} generated\")\n",
    "log_progress(f\"  - {skipped_count} skipped (up-to-date)\")\n",
    "log_progress(f\"  - {error_count} errors/timeouts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de69a28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDF notebook: data/albert_einstein_rdf.ipynb\n",
      "  - Updated incrementally during processing\n",
      "  - Ready for review/editing before final export\n"
     ]
    }
   ],
   "source": [
    "# Summary of RDF generation (notebook already updated incrementally)\n",
    "print(f\"RDF notebook: {rdf_path}\")\n",
    "print(f\"  - Updated incrementally during processing\")\n",
    "print(f\"  - Ready for review/editing before final export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a59d9",
   "metadata": {},
   "source": [
    "## Export Combined RDF\n",
    "\n",
    "Combine all RDF cells into a single Turtle file with prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d061083",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'formatted_prefixes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 32\u001b[0m\n\u001b[1;32m     24\u001b[0m             all_triples\u001b[38;5;241m.\u001b[39mappend(content)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Build complete Turtle file\u001b[39;00m\n\u001b[1;32m     27\u001b[0m turtle_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m# RDF Knowledge Graph: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovenance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_title\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124m# Source: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovenance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_url\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m# License: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprovenance[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlicense\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124m# Generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39misoformat()\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;132;01m{\u001b[39;00m\u001b[43mformatted_prefixes\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[38;5;124m# === Triples ===\u001b[39m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     38\u001b[0m turtle_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(all_triples)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Save to file\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'formatted_prefixes' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine all RDF into a single Turtle file\n",
    "all_triples = []\n",
    "\n",
    "# Read the updated RDF notebook\n",
    "rdf_nb = nbformat.read(rdf_path, as_version=4)\n",
    "\n",
    "# Collect RDF from all raw cells (skip provenance, registry, and signature cells)\n",
    "for cell in rdf_nb.cells[2:]:\n",
    "    if cell.cell_type == 'raw':\n",
    "        content = cell.source.strip()\n",
    "        \n",
    "        # Skip signature cells (JSON objects)\n",
    "        if content.startswith('{') and '\"cid\"' in content:\n",
    "            continue\n",
    "        \n",
    "        # Skip empty or error-only cells\n",
    "        if not content or content.startswith('# Error:'):\n",
    "            continue\n",
    "        \n",
    "        # Skip comment-only cells\n",
    "        lines = [line for line in content.split('\\n') \n",
    "                 if line.strip() and not line.strip().startswith('#')]\n",
    "        if lines:\n",
    "            all_triples.append(content)\n",
    "\n",
    "# Format prefixes for the Turtle file\n",
    "turtle_prefixes = RDF_PREFIXES.format(source_url=provenance['source_url'])\n",
    "\n",
    "# Build complete Turtle file\n",
    "turtle_output = f\"\"\"# RDF Knowledge Graph: {provenance['article_title']}\n",
    "# Source: {provenance['source_url']}\n",
    "# License: {provenance['license']}\n",
    "# Generated: {datetime.now().isoformat()}\n",
    "\n",
    "{turtle_prefixes}\n",
    "\n",
    "# === Triples ===\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "turtle_output += \"\\n\\n\".join(all_triples)\n",
    "\n",
    "# Save to file\n",
    "turtle_path = os.path.join(OUTPUT_DIR, f\"{article_slug}.ttl\")\n",
    "with open(turtle_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(turtle_output)\n",
    "\n",
    "print(f\"Exported RDF to: {turtle_path}\")\n",
    "print(f\"  - {len(all_triples)} chunks of triples\")\n",
    "print(f\"  - {len(turtle_output)} characters total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c4e37",
   "metadata": {},
   "source": [
    "## Save Initial Entity Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d416a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/entity_registry.json\n",
      "\n",
      "Initial entities: 1\n",
      "  - Albert Einstein (Person): https://en.wikipedia.org/wiki/Albert_Einstein#person_albert_einstein\n"
     ]
    }
   ],
   "source": [
    "# Save entity registry to JSON file\n",
    "registry_path = os.path.join(OUTPUT_DIR, \"entity_registry.json\")\n",
    "with open(registry_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(registry.to_json())\n",
    "\n",
    "print(f\"Saved: {registry_path}\")\n",
    "print(f\"\\nInitial entities: {len(registry.entities)}\")\n",
    "for key, entity in registry.entities.items():\n",
    "    print(f\"  - {entity['label']} ({entity['type']}): {entity['uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2368a",
   "metadata": {},
   "source": [
    "## Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bf97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE COMPLETE\n",
      "============================================================\n",
      "\n",
      "Article: Albert Einstein\n",
      "Source: https://en.wikipedia.org/wiki/Albert_Einstein\n",
      "License: CC BY-SA 4.0\n",
      "\n",
      "Generated content notebooks (no Python code):\n",
      "  1. data/albert_einstein_chunks.ipynb\n",
      "     - 63 chunks with breadcrumb context\n",
      "     - Markdown cells with unchanged source text\n",
      "  2. data/albert_einstein_facts.ipynb\n",
      "     - Placeholder cells for factual statements\n",
      "     - Prompt template in header for LLM processing\n",
      "  3. data/albert_einstein_rdf.ipynb\n",
      "     - Placeholder cells for Turtle RDF\n",
      "     - Prefixes and prompt in header\n",
      "\n",
      "Entity registry: data/entity_registry.json\n",
      "\n",
      "Workflow:\n",
      "  1. Review/edit chunks notebook\n",
      "  2. LLM/agent fills facts notebook from chunks\n",
      "  3. Human reviews/edits facts\n",
      "  4. LLM/agent fills RDF notebook from facts\n",
      "  5. Human reviews/edits RDF\n",
      "  6. Export final .ttl file\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nArticle: {ARTICLE_TITLE}\")\n",
    "print(f\"Source: {source_url}\")\n",
    "print(f\"License: {provenance['license']}\")\n",
    "print(f\"\\nGenerated artifacts:\")\n",
    "print(f\"  1. {chunks_path}\")\n",
    "print(f\"     - {len(contextual_chunks)} chunks with breadcrumb context\")\n",
    "print(f\"  2. {facts_path}\")\n",
    "print(f\"     - {len(extracted_facts)} cells with extracted factual statements\")\n",
    "print(f\"  3. {rdf_path}\")\n",
    "print(f\"     - {len(generated_rdf)} cells with RDF triples\")\n",
    "print(f\"  4. {turtle_path}\")\n",
    "print(f\"     - Combined Turtle file for import\")\n",
    "print(f\"\\nEntity registry: {registry_path}\")\n",
    "print(f\"\\nThe intermediate notebooks can be reviewed and edited before re-export.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
