{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q langchain langchain-text-splitters langchain-openai langchain-community multiformats pydantic nbformat requests wikitextparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d51473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote\n",
    "\n",
    "import nbformat\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "# Import from our modules\n",
    "from src.cid import extract_signatures, extract_statement_signatures\n",
    "from src.entity_registry import EntityRegistry\n",
    "from src.section_parser import extract_section_hierarchy, get_section_context\n",
    "from src.prompts import FACTS_EXTRACTION_PROMPT, RDF_STATEMENT_SYSTEM_PROMPT, RDF_STATEMENT_HUMAN_PROMPT, RDF_PREFIXES\n",
    "from src.notebook_generators import generate_source_notebook, generate_chunks_notebook, generate_facts_notebook, generate_rdf_notebook_header\n",
    "from src.rdf_tools import create_rdf_tools\n",
    "from src.utils import log_progress, ContextualChunk, setup_output_directory, create_contextual_chunks\n",
    "from src.wikipedia_loader import fetch_wikipedia_with_links, links_in_chunk, format_entity_context\n",
    "from src.processors import (\n",
    "    process_facts_extraction, process_rdf_generation, \n",
    "    read_chunk_data, read_facts_data\n",
    ")\n",
    "from schema_matcher import SchemaMatcher\n",
    "\n",
    "print(\"All modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20b2db",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a52a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PIPELINE CONFIGURATION ===\n",
    "\n",
    "ARTICLE_TITLE = \"Albert Einstein\"\n",
    "OUTPUT_DIR = \"data\"\n",
    "CHUNK_SIZE = 2000\n",
    "CHUNK_OVERLAP = 128\n",
    "\n",
    "# Continue from a previous run (set to None for fresh run)\n",
    "CONTINUE_FROM_RUN = \"data/albert_einstein/20251219_091810\" # e.g., \"data/albert_einstein/20241218_143022\"\n",
    "\n",
    "# LLM configuration\n",
    "LLM_CONFIG = {\n",
    "    \"provider\": \"lm_studio\",\n",
    "    \"model\": \"qwen/qwen3-coder-30b\",\n",
    "    \"temperature\": 1,\n",
    "    \"base_url\": os.environ.get(\"LM_STUDIO_BASE_URL\", \"http://host.docker.internal:1234/v1\"),\n",
    "}\n",
    "\n",
    "# Processing limits\n",
    "CELL_TIMEOUT_SECONDS = 60\n",
    "MAX_ITERATIONS = 150\n",
    "\n",
    "# Schema vocabulary cache\n",
    "VOCAB_CACHE_DIR = \"data/vocab_cache\"\n",
    "\n",
    "print(f\"Configuration set for: {ARTICLE_TITLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf581b",
   "metadata": {},
   "source": [
    "## Setup Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b765eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up output directory (handles CONTINUE_FROM_RUN)\n",
    "RUN_OUTPUT_DIR, RUN_TIMESTAMP = setup_output_directory(\n",
    "    OUTPUT_DIR, ARTICLE_TITLE, CONTINUE_FROM_RUN\n",
    ")\n",
    "\n",
    "print(f\"Output directory: {RUN_OUTPUT_DIR}\")\n",
    "print(f\"Run timestamp: {RUN_TIMESTAMP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ad61f",
   "metadata": {},
   "source": [
    "## Fetch Wikipedia Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Wikipedia article with markdown links preserved\n",
    "wiki = fetch_wikipedia_with_links(ARTICLE_TITLE, max_chars=100000)\n",
    "\n",
    "raw_content = wiki.content\n",
    "source_url = wiki.url\n",
    "\n",
    "# Build provenance including Wikipedia metadata\n",
    "provenance = {\n",
    "    \"source_url\": source_url,\n",
    "    \"article_title\": ARTICLE_TITLE,\n",
    "    \"wikidata_id\": wiki.wikidata_id,\n",
    "    \"fetched_at\": datetime.now().isoformat(),\n",
    "    \"content_length\": len(raw_content),\n",
    "    \"license\": \"CC BY-SA 4.0\",\n",
    "    \"license_url\": \"https://creativecommons.org/licenses/by-sa/4.0/\",\n",
    "    \"attribution\": \"Wikipedia contributors\",\n",
    "    \"infobox\": wiki.infobox,\n",
    "    \"categories\": wiki.categories[:20],  # First 20 categories\n",
    "}\n",
    "\n",
    "print(f\"Fetched: {ARTICLE_TITLE}\")\n",
    "print(f\"Source URL: {source_url}\")\n",
    "print(f\"Wikidata ID: {wiki.wikidata_id}\")\n",
    "print(f\"Content length: {len(raw_content)} characters\")\n",
    "print(f\"Links extracted: {len(wiki.links)}\")\n",
    "print(f\"Categories: {len(wiki.categories)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7af7c9",
   "metadata": {},
   "source": [
    "## Create Contextual Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse section hierarchy\n",
    "sections = extract_section_hierarchy(raw_content)\n",
    "print(f\"Found {len(sections)} sections\")\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "raw_chunks = splitter.split_text(raw_content)\n",
    "print(f\"Split into {len(raw_chunks)} chunks\")\n",
    "\n",
    "# Add context to each chunk\n",
    "contextual_chunks = create_contextual_chunks(\n",
    "    raw_chunks, raw_content, sections, ARTICLE_TITLE, get_section_context\n",
    ")\n",
    "\n",
    "print(f\"\\nChunks with context:\")\n",
    "for chunk in contextual_chunks[:3]:\n",
    "    print(f\"  Chunk {chunk.chunk_index + 1}: {chunk.breadcrumb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33320dc1",
   "metadata": {},
   "source": [
    "## Initialize Entity Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4535350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize entity registry with article subject\n",
    "registry = EntityRegistry(source_url=source_url)\n",
    "\n",
    "# Register the article subject with Wikipedia URL and Wikidata ID\n",
    "subject_uri = source_url  # Use Wikipedia URL as the subject URI\n",
    "registry.register(\n",
    "    label=ARTICLE_TITLE,\n",
    "    entity_type=\"Person\",  # Adjust based on article type\n",
    "    description=f\"Subject of Wikipedia article: {ARTICLE_TITLE}\",\n",
    "    aliases=[ARTICLE_TITLE.split()[-1]],\n",
    "    uri=subject_uri,  # Use Wikipedia URL\n",
    "    wikidata_id=wiki.wikidata_id,\n",
    ")\n",
    "\n",
    "# Pre-register key entities from Wikipedia links\n",
    "for label, url in list(wiki.links.items())[:50]:  # First 50 linked entities\n",
    "    if label != ARTICLE_TITLE:  # Don't overwrite the subject\n",
    "        registry.register(\n",
    "            label=label,\n",
    "            entity_type=\"Thing\",  # Generic type, will be refined during RDF generation\n",
    "            description=f\"Wikipedia entity: {label}\",\n",
    "            uri=url,\n",
    "        )\n",
    "\n",
    "print(f\"Entity registry initialized with subject: {ARTICLE_TITLE}\")\n",
    "print(f\"Subject URI: {subject_uri}\")\n",
    "print(f\"Subject Wikidata ID: {wiki.wikidata_id}\")\n",
    "print(f\"Pre-registered entities: {len(registry.entities)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192af9c",
   "metadata": {},
   "source": [
    "## Generate Output Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe837dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate source notebook (raw Wikipedia content)\n",
    "source_path = os.path.join(RUN_OUTPUT_DIR, \"source.ipynb\")\n",
    "if not os.path.exists(source_path):\n",
    "    _, source_cid = generate_source_notebook(raw_content, provenance, registry, source_path)\n",
    "    print(f\"Generated: {source_path}\")\n",
    "else:\n",
    "    # Read existing source CID\n",
    "    from src.cid import compute_cid\n",
    "    source_cid = compute_cid(raw_content)\n",
    "    print(f\"Using existing: {source_path}\")\n",
    "\n",
    "# Generate chunks notebook\n",
    "chunks_path = os.path.join(RUN_OUTPUT_DIR, \"chunks.ipynb\")\n",
    "generate_chunks_notebook(contextual_chunks, provenance, registry, CHUNK_SIZE, CHUNK_OVERLAP, chunks_path, source_cid)\n",
    "print(f\"Generated: {chunks_path}\")\n",
    "\n",
    "# Generate facts notebook (only if doesn't exist)\n",
    "facts_path = os.path.join(RUN_OUTPUT_DIR, \"facts.ipynb\")\n",
    "if not os.path.exists(facts_path):\n",
    "    generate_facts_notebook(contextual_chunks, provenance, registry, LLM_CONFIG, \"chunks.ipynb\", facts_path)\n",
    "    print(f\"Generated: {facts_path}\")\n",
    "else:\n",
    "    print(f\"Using existing: {facts_path}\")\n",
    "\n",
    "# Generate RDF notebook (only if doesn't exist)\n",
    "rdf_path = os.path.join(RUN_OUTPUT_DIR, \"rdf.ipynb\")\n",
    "if not os.path.exists(rdf_path):\n",
    "    rdf_nb = generate_rdf_notebook_header(provenance, registry, LLM_CONFIG)\n",
    "    with open(rdf_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(rdf_nb, f)\n",
    "    print(f\"Generated: {rdf_path}\")\n",
    "else:\n",
    "    print(f\"Using existing: {rdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4e9ca8",
   "metadata": {},
   "source": [
    "## Process Chunks → Extract Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM client for facts extraction\n",
    "facts_llm = ChatOpenAI(\n",
    "    model=LLM_CONFIG[\"model\"],\n",
    "    temperature=LLM_CONFIG[\"temperature\"],\n",
    "    base_url=LLM_CONFIG[\"base_url\"],\n",
    "    api_key=\"lm-studio\",\n",
    "    timeout=CELL_TIMEOUT_SECONDS,\n",
    "    max_retries=0,\n",
    ")\n",
    "\n",
    "# Create facts extraction chain\n",
    "facts_prompt = ChatPromptTemplate.from_template(FACTS_EXTRACTION_PROMPT)\n",
    "facts_chain = facts_prompt | facts_llm\n",
    "\n",
    "# Read chunks and facts notebooks\n",
    "log_progress(\"Reading notebooks...\")\n",
    "chunks_nb = nbformat.read(chunks_path, as_version=4)\n",
    "chunk_data = read_chunk_data(chunks_nb)\n",
    "log_progress(f\"Found {len(chunk_data)} chunks\")\n",
    "\n",
    "facts_nb = nbformat.read(facts_path, as_version=4)\n",
    "facts_signatures = extract_signatures(facts_nb)\n",
    "log_progress(f\"Found {len(facts_signatures)} existing fact signatures\")\n",
    "log_progress(\"-\" * 50)\n",
    "\n",
    "# Process facts extraction\n",
    "processed, skipped, errors = process_facts_extraction(\n",
    "    chunk_data=chunk_data,\n",
    "    facts_nb=facts_nb,\n",
    "    facts_signatures=facts_signatures,\n",
    "    facts_chain=facts_chain,\n",
    "    provenance=provenance,\n",
    "    registry=registry,\n",
    "    facts_path=facts_path,\n",
    "    timeout_seconds=CELL_TIMEOUT_SECONDS,\n",
    ")\n",
    "\n",
    "log_progress(\"-\" * 50)\n",
    "log_progress(f\"Facts extraction complete:\")\n",
    "log_progress(f\"  - {processed} generated\")\n",
    "log_progress(f\"  - {skipped} skipped (up-to-date)\")\n",
    "log_progress(f\"  - {errors} errors/timeouts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d471ecb7",
   "metadata": {},
   "source": [
    "## Process Facts → Generate RDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load schema matcher\n",
    "schema_matcher = SchemaMatcher.load(VOCAB_CACHE_DIR, embed_base_url=LLM_CONFIG[\"base_url\"])\n",
    "log_progress(f\"Loaded schema matcher with {len(schema_matcher.vocabularies)} vocabularies\")\n",
    "\n",
    "# Create RDF tools bound to schema matcher\n",
    "rdf_tools, get_triples, reset_triples = create_rdf_tools(schema_matcher)\n",
    "\n",
    "# Create LLM client for RDF generation\n",
    "rdf_llm = ChatOpenAI(\n",
    "    model=LLM_CONFIG[\"model\"],\n",
    "    temperature=LLM_CONFIG[\"temperature\"],\n",
    "    base_url=LLM_CONFIG[\"base_url\"],\n",
    "    api_key=\"lm-studio\",\n",
    "    timeout=CELL_TIMEOUT_SECONDS,\n",
    "    max_retries=0,\n",
    ")\n",
    "rdf_llm_with_tools = rdf_llm.bind_tools(rdf_tools)\n",
    "\n",
    "# Create RDF prompt\n",
    "rdf_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", RDF_STATEMENT_SYSTEM_PROMPT),\n",
    "    (\"human\", RDF_STATEMENT_HUMAN_PROMPT),\n",
    "])\n",
    "\n",
    "# Read facts notebook\n",
    "log_progress(\"Reading facts notebook...\")\n",
    "facts_nb = nbformat.read(facts_path, as_version=4)\n",
    "facts_data = read_facts_data(facts_nb)\n",
    "log_progress(f\"Found {len(facts_data)} facts cells\")\n",
    "\n",
    "# Read RDF notebook\n",
    "rdf_nb = nbformat.read(rdf_path, as_version=4)\n",
    "rdf_signatures = extract_statement_signatures(rdf_nb)\n",
    "log_progress(f\"Found {len(rdf_signatures)} existing RDF signatures\")\n",
    "log_progress(f\"HTTP timeout: {CELL_TIMEOUT_SECONDS}s per request\")\n",
    "log_progress(f\"Max iterations: {MAX_ITERATIONS} per statement\")\n",
    "log_progress(\"-\" * 50)\n",
    "\n",
    "# Process RDF generation\n",
    "processed, skipped, errors, total_triples, iteration_counts = process_rdf_generation(\n",
    "    facts_data=facts_data,\n",
    "    rdf_nb=rdf_nb,\n",
    "    rdf_signatures=rdf_signatures,\n",
    "    provenance=provenance,\n",
    "    registry=registry,\n",
    "    rdf_path=rdf_path,\n",
    "    rdf_prompt=rdf_prompt,\n",
    "    rdf_llm_with_tools=rdf_llm_with_tools,\n",
    "    rdf_tools=rdf_tools,\n",
    "    get_triples_fn=get_triples,\n",
    "    reset_triples_fn=reset_triples,\n",
    "    max_iterations=MAX_ITERATIONS,\n",
    "    timeout_seconds=CELL_TIMEOUT_SECONDS,\n",
    ")\n",
    "\n",
    "log_progress(\"-\" * 50)\n",
    "log_progress(f\"RDF generation complete:\")\n",
    "log_progress(f\"  - {processed} statements processed\")\n",
    "log_progress(f\"  - {total_triples} total triples emitted\")\n",
    "log_progress(f\"  - {skipped} skipped (up-to-date)\")\n",
    "log_progress(f\"  - {errors} errors/timeouts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba26d14",
   "metadata": {},
   "source": [
    "## Export Combined RDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e80f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all RDF into a single Turtle file\n",
    "all_triples = []\n",
    "\n",
    "rdf_nb = nbformat.read(rdf_path, as_version=4)\n",
    "\n",
    "for cell in rdf_nb.cells[2:]:\n",
    "    if cell.cell_type == 'raw':\n",
    "        content = cell.source.strip()\n",
    "        \n",
    "        # Skip signature cells\n",
    "        if content.startswith('{') and '\"cid\"' in content:\n",
    "            continue\n",
    "        \n",
    "        # Skip empty or error-only cells\n",
    "        if not content or content.startswith('# Error:'):\n",
    "            continue\n",
    "        \n",
    "        # Skip comment-only cells\n",
    "        lines = [line for line in content.split('\\n') \n",
    "                 if line.strip() and not line.strip().startswith('#')]\n",
    "        if lines:\n",
    "            all_triples.append(content)\n",
    "\n",
    "# Build complete Turtle file\n",
    "turtle_prefixes = RDF_PREFIXES.format(source_url=provenance['source_url'])\n",
    "turtle_output = f\"\"\"# RDF Knowledge Graph: {provenance['article_title']}\n",
    "# Source: {provenance['source_url']}\n",
    "# License: {provenance['license']}\n",
    "# Generated: {datetime.now().isoformat()}\n",
    "\n",
    "{turtle_prefixes}\n",
    "\n",
    "# === Triples ===\n",
    "\n",
    "\"\"\"\n",
    "turtle_output += \"\\n\\n\".join(all_triples)\n",
    "\n",
    "turtle_path = os.path.join(RUN_OUTPUT_DIR, \"triples.ttl\")\n",
    "with open(turtle_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(turtle_output)\n",
    "\n",
    "print(f\"Exported RDF to: {turtle_path}\")\n",
    "print(f\"  - {len(all_triples)} statement blocks\")\n",
    "print(f\"  - {len(turtle_output)} characters total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d7583",
   "metadata": {},
   "source": [
    "## Save Entity Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry_path = os.path.join(RUN_OUTPUT_DIR, \"registry.json\")\n",
    "with open(registry_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(registry.to_json())\n",
    "\n",
    "print(f\"Saved: {registry_path}\")\n",
    "print(f\"\\nEntities: {len(registry.entities)}\")\n",
    "for key, entity in registry.entities.items():\n",
    "    print(f\"  - {entity['label']} ({entity['type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538ca6a0",
   "metadata": {},
   "source": [
    "## Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nArticle: {ARTICLE_TITLE}\")\n",
    "print(f\"Source: {source_url}\")\n",
    "print(f\"License: {provenance['license']}\")\n",
    "print(f\"Run timestamp: {RUN_TIMESTAMP}\")\n",
    "print(f\"\\nOutput directory: {RUN_OUTPUT_DIR}\")\n",
    "print(f\"\\nGenerated artifacts:\")\n",
    "print(f\"  1. chunks.ipynb - {len(contextual_chunks)} chunks with breadcrumb context\")\n",
    "print(f\"  2. facts.ipynb - Extracted factual statements\")\n",
    "print(f\"  3. rdf.ipynb - RDF triples per statement\")\n",
    "print(f\"  4. triples.ttl - Combined Turtle file\")\n",
    "print(f\"  5. registry.json - Entity registry\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
