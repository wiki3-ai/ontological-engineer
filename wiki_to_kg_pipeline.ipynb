{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236e29d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# mamba install -c conda-forge langchain langchain-text-splitters langchain-openai langchain-community wikipedia pydantic nbformat nbconvert\n",
    "%pip install -q langchain langchain-text-splitters langchain-openai langchain-community wikipedia pydantic nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "833f695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional\n",
    "from urllib.parse import quote\n",
    "\n",
    "import nbformat\n",
    "from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell, new_raw_cell\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "print(\"Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252ef95",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f79c403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline configured for: Albert Einstein\n"
     ]
    }
   ],
   "source": [
    "# Pipeline configuration\n",
    "ARTICLE_TITLE = \"Albert Einstein\"\n",
    "OUTPUT_DIR = \"data\"\n",
    "CHUNK_SIZE = 2000\n",
    "CHUNK_OVERLAP = 128\n",
    "\n",
    "# LLM configuration (shared across stages)\n",
    "LLM_CONFIG = {\n",
    "    \"provider\": \"lm_studio\",  # or \"openai\"\n",
    "    \"model\": \"qwen/qwen3-coder-30b\",\n",
    "    \"temperature\": 1,\n",
    "    \"base_url\": os.environ.get(\"LM_STUDIO_BASE_URL\", \"http://host.docker.internal:1234/v1\"),\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Pipeline configured for: {ARTICLE_TITLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df90b5b",
   "metadata": {},
   "source": [
    "## Entity Registry\n",
    "\n",
    "Manages entity identity across chunks with stable URIs derived from source URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a66f9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntityRegistry class defined\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class EntityRegistry:\n",
    "    \"\"\"Tracks entities with stable IDs derived from source URL.\"\"\"\n",
    "    source_url: str\n",
    "    entities: dict = field(default_factory=dict)  # normalized_key -> entity\n",
    "    aliases: dict = field(default_factory=dict)   # alias -> canonical_key\n",
    "    \n",
    "    def normalize_key(self, label: str) -> str:\n",
    "        \"\"\"Create consistent key from entity label.\"\"\"\n",
    "        return re.sub(r'[^a-z0-9]+', '_', label.lower().strip()).strip('_')\n",
    "    \n",
    "    def generate_uri(self, entity_type: str, label: str) -> str:\n",
    "        \"\"\"Generate URI based on source URL with fragment identifier.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        # Use source URL as base, add fragment for entity\n",
    "        return f\"{self.source_url}#{entity_type.lower()}_{key}\"\n",
    "    \n",
    "    def generate_id(self, entity_type: str, label: str) -> str:\n",
    "        \"\"\"Generate local ID for internal reference.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        return f\"{entity_type.lower()}_{key}\"\n",
    "    \n",
    "    def register(self, label: str, entity_type: str, description: str = \"\",\n",
    "                 aliases: list = None, source_chunk: int = None) -> str:\n",
    "        \"\"\"Register or update an entity, return canonical ID.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        entity_id = self.generate_id(entity_type, label)\n",
    "        entity_uri = self.generate_uri(entity_type, label)\n",
    "        \n",
    "        if key not in self.entities:\n",
    "            self.entities[key] = {\n",
    "                \"id\": entity_id,\n",
    "                \"uri\": entity_uri,\n",
    "                \"label\": label,\n",
    "                \"type\": entity_type,\n",
    "                \"descriptions\": [description] if description else [],\n",
    "                \"source_chunks\": [source_chunk] if source_chunk is not None else [],\n",
    "                \"aliases\": list(aliases or []),\n",
    "            }\n",
    "        else:\n",
    "            existing = self.entities[key]\n",
    "            if description and description not in existing[\"descriptions\"]:\n",
    "                existing[\"descriptions\"].append(description)\n",
    "            if source_chunk is not None and source_chunk not in existing[\"source_chunks\"]:\n",
    "                existing[\"source_chunks\"].append(source_chunk)\n",
    "            if aliases:\n",
    "                existing[\"aliases\"] = list(set(existing[\"aliases\"]) | set(aliases))\n",
    "        \n",
    "        # Register aliases\n",
    "        for alias in (aliases or []):\n",
    "            self.aliases[self.normalize_key(alias)] = key\n",
    "        \n",
    "        return entity_id\n",
    "    \n",
    "    def lookup(self, label: str) -> Optional[dict]:\n",
    "        \"\"\"Find entity by label or alias.\"\"\"\n",
    "        key = self.normalize_key(label)\n",
    "        canonical_key = self.aliases.get(key, key)\n",
    "        return self.entities.get(canonical_key)\n",
    "    \n",
    "    def to_json(self) -> str:\n",
    "        \"\"\"Serialize registry to JSON.\"\"\"\n",
    "        return json.dumps({\n",
    "            \"source_url\": self.source_url,\n",
    "            \"entities\": self.entities,\n",
    "            \"aliases\": self.aliases,\n",
    "        }, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, json_str: str) -> 'EntityRegistry':\n",
    "        \"\"\"Deserialize registry from JSON.\"\"\"\n",
    "        data = json.loads(json_str)\n",
    "        registry = cls(source_url=data[\"source_url\"])\n",
    "        registry.entities = data[\"entities\"]\n",
    "        registry.aliases = data[\"aliases\"]\n",
    "        return registry\n",
    "\n",
    "print(\"EntityRegistry class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1094a385",
   "metadata": {},
   "source": [
    "## Section Hierarchy Parser\n",
    "\n",
    "Extracts Wikipedia section structure for breadcrumb context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fe68df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section hierarchy parser defined\n"
     ]
    }
   ],
   "source": [
    "def extract_section_hierarchy(content: str) -> list[dict]:\n",
    "    \"\"\"Parse Wikipedia == headers == into hierarchical structure with positions.\"\"\"\n",
    "    header_pattern = re.compile(r'^(={2,6})\\s*(.+?)\\s*\\1\\s*$', re.MULTILINE)\n",
    "    \n",
    "    sections = []\n",
    "    current_path = []  # Stack of (level, title)\n",
    "    \n",
    "    for match in header_pattern.finditer(content):\n",
    "        level = len(match.group(1))  # Number of '=' signs\n",
    "        title = match.group(2).strip()\n",
    "        \n",
    "        # Pop stack until we're at parent level\n",
    "        while current_path and current_path[-1][0] >= level:\n",
    "            current_path.pop()\n",
    "        \n",
    "        current_path.append((level, title))\n",
    "        breadcrumb = \" > \".join(t for _, t in current_path)\n",
    "        \n",
    "        sections.append({\n",
    "            \"level\": level,\n",
    "            \"title\": title,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"start_pos\": match.start(),\n",
    "            \"end_pos\": match.end(),\n",
    "        })\n",
    "    \n",
    "    return sections\n",
    "\n",
    "\n",
    "def get_section_context(position: int, sections: list[dict], article_title: str) -> dict:\n",
    "    \"\"\"Find the section context for a given character position.\"\"\"\n",
    "    active_section = {\n",
    "        \"title\": \"Introduction\",\n",
    "        \"breadcrumb\": \"Introduction\",\n",
    "        \"level\": 1,\n",
    "    }\n",
    "    \n",
    "    for section in sections:\n",
    "        if section[\"start_pos\"] <= position:\n",
    "            active_section = section\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        \"section_title\": active_section[\"title\"],\n",
    "        \"breadcrumb\": f\"{article_title} > {active_section['breadcrumb']}\",\n",
    "    }\n",
    "\n",
    "print(\"Section hierarchy parser defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360c37f",
   "metadata": {},
   "source": [
    "## Fetch Wikipedia Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b91833b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched: Albert Einstein\n",
      "Source URL: https://en.wikipedia.org/wiki/Albert_Einstein\n",
      "Content length: 87959 characters\n",
      "License: CC BY-SA 4.0\n"
     ]
    }
   ],
   "source": [
    "# Fetch Wikipedia article\n",
    "loader = WikipediaLoader(query=ARTICLE_TITLE, load_max_docs=1, doc_content_chars_max=100000)\n",
    "docs = loader.load()\n",
    "\n",
    "if not docs:\n",
    "    raise ValueError(f\"Could not fetch article: {ARTICLE_TITLE}\")\n",
    "\n",
    "raw_content = docs[0].page_content\n",
    "metadata = docs[0].metadata\n",
    "\n",
    "# Construct source URL and provenance\n",
    "source_url = f\"https://en.wikipedia.org/wiki/{quote(ARTICLE_TITLE.replace(' ', '_'))}\"\n",
    "\n",
    "provenance = {\n",
    "    \"source_url\": source_url,\n",
    "    \"article_title\": ARTICLE_TITLE,\n",
    "    \"fetched_at\": datetime.now().isoformat(),\n",
    "    \"content_length\": len(raw_content),\n",
    "    # Wikipedia license - standard for all Wikipedia content\n",
    "    \"license\": \"CC BY-SA 4.0\",\n",
    "    \"license_url\": \"https://creativecommons.org/licenses/by-sa/4.0/\",\n",
    "    \"attribution\": \"Wikipedia contributors\",\n",
    "}\n",
    "\n",
    "print(f\"Fetched: {ARTICLE_TITLE}\")\n",
    "print(f\"Source URL: {source_url}\")\n",
    "print(f\"Content length: {len(raw_content)} characters\")\n",
    "print(f\"License: {provenance['license']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1276b5",
   "metadata": {},
   "source": [
    "## Create Contextual Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3146caad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 sections\n",
      "Split into 63 chunks\n",
      "\n",
      "Chunks with context:\n",
      "  Chunk 1: Albert Einstein > Introduction\n",
      "  Chunk 2: Albert Einstein > Introduction\n",
      "  Chunk 3: Albert Einstein > Life and career\n"
     ]
    }
   ],
   "source": [
    "# Parse section hierarchy\n",
    "sections = extract_section_hierarchy(raw_content)\n",
    "print(f\"Found {len(sections)} sections\")\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "raw_chunks = splitter.split_text(raw_content)\n",
    "print(f\"Split into {len(raw_chunks)} chunks\")\n",
    "\n",
    "# Add context to each chunk\n",
    "@dataclass\n",
    "class ContextualChunk:\n",
    "    content: str\n",
    "    chunk_index: int\n",
    "    total_chunks: int\n",
    "    breadcrumb: str\n",
    "    section_title: str\n",
    "    char_start: int\n",
    "    char_end: int\n",
    "\n",
    "contextual_chunks = []\n",
    "current_pos = 0\n",
    "\n",
    "for i, chunk_text in enumerate(raw_chunks):\n",
    "    # Find position in original content\n",
    "    chunk_start = raw_content.find(chunk_text, current_pos)\n",
    "    if chunk_start == -1:\n",
    "        chunk_start = current_pos  # Fallback\n",
    "    chunk_end = chunk_start + len(chunk_text)\n",
    "    \n",
    "    # Get section context\n",
    "    context = get_section_context(chunk_start, sections, ARTICLE_TITLE)\n",
    "    \n",
    "    contextual_chunks.append(ContextualChunk(\n",
    "        content=chunk_text,\n",
    "        chunk_index=i,\n",
    "        total_chunks=len(raw_chunks),\n",
    "        breadcrumb=context[\"breadcrumb\"],\n",
    "        section_title=context[\"section_title\"],\n",
    "        char_start=chunk_start,\n",
    "        char_end=chunk_end,\n",
    "    ))\n",
    "    \n",
    "    current_pos = chunk_start + 1\n",
    "\n",
    "print(f\"\\nChunks with context:\")\n",
    "for chunk in contextual_chunks[:3]:\n",
    "    print(f\"  Chunk {chunk.chunk_index + 1}: {chunk.breadcrumb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3af602",
   "metadata": {},
   "source": [
    "## Initialize Entity Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34b0e8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity registry initialized with subject: Albert Einstein\n",
      "Subject URI: https://en.wikipedia.org/wiki/Albert_Einstein#person_albert_einstein\n"
     ]
    }
   ],
   "source": [
    "# Initialize entity registry with article subject\n",
    "registry = EntityRegistry(source_url=source_url)\n",
    "\n",
    "# Pre-seed with the article subject\n",
    "registry.register(\n",
    "    label=ARTICLE_TITLE,\n",
    "    entity_type=\"Person\",  # Adjust based on article type\n",
    "    description=f\"Subject of Wikipedia article: {ARTICLE_TITLE}\",\n",
    "    aliases=[ARTICLE_TITLE.split()[-1]],  # Last name as alias\n",
    ")\n",
    "\n",
    "print(f\"Entity registry initialized with subject: {ARTICLE_TITLE}\")\n",
    "print(f\"Subject URI: {registry.entities[registry.normalize_key(ARTICLE_TITLE)]['uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd3cdd",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "These prompts are embedded in the generated notebooks for transparency and adjustability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3089027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt templates defined\n"
     ]
    }
   ],
   "source": [
    "FACTS_EXTRACTION_PROMPT = \"\"\"You are an expert at extracting factual information from text.\n",
    "\n",
    "Given text from a Wikipedia article, extract simple English statements that capture the consensus factual information. Each statement should:\n",
    "- Be a single, clear sentence\n",
    "- Contain one main fact or relationship\n",
    "- Use the full name of entities on first mention\n",
    "- Be verifiable from the source text\n",
    "- Avoid opinions, interpretations, or hedged language\n",
    "\n",
    "The text comes from: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Known entities (use consistent names):\n",
    "{known_entities}\n",
    "\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\n",
    "Extract factual statements as a bulleted list. Also identify any new entities (people, places, organizations, concepts, events, works) that should be added to the registry.\n",
    "\"\"\"\n",
    "\n",
    "RDF_GENERATION_PROMPT = \"\"\"You are an expert at converting factual statements to RDF triples in Turtle format.\n",
    "\n",
    "Convert the following factual statements to RDF using schema.org vocabulary where possible.\n",
    "\n",
    "Source: {source_url}\n",
    "Section: {breadcrumb}\n",
    "\n",
    "Use these prefixes:\n",
    "{prefixes}\n",
    "\n",
    "Entity registry (use these URIs):\n",
    "{entity_registry}\n",
    "\n",
    "Guidelines:\n",
    "- Use schema.org properties (schema:birthDate, schema:birthPlace, schema:worksFor, etc.)\n",
    "- For relationships not in schema.org, use wiki3: prefix\n",
    "- Include rdfs:label for entities\n",
    "- Use xsd datatypes for dates and numbers\n",
    "- Entity URIs should use the source URL as base with fragment identifiers\n",
    "\n",
    "---\n",
    "{facts}\n",
    "---\n",
    "\n",
    "Generate Turtle RDF:\n",
    "\"\"\"\n",
    "\n",
    "RDF_PREFIXES = \"\"\"@prefix schema: <https://schema.org/> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix wiki3: <https://wiki3.ai/vocab/> .\n",
    "@base <{source_url}> .\n",
    "\"\"\"\n",
    "\n",
    "print(\"Prompt templates defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa35e61",
   "metadata": {},
   "source": [
    "## Generate Chunks Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d121080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: data/albert_einstein_chunks.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_chunks_notebook(chunks: list, provenance: dict, registry: EntityRegistry, \n",
    "                             llm_config: dict, output_path: str):\n",
    "    \"\"\"Generate a notebook with chunked source text and context metadata.\"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    # Cell 1: Provenance markdown\n",
    "    provenance_yaml = f\"\"\"# Chunked Text: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "fetched_at: {provenance['fetched_at']}\n",
    "content_length: {provenance['content_length']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "attribution: {provenance['attribution']}\n",
    "chunk_size: {CHUNK_SIZE}\n",
    "chunk_overlap: {CHUNK_OVERLAP}\n",
    "total_chunks: {len(chunks)}\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "Each chunk below contains source text with contextual metadata. The context line (before the separator) provides:\n",
    "- **Context**: Hierarchical breadcrumb showing article > section path\n",
    "- **Chunk**: Position in sequence\n",
    "\n",
    "The text below the `---` separator is the unchanged source content.\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_yaml))\n",
    "    \n",
    "    # Cell 2: Entity registry (raw cell)\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    # Chunk cells\n",
    "    for chunk in chunks:\n",
    "        chunk_cell = f\"\"\"**Context:** {chunk.breadcrumb}\n",
    "**Chunk:** {chunk.chunk_index + 1} of {chunk.total_chunks}\n",
    "\n",
    "---\n",
    "\n",
    "{chunk.content}\n",
    "\"\"\"\n",
    "        nb.cells.append(new_markdown_cell(chunk_cell))\n",
    "    \n",
    "    # Write notebook\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(nb, f)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate chunks notebook\n",
    "article_slug = ARTICLE_TITLE.lower().replace(' ', '_')\n",
    "chunks_path = os.path.join(OUTPUT_DIR, f\"{article_slug}_chunks.ipynb\")\n",
    "generate_chunks_notebook(contextual_chunks, provenance, registry, LLM_CONFIG, chunks_path)\n",
    "print(f\"Generated: {chunks_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c23dc4",
   "metadata": {},
   "source": [
    "## Generate Facts Notebook (Structure Only)\n",
    "\n",
    "Creates the facts notebook with placeholders. Actual content is generated in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cd4acab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: data/albert_einstein_facts.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_facts_notebook(chunks: list, provenance: dict, registry: EntityRegistry,\n",
    "                            llm_config: dict, prompt_template: str, output_path: str):\n",
    "    \"\"\"Generate a notebook for factual statements - content only, no Python code.\n",
    "    \n",
    "    This notebook contains:\n",
    "    - Provenance header with processing prompt\n",
    "    - Entity registry (raw cell)\n",
    "    - One markdown cell per chunk for factual statements (to be filled by LLM)\n",
    "    \"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    # Cell 1: Provenance and prompt\n",
    "    provenance_md = f\"\"\"# Factual Statements: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "source_notebook: {article_slug}_chunks.ipynb\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "llm_provider: {llm_config['provider']}\n",
    "llm_model: {llm_config['model']}\n",
    "llm_temperature: {llm_config['temperature']}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "This notebook contains simple English factual statements extracted from source text chunks.\n",
    "Each cell corresponds to one chunk from the source notebook.\n",
    "\n",
    "An LLM/agent processes this notebook by:\n",
    "1. Reading each content cell below\n",
    "2. The context line indicates the source section\n",
    "3. Statements can be edited, corrected, or expanded by humans\n",
    "\n",
    "## Prompt Template\n",
    "\n",
    "The following prompt was used to extract facts from each chunk:\n",
    "\n",
    "```\n",
    "{prompt_template}\n",
    "```\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_md))\n",
    "    \n",
    "    # Cell 2: Entity registry (raw cell)\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    # Content cells - one per chunk with placeholder for facts\n",
    "    for chunk in chunks:\n",
    "        # Markdown cell with context header and placeholder for factual statements\n",
    "        facts_cell = f\"\"\"**Context:** {chunk.breadcrumb}\n",
    "**Chunk:** {chunk.chunk_index + 1} of {chunk.total_chunks}\n",
    "\n",
    "---\n",
    "\n",
    "[Factual statements to be extracted from source chunk]\n",
    "\"\"\"\n",
    "        nb.cells.append(new_markdown_cell(facts_cell))\n",
    "    \n",
    "    # Write notebook\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(nb, f)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate facts notebook\n",
    "facts_path = os.path.join(OUTPUT_DIR, f\"{article_slug}_facts.ipynb\")\n",
    "generate_facts_notebook(contextual_chunks, provenance, registry, LLM_CONFIG, \n",
    "                        FACTS_EXTRACTION_PROMPT, facts_path)\n",
    "print(f\"Generated: {facts_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256713fd",
   "metadata": {},
   "source": [
    "## Generate RDF Notebook (Structure Only)\n",
    "\n",
    "Creates the RDF notebook with placeholders. Actual content is generated after facts extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a7ae750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: data/albert_einstein_rdf.ipynb\n"
     ]
    }
   ],
   "source": [
    "def generate_rdf_notebook(chunks: list, provenance: dict, registry: EntityRegistry,\n",
    "                          llm_config: dict, prompt_template: str, prefixes: str, output_path: str):\n",
    "    \"\"\"Generate a notebook for RDF triples - content only, no Python code.\n",
    "    \n",
    "    This notebook contains:\n",
    "    - Provenance header with RDF generation prompt and prefixes\n",
    "    - Entity registry (raw cell)\n",
    "    - One raw cell per chunk for Turtle RDF (to be filled by LLM)\n",
    "    \"\"\"\n",
    "    nb = new_notebook()\n",
    "    \n",
    "    # Cell 1: Provenance and prompt\n",
    "    formatted_prefixes = prefixes.format(source_url=provenance['source_url'])\n",
    "    provenance_md = f\"\"\"# RDF Triples: {provenance['article_title']}\n",
    "\n",
    "## Provenance\n",
    "\n",
    "```yaml\n",
    "source_url: {provenance['source_url']}\n",
    "article_title: {provenance['article_title']}\n",
    "license: {provenance['license']}\n",
    "license_url: {provenance['license_url']}\n",
    "source_notebook: {article_slug}_facts.ipynb\n",
    "generated_by: wiki_to_kg_pipeline.ipynb\n",
    "generated_at: {datetime.now().isoformat()}\n",
    "llm_provider: {llm_config['provider']}\n",
    "llm_model: {llm_config['model']}\n",
    "llm_temperature: {llm_config['temperature']}\n",
    "rdf_format: Turtle\n",
    "```\n",
    "\n",
    "## RDF Prefixes\n",
    "\n",
    "The following prefixes are used throughout this notebook:\n",
    "\n",
    "```turtle\n",
    "{formatted_prefixes}\n",
    "```\n",
    "\n",
    "## Processing Instructions\n",
    "\n",
    "This notebook contains RDF triples in Turtle format, one cell per source facts cell.\n",
    "Each cell corresponds to factual statements from the facts notebook.\n",
    "\n",
    "An LLM/agent processes this notebook by:\n",
    "1. Reading the corresponding facts cell\n",
    "2. Converting statements to RDF using schema.org vocabulary\n",
    "3. Entity URIs use source URL with fragment identifiers (e.g., `<#person_albert_einstein>`)\n",
    "\n",
    "Triples can be edited, corrected, or expanded by humans before final export.\n",
    "\n",
    "## Prompt Template\n",
    "\n",
    "The following prompt was used to convert facts to RDF:\n",
    "\n",
    "```\n",
    "{prompt_template}\n",
    "```\n",
    "\"\"\"\n",
    "    nb.cells.append(new_markdown_cell(provenance_md))\n",
    "    \n",
    "    # Cell 2: Entity registry (raw cell)\n",
    "    nb.cells.append(new_raw_cell(registry.to_json()))\n",
    "    \n",
    "    # Content cells - one raw cell per chunk for Turtle RDF\n",
    "    for chunk in chunks:\n",
    "        # Raw cell with context comment and placeholder for RDF\n",
    "        rdf_cell = f\"\"\"# Chunk {chunk.chunk_index + 1}: {chunk.section_title}\n",
    "# Context: {chunk.breadcrumb}\n",
    "\n",
    "# [RDF triples to be generated from corresponding facts cell]\n",
    "\"\"\"\n",
    "        nb.cells.append(new_raw_cell(rdf_cell))\n",
    "    \n",
    "    # Write notebook\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(nb, f)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate RDF notebook\n",
    "rdf_path = os.path.join(OUTPUT_DIR, f\"{article_slug}_rdf.ipynb\")\n",
    "generate_rdf_notebook(contextual_chunks, provenance, registry, LLM_CONFIG,\n",
    "                      RDF_GENERATION_PROMPT, RDF_PREFIXES, rdf_path)\n",
    "print(f\"Generated: {rdf_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696afdf",
   "metadata": {},
   "source": [
    "## Process Chunks → Extract Facts\n",
    "\n",
    "Run the LLM on each chunk to extract factual statements and update the facts notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2350ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 63 chunks to extract facts...\n",
      "Timeout per cell: 300 seconds (5 minutes)\n",
      "  Chunk 1/63... ✓ (2040 chars, 6.6s)\n",
      "  Chunk 2/63... ✓ (1788 chars, 4.7s)\n",
      "  Chunk 3/63... ✓ (1203 chars, 3.8s)\n",
      "  Chunk 4/63... ✓ (1144 chars, 3.5s)\n",
      "  Chunk 5/63... ✓ (1009 chars, 2.7s)\n",
      "  Chunk 6/63... ✓ (1053 chars, 3.1s)\n",
      "  Chunk 7/63... ✓ (1503 chars, 4.3s)\n",
      "  Chunk 8/63... ✓ (1662 chars, 6.0s)\n",
      "  Chunk 9/63... ✓ (1901 chars, 6.0s)\n",
      "  Chunk 10/63... ✓ (1720 chars, 4.6s)\n",
      "  Chunk 11/63... ✓ (1473 chars, 4.7s)\n",
      "  Chunk 12/63... ✓ (1543 chars, 4.6s)\n",
      "  Chunk 13/63... ✓ (1622 chars, 5.2s)\n",
      "  Chunk 14/63... ✓ (399 chars, 1.2s)\n",
      "  Chunk 15/63... ✓ (1313 chars, 4.1s)\n",
      "  Chunk 16/63... ✓ (1233 chars, 3.5s)\n",
      "  Chunk 17/63... ✓ (1149 chars, 3.3s)\n",
      "  Chunk 18/63... ✓ (854 chars, 3.0s)\n",
      "  Chunk 19/63... ✓ (1444 chars, 4.3s)\n",
      "  Chunk 20/63... ✓ (1122 chars, 3.5s)\n",
      "  Chunk 21/63... ✓ (1873 chars, 5.3s)\n",
      "  Chunk 22/63... ✓ (1293 chars, 4.4s)\n",
      "  Chunk 23/63... ✓ (1193 chars, 3.9s)\n",
      "  Chunk 24/63... ✓ (1021 chars, 3.5s)\n",
      "  Chunk 25/63... ✓ (1342 chars, 3.5s)\n",
      "  Chunk 26/63... ✓ (1471 chars, 4.4s)\n",
      "  Chunk 27/63... ✓ (1504 chars, 4.5s)\n",
      "  Chunk 28/63... ✓ (1191 chars, 3.4s)\n",
      "  Chunk 29/63... ✓ (531 chars, 1.8s)\n",
      "  Chunk 30/63... ✓ (647 chars, 2.2s)\n",
      "  Chunk 31/63... ✓ (1092 chars, 3.0s)\n",
      "  Chunk 32/63... ✓ (628 chars, 2.1s)\n",
      "  Chunk 33/63... ✓ (909 chars, 2.7s)\n",
      "  Chunk 34/63... ✓ (1415 chars, 4.3s)\n",
      "  Chunk 35/63... ✓ (1086 chars, 3.2s)\n",
      "  Chunk 36/63... ✓ (1023 chars, 2.7s)\n",
      "  Chunk 37/63... ✓ (1443 chars, 4.2s)\n",
      "  Chunk 38/63... ✓ (1303 chars, 3.9s)\n",
      "  Chunk 39/63... ✓ (1106 chars, 3.3s)\n",
      "  Chunk 40/63... ✓ (973 chars, 3.1s)\n",
      "  Chunk 41/63... ✓ (1353 chars, 3.9s)\n",
      "  Chunk 42/63... ✓ (1406 chars, 4.5s)\n",
      "  Chunk 43/63... ✓ (1181 chars, 3.6s)\n",
      "  Chunk 44/63... ✓ (1565 chars, 4.3s)\n",
      "  Chunk 45/63... ✓ (1491 chars, 4.2s)\n",
      "  Chunk 46/63... ✓ (961 chars, 2.7s)\n",
      "  Chunk 47/63... ✓ (1028 chars, 2.9s)\n",
      "  Chunk 48/63... ✓ (461 chars, 1.5s)\n",
      "  Chunk 49/63... ✓ (1364 chars, 3.8s)\n",
      "  Chunk 50/63... ✓ (1035 chars, 3.0s)\n",
      "  Chunk 51/63... ✓ (1843 chars, 5.5s)\n",
      "  Chunk 52/63... ✓ (1411 chars, 4.0s)\n",
      "  Chunk 53/63... ✓ (1021 chars, 2.9s)\n",
      "  Chunk 54/63... ✓ (1424 chars, 3.5s)\n",
      "  Chunk 55/63... ✓ (1236 chars, 3.3s)\n",
      "  Chunk 56/63... ✓ (1173 chars, 3.0s)\n",
      "  Chunk 57/63... ✓ (1147 chars, 3.0s)\n",
      "  Chunk 58/63... ✓ (1578 chars, 4.9s)\n",
      "  Chunk 59/63... ✓ (1668 chars, 5.0s)\n",
      "  Chunk 60/63... ✓ (1007 chars, 3.5s)\n",
      "  Chunk 61/63... ✓ (1353 chars, 3.8s)\n",
      "  Chunk 62/63... ✓ (1145 chars, 3.8s)\n",
      "  Chunk 63/63... ✓ (1204 chars, 3.3s)\n",
      "\n",
      "Facts extraction complete:\n",
      "  - 63 total cells\n",
      "  - 0 skipped (already populated)\n",
      "  - 0 errors/timeouts\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import signal\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Timeout handling\n",
    "CELL_TIMEOUT_SECONDS = 300  # 5 minutes\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "@contextmanager\n",
    "def timeout_context(seconds):\n",
    "    \"\"\"Context manager for timing out long-running operations.\"\"\"\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise TimeoutException(f\"Operation timed out after {seconds} seconds\")\n",
    "    \n",
    "    # Set the signal handler\n",
    "    old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(seconds)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        signal.alarm(0)\n",
    "        signal.signal(signal.SIGALRM, old_handler)\n",
    "\n",
    "def is_cell_populated(cell_content: str) -> bool:\n",
    "    \"\"\"Check if a cell has real content (not just placeholders).\"\"\"\n",
    "    # Check for placeholder markers\n",
    "    placeholders = [\n",
    "        \"[Factual statements to be extracted\",\n",
    "        \"[Error extracting facts:\",\n",
    "        \"[RDF triples to be generated\",\n",
    "        \"# [RDF triples to be generated\",\n",
    "    ]\n",
    "    content_after_separator = cell_content.split(\"---\\n\", 1)[-1].strip()\n",
    "    \n",
    "    # Empty or only whitespace\n",
    "    if not content_after_separator:\n",
    "        return False\n",
    "    \n",
    "    # Contains placeholder text\n",
    "    for placeholder in placeholders:\n",
    "        if placeholder in content_after_separator:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Initialize LLM for processing\n",
    "llm = ChatOpenAI(\n",
    "    model=LLM_CONFIG[\"model\"],\n",
    "    temperature=LLM_CONFIG[\"temperature\"],\n",
    "    base_url=LLM_CONFIG[\"base_url\"],\n",
    "    api_key=\"lm-studio\",\n",
    ")\n",
    "\n",
    "# Create facts extraction prompt\n",
    "facts_prompt = ChatPromptTemplate.from_template(\"\"\"You are an expert at extracting factual information from text.\n",
    "\n",
    "Given text from a Wikipedia article, extract simple English statements that capture the consensus factual information. Each statement should:\n",
    "- Be a single, clear sentence\n",
    "- Contain one main fact or relationship\n",
    "- Use the full name of entities on first mention\n",
    "- Be verifiable from the source text\n",
    "- Avoid opinions, interpretations, or hedged language\n",
    "\n",
    "Source: {source_url}\n",
    "Section context: {breadcrumb}\n",
    "\n",
    "Known entities (use consistent names):\n",
    "{known_entities}\n",
    "\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\n",
    "Extract factual statements as a bulleted list:\"\"\")\n",
    "\n",
    "facts_chain = facts_prompt | llm\n",
    "\n",
    "def get_known_entities_text(registry: EntityRegistry) -> str:\n",
    "    \"\"\"Format known entities for prompt context.\"\"\"\n",
    "    lines = []\n",
    "    for entity in registry.entities.values():\n",
    "        lines.append(f\"- {entity['label']} ({entity['type']})\")\n",
    "    return \"\\n\".join(lines) if lines else \"None yet\"\n",
    "\n",
    "# Read chunks notebook\n",
    "chunks_nb = nbformat.read(chunks_path, as_version=4)\n",
    "\n",
    "# Extract chunk cells (skip provenance header and entity registry)\n",
    "chunk_cells = [cell for cell in chunks_nb.cells[2:] if cell.cell_type == 'markdown']\n",
    "\n",
    "# Read existing facts notebook to check for already-populated cells\n",
    "facts_nb = nbformat.read(facts_path, as_version=4)\n",
    "\n",
    "print(f\"Processing {len(chunk_cells)} chunks to extract facts...\")\n",
    "print(f\"Timeout per cell: {CELL_TIMEOUT_SECONDS} seconds ({CELL_TIMEOUT_SECONDS // 60} minutes)\")\n",
    "\n",
    "# Process each chunk and update notebook incrementally\n",
    "extracted_facts = []\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for i, cell in enumerate(chunk_cells):\n",
    "    cell_index = i + 2  # Skip header and registry cells in facts notebook\n",
    "    \n",
    "    # Check if this cell is already populated\n",
    "    if cell_index < len(facts_nb.cells):\n",
    "        existing_content = facts_nb.cells[cell_index].source\n",
    "        if is_cell_populated(existing_content):\n",
    "            print(f\"  Chunk {i + 1}/{len(chunk_cells)}: ⊘ Already populated, skipping\")\n",
    "            skipped_count += 1\n",
    "            # Still add to extracted_facts for later reference\n",
    "            parts = existing_content.split(\"---\\n\", 1)\n",
    "            context_match = re.search(r'\\*\\*Context:\\*\\*\\s*(.+)', parts[0]) if len(parts) > 1 else None\n",
    "            extracted_facts.append({\n",
    "                \"chunk_index\": i,\n",
    "                \"breadcrumb\": context_match.group(1) if context_match else f\"Chunk {i + 1}\",\n",
    "                \"facts\": parts[1].strip() if len(parts) > 1 else existing_content,\n",
    "            })\n",
    "            continue\n",
    "    \n",
    "    print(f\"  Chunk {i + 1}/{len(chunk_cells)}...\", end=\" \", flush=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Parse the chunk cell to extract content after the separator\n",
    "    cell_content = cell.source\n",
    "    parts = cell_content.split(\"---\\n\", 1)\n",
    "    if len(parts) > 1:\n",
    "        chunk_text = parts[1].strip()\n",
    "        context_match = re.search(r'\\*\\*Context:\\*\\*\\s*(.+)', parts[0])\n",
    "        breadcrumb = context_match.group(1) if context_match else f\"Chunk {i + 1}\"\n",
    "    else:\n",
    "        chunk_text = cell_content\n",
    "        breadcrumb = f\"Chunk {i + 1}\"\n",
    "    \n",
    "    # Call LLM to extract facts with timeout\n",
    "    try:\n",
    "        with timeout_context(CELL_TIMEOUT_SECONDS):\n",
    "            result = facts_chain.invoke({\n",
    "                \"source_url\": provenance[\"source_url\"],\n",
    "                \"breadcrumb\": breadcrumb,\n",
    "                \"known_entities\": get_known_entities_text(registry),\n",
    "                \"chunk_text\": chunk_text,\n",
    "            })\n",
    "        facts_content = result.content\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ ({len(facts_content)} chars, {elapsed:.1f}s)\")\n",
    "    except TimeoutException as e:\n",
    "        facts_content = f\"[Error: Timeout after {CELL_TIMEOUT_SECONDS}s]\"\n",
    "        print(f\"⏱ Timeout after {CELL_TIMEOUT_SECONDS}s\")\n",
    "        error_count += 1\n",
    "    except Exception as e:\n",
    "        facts_content = f\"[Error extracting facts: {e}]\"\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✗ Error after {elapsed:.1f}s: {e}\")\n",
    "        error_count += 1\n",
    "    \n",
    "    extracted_facts.append({\n",
    "        \"chunk_index\": i,\n",
    "        \"breadcrumb\": breadcrumb,\n",
    "        \"facts\": facts_content,\n",
    "    })\n",
    "    \n",
    "    # Update the facts notebook immediately after each cell\n",
    "    updated_content = f\"\"\"**Context:** {breadcrumb}\n",
    "**Chunk:** {i + 1} of {len(chunk_cells)}\n",
    "\n",
    "---\n",
    "\n",
    "{facts_content}\n",
    "\"\"\"\n",
    "    if cell_index < len(facts_nb.cells):\n",
    "        facts_nb.cells[cell_index].source = updated_content\n",
    "    \n",
    "    # Save notebook after each cell\n",
    "    with open(facts_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(facts_nb, f)\n",
    "\n",
    "print(f\"\\nFacts extraction complete:\")\n",
    "print(f\"  - {len(extracted_facts)} total cells\")\n",
    "print(f\"  - {skipped_count} skipped (already populated)\")\n",
    "print(f\"  - {error_count} errors/timeouts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60d29b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Facts notebook: data/albert_einstein_facts.ipynb\n",
      "  - Updated incrementally during processing\n",
      "  - Ready for review/editing before RDF generation\n"
     ]
    }
   ],
   "source": [
    "# Summary of facts extraction (notebook already updated incrementally)\n",
    "print(f\"Facts notebook: {facts_path}\")\n",
    "print(f\"  - Updated incrementally during processing\")\n",
    "print(f\"  - Ready for review/editing before RDF generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65de7f1f",
   "metadata": {},
   "source": [
    "## Process Facts → Generate RDF\n",
    "\n",
    "Run the LLM on each facts cell to generate RDF triples and update the RDF notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afd0797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 63 facts cells to generate RDF...\n",
      "Timeout per cell: 300 seconds (5 minutes)\n",
      "  Facts cell 1/63... ✓ (12346 chars, 48.9s)\n",
      "  Facts cell 2/63... ✓ (2865 chars, 11.8s)\n",
      "  Facts cell 3/63... ✓ (2083 chars, 8.8s)\n",
      "  Facts cell 4/63... ✓ (3801 chars, 16.3s)\n",
      "  Facts cell 5/63... ✓ (2676 chars, 11.1s)\n",
      "  Facts cell 6/63... ✓ (2523 chars, 10.2s)\n",
      "  Facts cell 7/63... ✓ (2796 chars, 12.3s)\n",
      "  Facts cell 8/63... ✓ (4173 chars, 21.4s)\n",
      "  Facts cell 9/63... ✓ (3859 chars, 17.0s)\n",
      "  Facts cell 10/63... ✓ (2386 chars, 11.2s)\n",
      "  Facts cell 11/63... ✓ (5194 chars, 20.5s)\n",
      "  Facts cell 12/63... ✓ (4452 chars, 18.8s)\n",
      "  Facts cell 13/63... ✓ (6327 chars, 25.5s)\n",
      "  Facts cell 14/63... ✓ (1470 chars, 5.7s)\n",
      "  Facts cell 15/63... ✓ (2490 chars, 11.2s)\n",
      "  Facts cell 16/63... ✓ (6967 chars, 30.7s)\n",
      "  Facts cell 17/63... ✓ (4074 chars, 15.4s)\n",
      "  Facts cell 18/63... ✓ (3205 chars, 14.8s)\n",
      "  Facts cell 19/63... ✓ (3035 chars, 12.5s)\n",
      "  Facts cell 20/63... ✓ (2972 chars, 11.8s)\n",
      "  Facts cell 21/63... ✓ (3333 chars, 13.8s)\n",
      "  Facts cell 22/63... ✓ (2395 chars, 11.0s)\n",
      "  Facts cell 23/63... ✓ (3753 chars, 15.1s)\n",
      "  Facts cell 24/63... ✓ (2377 chars, 9.5s)\n",
      "  Facts cell 25/63... ✓ (3720 chars, 14.4s)\n",
      "  Facts cell 26/63... ✓ (3764 chars, 14.5s)\n",
      "  Facts cell 27/63... ✓ (4696 chars, 318.8s)\n",
      "  Facts cell 28/63... ✓ (3243 chars, 13.4s)\n",
      "  Facts cell 29/63... ✓ (1192 chars, 5.3s)\n",
      "  Facts cell 30/63... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 124\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m timeout_context(CELL_TIMEOUT_SECONDS):\n\u001b[0;32m--> 124\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mrdf_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovenance\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_url\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbreadcrumb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbreadcrumb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprefixes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatted_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentity_registry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_entity_registry_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfacts_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     rdf_content \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m    132\u001b[0m     elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:3143\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3142\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3143\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3144\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:398\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    392\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AIMessage:\n\u001b[1;32m    393\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    395\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAIMessage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    396\u001b[0m         cast(\n\u001b[1;32m    397\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 398\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    408\u001b[0m         )\u001b[38;5;241m.\u001b[39mmessage,\n\u001b[1;32m    409\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1115\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1116\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:927\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    926\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 927\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m         )\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    935\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1221\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1225\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:1375\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[1;32m   1369\u001b[0m             response,\n\u001b[1;32m   1370\u001b[0m             schema\u001b[38;5;241m=\u001b[39moriginal_schema_obj,\n\u001b[1;32m   1371\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mgeneration_info,\n\u001b[1;32m   1372\u001b[0m             output_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_version,\n\u001b[1;32m   1373\u001b[0m         )\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         raw_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_raw_response\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m         response \u001b[38;5;241m=\u001b[39m raw_response\u001b[38;5;241m.\u001b[39mparse()\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/openai/_legacy_response.py:364\u001b[0m, in \u001b[0;36mto_raw_response_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m extra_headers[RAW_RESPONSE_HEADER] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extra_headers\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:286\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m   1190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_retention\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/openai/_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 982\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    988\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Format prefixes for RDF generation\n",
    "formatted_prefixes = RDF_PREFIXES.format(source_url=provenance['source_url'])\n",
    "\n",
    "# Create RDF generation prompt\n",
    "rdf_prompt = ChatPromptTemplate.from_template(\"\"\"You are an expert at converting factual statements to RDF triples in Turtle format.\n",
    "\n",
    "Convert the following factual statements to RDF using schema.org vocabulary where possible.\n",
    "\n",
    "Source: {source_url}\n",
    "Section: {breadcrumb}\n",
    "\n",
    "Use these prefixes:\n",
    "{prefixes}\n",
    "\n",
    "Entity registry (use these URIs for known entities):\n",
    "{entity_registry}\n",
    "\n",
    "Guidelines:\n",
    "- Use schema.org properties (schema:birthDate, schema:birthPlace, schema:worksFor, schema:alumniOf, etc.)\n",
    "- For relationships not in schema.org, use wiki3: prefix\n",
    "- Include rdfs:label for new entities\n",
    "- Use xsd datatypes for dates and numbers\n",
    "- Entity URIs use the base with fragment identifiers: <#type_name> (e.g., <#person_albert_einstein>)\n",
    "- Do NOT repeat the @prefix declarations - just output the triples\n",
    "\n",
    "---\n",
    "{facts}\n",
    "---\n",
    "\n",
    "Generate Turtle RDF triples (without prefix declarations):\"\"\")\n",
    "\n",
    "rdf_chain = rdf_prompt | llm\n",
    "\n",
    "def get_entity_registry_text(registry: EntityRegistry) -> str:\n",
    "    \"\"\"Format entity registry for RDF prompt context.\"\"\"\n",
    "    lines = []\n",
    "    for entity in registry.entities.values():\n",
    "        lines.append(f\"- {entity['label']}: <#{entity['id']}> (type: {entity['type']})\")\n",
    "    return \"\\n\".join(lines) if lines else \"None yet\"\n",
    "\n",
    "def is_rdf_cell_populated(cell_content: str) -> bool:\n",
    "    \"\"\"Check if an RDF cell has real content (not just placeholders/comments).\"\"\"\n",
    "    lines = cell_content.strip().split('\\n')\n",
    "    # Check if there are any non-comment, non-empty lines\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if stripped and not stripped.startswith('#'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Read the updated facts notebook\n",
    "facts_nb = nbformat.read(facts_path, as_version=4)\n",
    "\n",
    "# Extract facts cells (skip provenance header and entity registry)\n",
    "facts_cells = [cell for cell in facts_nb.cells[2:] if cell.cell_type == 'markdown']\n",
    "\n",
    "# Read existing RDF notebook to check for already-populated cells\n",
    "rdf_nb = nbformat.read(rdf_path, as_version=4)\n",
    "\n",
    "print(f\"Processing {len(facts_cells)} facts cells to generate RDF...\")\n",
    "print(f\"Timeout per cell: {CELL_TIMEOUT_SECONDS} seconds ({CELL_TIMEOUT_SECONDS // 60} minutes)\")\n",
    "\n",
    "# Process each facts cell and update notebook incrementally\n",
    "generated_rdf = []\n",
    "skipped_count = 0\n",
    "error_count = 0\n",
    "\n",
    "for i, cell in enumerate(facts_cells):\n",
    "    cell_index = i + 2  # Skip header and registry cells in RDF notebook\n",
    "    \n",
    "    # Check if this RDF cell is already populated\n",
    "    if cell_index < len(rdf_nb.cells):\n",
    "        existing_content = rdf_nb.cells[cell_index].source\n",
    "        if is_rdf_cell_populated(existing_content):\n",
    "            print(f\"  Facts cell {i + 1}/{len(facts_cells)}: ⊘ Already populated, skipping\")\n",
    "            skipped_count += 1\n",
    "            # Extract breadcrumb from comment\n",
    "            context_match = re.search(r'# Context:\\s*(.+)', existing_content)\n",
    "            generated_rdf.append({\n",
    "                \"chunk_index\": i,\n",
    "                \"breadcrumb\": context_match.group(1) if context_match else f\"Chunk {i + 1}\",\n",
    "                \"turtle\": existing_content,\n",
    "            })\n",
    "            continue\n",
    "    \n",
    "    print(f\"  Facts cell {i + 1}/{len(facts_cells)}...\", end=\" \", flush=True)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Parse the facts cell to extract content after the separator\n",
    "    cell_content = cell.source\n",
    "    parts = cell_content.split(\"---\\n\", 1)\n",
    "    if len(parts) > 1:\n",
    "        facts_text = parts[1].strip()\n",
    "        context_match = re.search(r'\\*\\*Context:\\*\\*\\s*(.+)', parts[0])\n",
    "        breadcrumb = context_match.group(1) if context_match else f\"Chunk {i + 1}\"\n",
    "    else:\n",
    "        facts_text = cell_content\n",
    "        breadcrumb = f\"Chunk {i + 1}\"\n",
    "    \n",
    "    # Skip if no real facts content\n",
    "    if facts_text.startswith(\"[Factual statements\") or facts_text.startswith(\"[Error\"):\n",
    "        print(\"⊘ Skipped (no facts to convert)\")\n",
    "        rdf_content = f\"# Chunk {i + 1}: No facts to convert\\n\"\n",
    "        generated_rdf.append({\n",
    "            \"chunk_index\": i,\n",
    "            \"breadcrumb\": breadcrumb,\n",
    "            \"turtle\": rdf_content,\n",
    "        })\n",
    "        # Update RDF notebook with placeholder\n",
    "        updated_content = f\"\"\"# Chunk {i + 1}\n",
    "# Context: {breadcrumb}\n",
    "\n",
    "{rdf_content}\n",
    "\"\"\"\n",
    "        if cell_index < len(rdf_nb.cells):\n",
    "            rdf_nb.cells[cell_index].source = updated_content\n",
    "        with open(rdf_path, 'w', encoding='utf-8') as f:\n",
    "            nbformat.write(rdf_nb, f)\n",
    "        continue\n",
    "    \n",
    "    # Call LLM to generate RDF with timeout\n",
    "    try:\n",
    "        with timeout_context(CELL_TIMEOUT_SECONDS):\n",
    "            result = rdf_chain.invoke({\n",
    "                \"source_url\": provenance[\"source_url\"],\n",
    "                \"breadcrumb\": breadcrumb,\n",
    "                \"prefixes\": formatted_prefixes,\n",
    "                \"entity_registry\": get_entity_registry_text(registry),\n",
    "                \"facts\": facts_text,\n",
    "            })\n",
    "        rdf_content = result.content\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✓ ({len(rdf_content)} chars, {elapsed:.1f}s)\")\n",
    "    except TimeoutException as e:\n",
    "        rdf_content = f\"# Error: Timeout after {CELL_TIMEOUT_SECONDS}s\\n\"\n",
    "        print(f\"⏱ Timeout after {CELL_TIMEOUT_SECONDS}s\")\n",
    "        error_count += 1\n",
    "    except Exception as e:\n",
    "        rdf_content = f\"# Error generating RDF: {e}\\n\"\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✗ Error after {elapsed:.1f}s: {e}\")\n",
    "        error_count += 1\n",
    "    \n",
    "    generated_rdf.append({\n",
    "        \"chunk_index\": i,\n",
    "        \"breadcrumb\": breadcrumb,\n",
    "        \"turtle\": rdf_content,\n",
    "    })\n",
    "    \n",
    "    # Update the RDF notebook immediately after each cell\n",
    "    updated_content = f\"\"\"# Chunk {i + 1}\n",
    "# Context: {breadcrumb}\n",
    "\n",
    "{rdf_content}\n",
    "\"\"\"\n",
    "    if cell_index < len(rdf_nb.cells):\n",
    "        rdf_nb.cells[cell_index].source = updated_content\n",
    "    \n",
    "    # Save notebook after each cell\n",
    "    with open(rdf_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(rdf_nb, f)\n",
    "\n",
    "print(f\"\\nRDF generation complete:\")\n",
    "print(f\"  - {len(generated_rdf)} total cells\")\n",
    "print(f\"  - {skipped_count} skipped (already populated)\")\n",
    "print(f\"  - {error_count} errors/timeouts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of RDF generation (notebook already updated incrementally)\n",
    "print(f\"RDF notebook: {rdf_path}\")\n",
    "print(f\"  - Updated incrementally during processing\")\n",
    "print(f\"  - Ready for review/editing before final export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a59d9",
   "metadata": {},
   "source": [
    "## Export Combined RDF\n",
    "\n",
    "Combine all RDF cells into a single Turtle file with prefixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d061083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all RDF into a single Turtle file\n",
    "all_triples = []\n",
    "\n",
    "# Read the updated RDF notebook\n",
    "rdf_nb = nbformat.read(rdf_path, as_version=4)\n",
    "\n",
    "# Collect RDF from all raw cells (skip provenance and registry)\n",
    "for cell in rdf_nb.cells[2:]:\n",
    "    if cell.cell_type == 'raw':\n",
    "        # Skip empty or comment-only cells\n",
    "        content = cell.source.strip()\n",
    "        lines = [line for line in content.split('\\n') if line.strip() and not line.strip().startswith('#')]\n",
    "        if lines:\n",
    "            all_triples.append(content)\n",
    "\n",
    "# Build complete Turtle file\n",
    "turtle_output = f\"\"\"# RDF Knowledge Graph: {provenance['article_title']}\n",
    "# Source: {provenance['source_url']}\n",
    "# License: {provenance['license']}\n",
    "# Generated: {datetime.now().isoformat()}\n",
    "\n",
    "{formatted_prefixes}\n",
    "\n",
    "# === Triples ===\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "turtle_output += \"\\n\\n\".join(all_triples)\n",
    "\n",
    "# Save to file\n",
    "turtle_path = os.path.join(OUTPUT_DIR, f\"{article_slug}.ttl\")\n",
    "with open(turtle_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(turtle_output)\n",
    "\n",
    "print(f\"Exported RDF to: {turtle_path}\")\n",
    "print(f\"  - {len(all_triples)} chunks of triples\")\n",
    "print(f\"  - {len(turtle_output)} characters total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7c4e37",
   "metadata": {},
   "source": [
    "## Save Initial Entity Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d416a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data/entity_registry.json\n",
      "\n",
      "Initial entities: 1\n",
      "  - Albert Einstein (Person): https://en.wikipedia.org/wiki/Albert_Einstein#person_albert_einstein\n"
     ]
    }
   ],
   "source": [
    "# Save entity registry to JSON file\n",
    "registry_path = os.path.join(OUTPUT_DIR, \"entity_registry.json\")\n",
    "with open(registry_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(registry.to_json())\n",
    "\n",
    "print(f\"Saved: {registry_path}\")\n",
    "print(f\"\\nInitial entities: {len(registry.entities)}\")\n",
    "for key, entity in registry.entities.items():\n",
    "    print(f\"  - {entity['label']} ({entity['type']}): {entity['uri']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2368a",
   "metadata": {},
   "source": [
    "## Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31bf97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PIPELINE COMPLETE\n",
      "============================================================\n",
      "\n",
      "Article: Albert Einstein\n",
      "Source: https://en.wikipedia.org/wiki/Albert_Einstein\n",
      "License: CC BY-SA 4.0\n",
      "\n",
      "Generated content notebooks (no Python code):\n",
      "  1. data/albert_einstein_chunks.ipynb\n",
      "     - 63 chunks with breadcrumb context\n",
      "     - Markdown cells with unchanged source text\n",
      "  2. data/albert_einstein_facts.ipynb\n",
      "     - Placeholder cells for factual statements\n",
      "     - Prompt template in header for LLM processing\n",
      "  3. data/albert_einstein_rdf.ipynb\n",
      "     - Placeholder cells for Turtle RDF\n",
      "     - Prefixes and prompt in header\n",
      "\n",
      "Entity registry: data/entity_registry.json\n",
      "\n",
      "Workflow:\n",
      "  1. Review/edit chunks notebook\n",
      "  2. LLM/agent fills facts notebook from chunks\n",
      "  3. Human reviews/edits facts\n",
      "  4. LLM/agent fills RDF notebook from facts\n",
      "  5. Human reviews/edits RDF\n",
      "  6. Export final .ttl file\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nArticle: {ARTICLE_TITLE}\")\n",
    "print(f\"Source: {source_url}\")\n",
    "print(f\"License: {provenance['license']}\")\n",
    "print(f\"\\nGenerated artifacts:\")\n",
    "print(f\"  1. {chunks_path}\")\n",
    "print(f\"     - {len(contextual_chunks)} chunks with breadcrumb context\")\n",
    "print(f\"  2. {facts_path}\")\n",
    "print(f\"     - {len(extracted_facts)} cells with extracted factual statements\")\n",
    "print(f\"  3. {rdf_path}\")\n",
    "print(f\"     - {len(generated_rdf)} cells with RDF triples\")\n",
    "print(f\"  4. {turtle_path}\")\n",
    "print(f\"     - Combined Turtle file for import\")\n",
    "print(f\"\\nEntity registry: {registry_path}\")\n",
    "print(f\"\\nThe intermediate notebooks can be reviewed and edited before re-export.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
