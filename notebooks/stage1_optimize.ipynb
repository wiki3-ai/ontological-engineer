{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48be6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/workspaces/wiki3-kg-project')\n",
    "\n",
    "import dspy\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from ontological_engineer import (\n",
    "    configure_lm,\n",
    "    StatementExtractor,\n",
    "    StatementQualityJudge,\n",
    ")\n",
    "from ontological_engineer.judges import statement_quality_metric\n",
    "from ontological_engineer.training import (\n",
    "    load_stage1_config,\n",
    "    load_trainset,\n",
    "    load_devset,\n",
    "    load_fewshot_examples,\n",
    "    save_optimized_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500a0e1",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Artifacts\n",
    "\n",
    "Load the config and datasets saved by `stage1_statements.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff934464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config:\n",
      "  Model: qwen/qwen3-coder-30b\n",
      "  API base: http://host.docker.internal:1234/v1\n",
      "  Temperature: 0.7\n",
      "  Num fewshot: 3\n",
      "  Config CID: bafkreicshdgzdtbuwtk7efcce2e7bcjq2dskfkwrn3xrq4pjkjgzubwhva\n"
     ]
    }
   ],
   "source": [
    "# Load stage1 config (saved with CID provenance)\n",
    "training_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "\n",
    "config = load_stage1_config(training_dir)\n",
    "print(f\"Loaded config:\")\n",
    "print(f\"  Model: {config['model']}\")\n",
    "print(f\"  API base: {config['api_base']}\")\n",
    "print(f\"  Temperature: {config['temperature']}\")\n",
    "print(f\"  Num fewshot: {config['num_fewshot']}\")\n",
    "print(f\"  Config CID: {config.get('cid', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c8d1c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured LM: <dspy.clients.lm.LM object at 0xffffb05bd8a0>\n"
     ]
    }
   ],
   "source": [
    "# Configure LM from saved config\n",
    "lm = configure_lm(\n",
    "    model=config['model'],\n",
    "    api_base=config['api_base'],\n",
    "    temperature=config['temperature'],\n",
    ")\n",
    "print(f\"Configured LM: {lm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e374bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trainset: 2742 examples\n",
      "Loaded devset: 686 examples\n"
     ]
    }
   ],
   "source": [
    "# Load training and dev sets (with provenance)\n",
    "trainset = load_trainset(training_dir)\n",
    "devset = load_devset(training_dir)\n",
    "\n",
    "print(f\"Loaded trainset: {len(trainset)} examples\")\n",
    "print(f\"Loaded devset: {len(devset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27019b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 few-shot examples\n"
     ]
    }
   ],
   "source": [
    "# Load few-shot examples\n",
    "fewshot_examples = load_fewshot_examples(training_dir)\n",
    "print(f\"Loaded {len(fewshot_examples)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86eda37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score: 92.36\n",
      "Baseline CID: bafkreifcbdiqhxekomajwh4fhpzo3azyli5dydmfimdrdp3kbawnedvnwu\n"
     ]
    }
   ],
   "source": [
    "# Load baseline results for comparison\n",
    "baseline_path = training_dir / \"baseline_results.json\"\n",
    "if baseline_path.exists():\n",
    "    with open(baseline_path) as f:\n",
    "        baseline_results = json.load(f)\n",
    "    baseline_score = baseline_results['score']\n",
    "    print(f\"Baseline score: {baseline_score:.2f}\")\n",
    "    print(f\"Baseline CID: {baseline_results.get('cid', 'N/A')}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No baseline results found. Run stage1_statements.ipynb first.\")\n",
    "    baseline_score = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a483ef",
   "metadata": {},
   "source": [
    "## 2. MLflow Setup\n",
    "\n",
    "Configure MLflow for optimization tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085a202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 18:46:04 INFO mlflow.tracking.fluent: Experiment with name 'wiki3-kg-stage1-optimization' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… MLflow configured\n",
      "   Tracking URI: http://127.0.0.1:5000\n",
      "   Experiment: wiki3-kg-stage1-optimization\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"http://127.0.0.1:5000\"\n",
    "\n",
    "try:\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(\"wiki3-kg-stage1-optimization\")\n",
    "    \n",
    "    # Enable autologging with full optimizer tracking\n",
    "    # Reference: https://dspy.ai/tutorials/optimizer_tracking/\n",
    "    mlflow.dspy.autolog(\n",
    "        log_compiles=True,\n",
    "        log_evals=True,\n",
    "        log_traces_from_compile=True\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… MLflow configured\")\n",
    "    print(f\"   Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "    print(f\"   Experiment: wiki3-kg-stage1-optimization\")\n",
    "    MLFLOW_ENABLED = True\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ MLflow not available: {e}\")\n",
    "    MLFLOW_ENABLED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923827e3",
   "metadata": {},
   "source": [
    "## 3. MIPROv2 Optimization\n",
    "\n",
    "Run prompt optimization using MIPROv2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a53a1ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing with 2742 training examples...\n",
      "Using 3 few-shot demos for bootstrapping...\n",
      "MIPROv2 mode: auto='light'\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "# Configure optimizer\n",
    "optimizer = MIPROv2(\n",
    "    metric=statement_quality_metric,\n",
    "    auto=\"light\",  # \"light\", \"medium\", or \"heavy\"\n",
    ")\n",
    "\n",
    "NUM_FEWSHOT = config['num_fewshot']\n",
    "\n",
    "print(f\"Optimizing with {len(trainset)} training examples...\")\n",
    "print(f\"Using {NUM_FEWSHOT} few-shot demos for bootstrapping...\")\n",
    "print(f\"MIPROv2 mode: auto='light'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80c076c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 18:46:12 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '8f5c4c5bf86e4340a656f181d328fc80', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current dspy workflow\n",
      "2025/12/20 18:46:12 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING LIGHT AUTO RUN SETTINGS:\n",
      "num_trials: 10\n",
      "minibatch: True\n",
      "num_fewshot_candidates: 6\n",
      "num_instruct_candidates: 3\n",
      "valset size: 100\n",
      "\n",
      "2025/12/20 18:46:12 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/12/20 18:46:12 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/12/20 18:46:12 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=6 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 1/6\n",
      "Bootstrapping set 2/6\n",
      "Bootstrapping set 3/6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e74eb7ec1024a02afa9f20abb268d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1742 [00:36<5:48:18, 12.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08bbd4754d843009ee6cf078f65ab5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 4/6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6ec81ab92d485ba267c7148d99b665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1742 [00:20<4:52:47, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caa531284954ee88ae601502051b34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 5/6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1511434bb242b99ec7e8412bd98b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1742 [00:28<6:55:33, 14.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1456a3508ca24bc38ca505d822784d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 6/6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2fdb48c0fb4f66b6ea4b7411998e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1742 [00:13<6:22:46, 13.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9494f20d73124a73b657f98be75d2016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 18:47:54 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/12/20 18:47:54 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "2025/12/20 18:48:48 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing N=3 instructions...\n",
      "\n",
      "2025/12/20 18:49:38 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/12/20 18:49:38 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Extract atomic, verifiable statements from Wikipedia text.\n",
      "\n",
      "Each statement must:\n",
      "- Be self-contained (understandable without the original text)\n",
      "- Preserve markdown links: [Entity Name](/wiki/Entity_Name)\n",
      "- Contain exactly one verifiable claim\n",
      "- Not editorialize or interpret beyond what's stated\n",
      "\n",
      "Example input chunk:\n",
      "    \"Albert Einstein was born in Ulm, in the Kingdom of WÃ¼rttemberg \n",
      "    in the German Empire, on 14 March 1879.\"\n",
      "    \n",
      "Example output statements:\n",
      "    - \"[Albert Einstein](/wiki/Albert_Einstein) was born in [Ulm](/wiki/Ulm).\"\n",
      "    - \"[Albert Einstein](/wiki/Albert_Einstein) was born on 14 March 1879.\"\n",
      "    - \"[Ulm](/wiki/Ulm) was in the [Kingdom of WÃ¼rttemberg](/wiki/Kingdom_of_WÃ¼rttemberg).\"\n",
      "\n",
      "2025/12/20 18:49:38 INFO dspy.teleprompt.mipro_optimizer_v2: 1: Extract atomic, verifiable statements from Wikipedia text chunks while preserving entity links and ensuring each statement conveys a single, self-contained fact.\n",
      "\n",
      "**Task Overview:**\n",
      "You are given a chunk of text from a Wikipedia article and a section context (breadcrumb showing location: Article > Section > Subsection). Your goal is to extract individual, verifiable claims from this text as atomic statements.\n",
      "\n",
      "**Instructions:**\n",
      "1. **Each statement must be self-contained**: Understandable without referring back to the original text.\n",
      "2. **Preserve markdown links**: If entities are linked using [Entity Name](/wiki/Entity_Name) format, maintain these links exactly as they appear in the source.\n",
      "3. **One claim per statement**: Each output statement should contain only one verifiable fact.\n",
      "4. **Avoid editorializing**: Do not add interpretation, opinion, or extra explanation beyond what is directly stated in the text.\n",
      "5. **Maintain factual integrity**: Ensure all extracted statements reflect information explicitly present in the chunk.\n",
      "\n",
      "**Example Input:**\n",
      "Chunk Text:\n",
      "\"Albert Einstein was born in Ulm, in the Kingdom of WÃ¼rttemberg in the German Empire, on 14 March 1879.\"\n",
      "\n",
      "Section Context:\n",
      "\"Albert Einstein > Early life\"\n",
      "\n",
      "**Expected Output:**\n",
      "- \"[Albert Einstein](/wiki/Albert_Einstein) was born in [Ulm](/wiki/Ulm).\"\n",
      "- \"[Albert Einstein](/wiki/Albert_Einstein) was born on 14 March 1879.\"\n",
      "- \"[Ulm](/wiki/Ulm) was in the [Kingdom of WÃ¼rttemberg](/wiki/Kingdom_of_WÃ¼rttemberg).\"\n",
      "\n",
      "**Processing Notes:**\n",
      "- If no markdown links are present, do not add any.\n",
      "- Statements should be logically independent and concise.\n",
      "- Handle multi-sentence input by identifying each distinct claim.\n",
      "\n",
      "2025/12/20 18:49:38 INFO dspy.teleprompt.mipro_optimizer_v2: 2: Extract atomic, verifiable statements from Wikipedia text chunks.\n",
      "\n",
      "Each statement must:\n",
      "- Be self-contained (understandable without the original text)\n",
      "- Preserve markdown links: [Entity Name](/wiki/Entity_Name)\n",
      "- Contain exactly one verifiable claim\n",
      "- Not editorialize or interpret beyond what's stated\n",
      "\n",
      "Example input chunk:\n",
      "    \"Albert Einstein was born in Ulm, in the Kingdom of WÃ¼rttemberg \n",
      "    in the German Empire, on 14 March 1879.\"\n",
      "    \n",
      "Example output statements:\n",
      "    - \"[Albert Einstein](/wiki/Albert_Einstein) was born in [Ulm](/wiki/Ulm).\"\n",
      "    - \"[Albert Einstein](/wiki/Albert_Einstein) was born on 14 March 1879.\"\n",
      "    - \"[Ulm](/wiki/Ulm) was in the [Kingdom of WÃ¼rttemberg](/wiki/Kingdom_of_WÃ¼rttemberg).\n",
      "\n",
      "2025/12/20 18:49:38 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/12/20 18:49:38 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/12/20 18:49:38 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/12/20 18:49:38 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 13 - Full Evaluation of Default Program ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 92.70 / 100 (92.7%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [19:00<00:00, 11.41s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:08:39 INFO dspy.evaluate.evaluate: Average Metric: 92.70166666666663 / 100 (92.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:08:39 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 92.7\n",
      "\n",
      "/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/12/20 19:08:39 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 2 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_full_0 at: http://127.0.0.1:5000/#/experiments/2/runs/d753bab8f8cb478fb04a2dbf0f1c1e21\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "Average Metric: 32.89 / 35 (94.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [06:44<00:00, 11.57s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:15:24 INFO dspy.evaluate.evaluate: Average Metric: 32.88750000000001 / 35 (94.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:15:25 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 93.96 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/12/20 19:15:25 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [93.96]\n",
      "2025/12/20 19:15:25 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [92.7]\n",
      "2025/12/20 19:15:25 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 92.7\n",
      "2025/12/20 19:15:25 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/20 19:15:25 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 3 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_minibatch_0 at: http://127.0.0.1:5000/#/experiments/2/runs/ac6153b2d46c424aaf4f5e6313fd90b4\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "Average Metric: 33.48 / 35 (95.7%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [06:43<00:00, 11.52s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:22:08 INFO dspy.evaluate.evaluate: Average Metric: 33.4775 / 35 (95.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:22:09 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 95.65 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/12/20 19:22:09 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [93.96, 95.65]\n",
      "2025/12/20 19:22:09 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [92.7]\n",
      "2025/12/20 19:22:09 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 92.7\n",
      "2025/12/20 19:22:09 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/20 19:22:09 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 4 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_minibatch_1 at: http://127.0.0.1:5000/#/experiments/2/runs/7bc86e22dadd4bb1a51483fa4b4cf7a8\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "Average Metric: 32.11 / 35 (91.8%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [06:35<00:00, 11.30s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:28:44 INFO dspy.evaluate.evaluate: Average Metric: 32.112500000000004 / 35 (91.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:28:45 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 91.75 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/12/20 19:28:45 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [93.96, 95.65, 91.75]\n",
      "2025/12/20 19:28:45 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [92.7]\n",
      "2025/12/20 19:28:45 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 92.7\n",
      "2025/12/20 19:28:45 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/20 19:28:45 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 5 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_minibatch_2 at: http://127.0.0.1:5000/#/experiments/2/runs/bfa47ee4e4d844148a8715f5a5a96653\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "Average Metric: 33.94 / 35 (97.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [06:39<00:00, 11.40s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:35:24 INFO dspy.evaluate.evaluate: Average Metric: 33.937500000000014 / 35 (97.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:35:24 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 96.96 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 2'].\n",
      "2025/12/20 19:35:24 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [93.96, 95.65, 91.75, 96.96]\n",
      "2025/12/20 19:35:24 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [92.7]\n",
      "2025/12/20 19:35:24 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 92.7\n",
      "2025/12/20 19:35:24 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/20 19:35:24 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 6 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_minibatch_3 at: http://127.0.0.1:5000/#/experiments/2/runs/3b61a9bc944141728a0a5be877736cb8\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "Average Metric: 33.26 / 35 (95.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [05:52<00:00, 10.07s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:41:17 INFO dspy.evaluate.evaluate: Average Metric: 33.25750000000001 / 35 (95.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:41:17 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 95.02 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/12/20 19:41:17 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [93.96, 95.65, 91.75, 96.96, 95.02]\n",
      "2025/12/20 19:41:17 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [92.7]\n",
      "2025/12/20 19:41:17 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 92.7\n",
      "2025/12/20 19:41:17 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/20 19:41:17 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 13 - Full Evaluation =====\n",
      "2025/12/20 19:41:17 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 96.96) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_minibatch_4 at: http://127.0.0.1:5000/#/experiments/2/runs/342fa845282d405cbcab28450d68d671\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "Average Metric: 95.00 / 100 (95.0%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [12:11<00:00,  7.32s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:53:29 INFO dspy.evaluate.evaluate: Average Metric: 94.99999999999994 / 100 (95.0%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:53:30 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mNew best full eval score!\u001b[0m Score: 95.0\n",
      "2025/12/20 19:53:30 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [92.7, 95.0]\n",
      "2025/12/20 19:53:30 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 95.0\n",
      "2025/12/20 19:53:30 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/12/20 19:53:30 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/12/20 19:53:30 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 8 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_full_1 at: http://127.0.0.1:5000/#/experiments/2/runs/c074578dabec486dbfdf606708960207\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "Average Metric: 31.20 / 35 (89.1%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:59<00:00,  6.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:57:30 INFO dspy.evaluate.evaluate: Average Metric: 31.200000000000003 / 35 (89.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 19:57:30 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 89.14 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/12/20 19:57:30 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [93.96, 95.65, 91.75, 96.96, 95.02, 89.14]\n",
      "2025/12/20 19:57:30 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [92.7, 95.0]\n",
      "2025/12/20 19:57:30 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 95.0\n",
      "2025/12/20 19:57:30 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/20 19:57:30 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 9 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_minibatch_5 at: http://127.0.0.1:5000/#/experiments/2/runs/9fc4105e6002427ca58e3b21e13b8376\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "Average Metric: 33.48 / 35 (95.6%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [06:15<00:00, 10.73s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 20:03:46 INFO dspy.evaluate.evaluate: Average Metric: 33.475 / 35 (95.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 20:03:46 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 95.64 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/12/20 20:03:46 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [93.96, 95.65, 91.75, 96.96, 95.02, 89.14, 95.64]\n",
      "2025/12/20 20:03:46 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [92.7, 95.0]\n",
      "2025/12/20 20:03:46 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 95.0\n",
      "2025/12/20 20:03:46 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
      "\n",
      "\n",
      "2025/12/20 20:03:46 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 10 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_minibatch_6 at: http://127.0.0.1:5000/#/experiments/2/runs/5049e2da346143b6ac08cfe7f6b5a491\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "Average Metric: 32.65 / 35 (93.3%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [07:13<00:00, 12.39s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 20:11:00 INFO dspy.evaluate.evaluate: Average Metric: 32.65000000000001 / 35 (93.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 20:11:01 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 93.29 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 4'].\n",
      "2025/12/20 20:11:01 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [93.96, 95.65, 91.75, 96.96, 95.02, 89.14, 95.64, 93.29]\n",
      "2025/12/20 20:11:01 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [92.7, 95.0]\n",
      "2025/12/20 20:11:01 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 95.0\n",
      "2025/12/20 20:11:01 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/20 20:11:01 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 11 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_minibatch_7 at: http://127.0.0.1:5000/#/experiments/2/runs/0efdce0c6e374f8487d4592cb6b4fe32\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "Average Metric: 33.56 / 35 (95.9%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [06:45<00:00, 11.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 20:17:46 INFO dspy.evaluate.evaluate: Average Metric: 33.56250000000001 / 35 (95.9%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 20:17:46 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 95.89 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 2'].\n",
      "2025/12/20 20:17:46 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [93.96, 95.65, 91.75, 96.96, 95.02, 89.14, 95.64, 93.29, 95.89]\n",
      "2025/12/20 20:17:46 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [92.7, 95.0]\n",
      "2025/12/20 20:17:46 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 95.0\n",
      "2025/12/20 20:17:46 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/20 20:17:46 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 12 / 13 - Minibatch ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_minibatch_8 at: http://127.0.0.1:5000/#/experiments/2/runs/109b64b9ba47470e9ceb2ab078d9bcb5\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "Average Metric: 32.66 / 35 (93.3%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:16<00:00,  5.62s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 20:21:03 INFO dspy.evaluate.evaluate: Average Metric: 32.66250000000001 / 35 (93.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 20:21:04 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 93.32 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 2'].\n",
      "2025/12/20 20:21:04 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [93.96, 95.65, 91.75, 96.96, 95.02, 89.14, 95.64, 93.29, 95.89, 93.32]\n",
      "2025/12/20 20:21:04 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [92.7, 95.0]\n",
      "2025/12/20 20:21:04 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 95.0\n",
      "2025/12/20 20:21:04 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
      "\n",
      "\n",
      "2025/12/20 20:21:04 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 13 / 13 - Full Evaluation =====\n",
      "2025/12/20 20:21:04 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 95.64) from minibatch trials...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_minibatch_9 at: http://127.0.0.1:5000/#/experiments/2/runs/299e3f39deaf4df6a8372738b43d29de\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "Average Metric: 95.82 / 100 (95.8%): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [11:52<00:00,  7.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 20:32:56 INFO dspy.evaluate.evaluate: Average Metric: 95.81999999999991 / 100 (95.8%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 20:32:57 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mNew best full eval score!\u001b[0m Score: 95.82\n",
      "2025/12/20 20:32:57 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [92.7, 95.0, 95.82]\n",
      "2025/12/20 20:32:57 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 95.82\n",
      "2025/12/20 20:32:57 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
      "2025/12/20 20:32:57 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/12/20 20:32:57 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 95.82!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run eval_full_2 at: http://127.0.0.1:5000/#/experiments/2/runs/0b9f15028333461ba088e9f0d42ec3fa\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac42129a5744ca09a05ecb147c7a9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run hilarious-roo-68 at: http://127.0.0.1:5000/#/experiments/2/runs/8f5c4c5bf86e4340a656f181d328fc80\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n",
      "\n",
      "âœ… Optimization complete!\n",
      "ðŸ“Š View traces in MLflow UI: http://127.0.0.1:5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-ae9aeb5bb1a0c3cbb7e2f1a412c91806&amp;experiment_id=2&amp;trace_id=tr-5bcb80e848a3bc27751bc383e3497908&amp;experiment_id=2&amp;trace_id=tr-f0393c7446c24ed59045f045cc4ac867&amp;experiment_id=2&amp;trace_id=tr-f07fc99551559bf06b61a192c5620ff5&amp;experiment_id=2&amp;trace_id=tr-b5ce33b472a45adec1ac77e7acd1be94&amp;experiment_id=2&amp;trace_id=tr-ffc87ea980918f45b9561058c57cf207&amp;experiment_id=2&amp;trace_id=tr-b2d7d4d243d04e7de345a2ee5401d361&amp;experiment_id=2&amp;trace_id=tr-039b45ba8744b2d6c03d8befd4b28c7a&amp;experiment_id=2&amp;trace_id=tr-2e65362ca6598de62644ff1a4d2c3b72&amp;experiment_id=2&amp;trace_id=tr-84ecc9ae3c0bd9341b43e2249fc39171&amp;experiment_id=2&amp;version=3.7.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-ae9aeb5bb1a0c3cbb7e2f1a412c91806), Trace(trace_id=tr-5bcb80e848a3bc27751bc383e3497908), Trace(trace_id=tr-f0393c7446c24ed59045f045cc4ac867), Trace(trace_id=tr-f07fc99551559bf06b61a192c5620ff5), Trace(trace_id=tr-b5ce33b472a45adec1ac77e7acd1be94), Trace(trace_id=tr-ffc87ea980918f45b9561058c57cf207), Trace(trace_id=tr-b2d7d4d243d04e7de345a2ee5401d361), Trace(trace_id=tr-039b45ba8744b2d6c03d8befd4b28c7a), Trace(trace_id=tr-2e65362ca6598de62644ff1a4d2c3b72), Trace(trace_id=tr-84ecc9ae3c0bd9341b43e2249fc39171)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run optimization\n",
    "optimized_extractor = optimizer.compile(\n",
    "    StatementExtractor(),\n",
    "    trainset=trainset,\n",
    "    max_bootstrapped_demos=NUM_FEWSHOT,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Optimization complete!\")\n",
    "if MLFLOW_ENABLED:\n",
    "    print(f\"ðŸ“Š View traces in MLflow UI: {MLFLOW_TRACKING_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c9a8ab",
   "metadata": {},
   "source": [
    "## 4. Evaluate Optimized Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b27b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on dev set\n",
    "EVAL_SIZE = len(devset)\n",
    "\n",
    "evaluator = dspy.Evaluate(\n",
    "    devset=devset[:EVAL_SIZE],\n",
    "    metric=statement_quality_metric,\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    ")\n",
    "\n",
    "optimized_result = evaluator(optimized_extractor)\n",
    "optimized_score = optimized_result.score if hasattr(optimized_result, 'score') else float(optimized_result)\n",
    "\n",
    "print(f\"\\nOptimized score: {optimized_score:.2f}\")\n",
    "if baseline_score is not None:\n",
    "    improvement = optimized_score - baseline_score\n",
    "    print(f\"Baseline score:  {baseline_score:.2f}\")\n",
    "    print(f\"Improvement:     {improvement:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7209a46f",
   "metadata": {},
   "source": [
    "## 5. Inspect Optimized Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f77a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized extractor configuration:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if hasattr(optimized_extractor, 'demos'):\n",
    "    print(f\"\\nDemonstrations: {len(optimized_extractor.demos)}\")\n",
    "    for i, demo in enumerate(optimized_extractor.demos[:2], 1):\n",
    "        print(f\"  Demo {i}: {demo.section_context[:50]}...\")\n",
    "\n",
    "if hasattr(optimized_extractor, 'signature'):\n",
    "    print(f\"\\nSignature: {optimized_extractor.signature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ac950",
   "metadata": {},
   "source": [
    "## 6. Save Optimized Model\n",
    "\n",
    "Save with CID provenance for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12541c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimized extractor with provenance\n",
    "output_path = save_optimized_extractor(\n",
    "    extractor=optimized_extractor,\n",
    "    output_dir=training_dir,\n",
    "    config=config,\n",
    "    baseline_score=baseline_score,\n",
    "    optimized_score=optimized_score,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Saved optimized extractor to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimization results summary\n",
    "results = {\n",
    "    \"baseline_score\": baseline_score,\n",
    "    \"optimized_score\": optimized_score,\n",
    "    \"improvement\": optimized_score - baseline_score if baseline_score else None,\n",
    "    \"train_size\": len(trainset),\n",
    "    \"eval_size\": EVAL_SIZE,\n",
    "    \"num_fewshot\": NUM_FEWSHOT,\n",
    "    \"optimizer\": \"MIPROv2\",\n",
    "    \"auto_mode\": \"light\",\n",
    "    \"config_cid\": config.get('cid'),\n",
    "}\n",
    "\n",
    "results_path = training_dir / \"optimization_results.json\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Saved optimization results to: {results_path}\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0e2af",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. Loaded config and datasets from `stage1_statements.ipynb`\n",
    "2. Ran MIPROv2 prompt optimization\n",
    "3. Evaluated the optimized extractor\n",
    "4. Saved the optimized model with provenance\n",
    "\n",
    "**Next steps**:\n",
    "- Review optimization traces in MLflow UI\n",
    "- Run Stage 2: Schema matching\n",
    "- Run Stage 3: RDF generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
