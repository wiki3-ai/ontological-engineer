{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6df303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 1: Statement Extraction with DSPy\n",
    "# =============================================================================\n",
    "# This notebook orchestrates statement extraction training/evaluation.\n",
    "# All application logic is in ontological_engineer - this notebook coordinates.\n",
    "#\n",
    "# Outputs (with CID provenance):\n",
    "#   - data/training/chunks/*.ipynb - Chunked Wikipedia pages\n",
    "#   - data/training/statements.ipynb - Extracted statements\n",
    "#   - data/training/classifications.ipynb - Per-statement judgments\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/workspaces/wiki3-kg-project')\n",
    "\n",
    "import dspy\n",
    "import json\n",
    "from pathlib import Path\n",
    "from random import shuffle, seed as random_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ontological_engineer import (\n",
    "    # LM Configuration\n",
    "    configure_lm,\n",
    "    # DSPy Modules\n",
    "    StatementExtractor,\n",
    "    StatementQualityJudge,\n",
    "    StatementClassifier,\n",
    "    StatementClassification,\n",
    "    # Data Loading (from provenance-tracked notebooks)\n",
    "    WikipediaPage,\n",
    "    WikipediaChunk,\n",
    "    load_sample_from_notebook,\n",
    "    load_chunks_from_notebook,\n",
    "    # Processing (all logic in module!)\n",
    "    process_wikipedia_sample,\n",
    "    fetch_page_content,\n",
    "    chunk_article,\n",
    "    # Provenance notebook generation\n",
    "    save_notebook,\n",
    "    get_processed_chunk_cids,\n",
    ")\n",
    "from ontological_engineer.judges import statement_quality_metric\n",
    "from ontological_engineer.training.bootstrap import (\n",
    "    load_chunks_from_notebook as load_albert_chunks,\n",
    "    load_facts_from_notebook,\n",
    "    create_training_examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f86eb6",
   "metadata": {},
   "source": [
    "## 1. Configure Language Model\n",
    "\n",
    "Connect to LM Studio running Qwen-30B (or your preferred model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the LM (defaults to Qwen-30B via LM Studio)\n",
    "lm = configure_lm(\n",
    "    model=\"qwen/qwen3-coder-30b\",\n",
    "    api_base=\"http://host.docker.internal:1234/v1\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(f\"Configured LM: {lm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185bc4f7",
   "metadata": {},
   "source": [
    "## 2. Load Few-Shot Examples (Albert Einstein)\n",
    "\n",
    "Albert Einstein is our gold-standard example. These chunks and their extracted facts\n",
    "serve as few-shot demonstrations for the extractor and judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b715a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Albert Einstein data for few-shot examples\n",
    "fewshot_dir = Path(\"/workspaces/wiki3-kg-project/data/albert_einstein/20251218_231446\")\n",
    "\n",
    "# Use load_albert_chunks (from bootstrap module) - NOT load_chunks_from_notebook\n",
    "fewshot_chunks = load_albert_chunks(fewshot_dir / \"chunks.ipynb\")\n",
    "fewshot_facts = load_facts_from_notebook(fewshot_dir / \"facts.ipynb\")\n",
    "\n",
    "print(f\"Loaded {len(fewshot_chunks)} chunks from Albert Einstein\")\n",
    "print(f\"Loaded {len(fewshot_facts)} fact sets\")\n",
    "\n",
    "# Create few-shot examples\n",
    "fewshot_examples = create_training_examples(fewshot_chunks, fewshot_facts)\n",
    "print(f\"Created {len(fewshot_examples)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5911ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few-shot example\n",
    "if fewshot_examples:\n",
    "    ex = fewshot_examples[0]\n",
    "    print(\"Sample few-shot example:\")\n",
    "    print(f\"  Context: {ex.section_context}\")\n",
    "    print(f\"  Text: {ex.chunk_text[:200]}...\")\n",
    "    print(f\"  Statements: {len(ex.statements)} items\")\n",
    "    for stmt in ex.statements[:3]:\n",
    "        print(f\"    - {stmt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06944",
   "metadata": {},
   "source": [
    "## 3. Load Wikipedia Sample for Training\n",
    "\n",
    "Load the 100-page Wikipedia sample from the provenance-tracked notebook.\n",
    "If the notebook doesn't exist, fall back to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9640f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wikipedia sample (prefer provenance-tracked notebook)\n",
    "sample_notebook = Path(\"/workspaces/wiki3-kg-project/data/training/wikipedia_sample.ipynb\")\n",
    "sample_json = Path(\"/workspaces/wiki3-kg-project/data/training/wikipedia_sample.json\")\n",
    "\n",
    "if sample_notebook.exists():\n",
    "    # Load from provenance-tracked notebook\n",
    "    wiki_pages = load_sample_from_notebook(sample_notebook)\n",
    "    print(f\"‚úÖ Loaded {len(wiki_pages)} pages from provenance-tracked notebook\")\n",
    "    print(f\"   Source: {sample_notebook}\")\n",
    "elif sample_json.exists():\n",
    "    # Fall back to JSON format\n",
    "    with open(sample_json) as f:\n",
    "        wiki_sample = json.load(f)\n",
    "    wiki_pages = [WikipediaPage(title=p['title'], views=p['views']) for p in wiki_sample['pages']]\n",
    "    print(f\"‚ö†Ô∏è  Loaded {len(wiki_pages)} pages from JSON (no provenance)\")\n",
    "    print(f\"   Run sample_wikipedia_pages.ipynb to generate provenance-tracked version\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No Wikipedia sample found. Run sample_wikipedia_pages.ipynb first.\")\n",
    "\n",
    "print(f\"\\nFirst 10 pages:\")\n",
    "for p in wiki_pages[:10]:\n",
    "    print(f\"  - {p.title} ({p.views:,} views)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab17fa",
   "metadata": {},
   "source": [
    "## 4. Fetch and Chunk Wikipedia Pages (with CID Provenance)\n",
    "\n",
    "Fetch page content and chunk it. Each page's chunks are saved to a \n",
    "provenance-tracked notebook with CID signatures.\n",
    "\n",
    "**Note**: Uses `fetch_page_content` and `chunk_article` from `ontological_engineer` - \n",
    "no application logic defined in this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be01db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing parameters\n",
    "chunks_dir = Path(\"/workspaces/wiki3-kg-project/data/training/chunks\")\n",
    "MAX_PAGES = len(wiki_pages)\n",
    "MIN_CHUNK_LENGTH = 60  # Skip very short chunks\n",
    "\n",
    "# Quick test on one page first\n",
    "test_page = wiki_pages[0]\n",
    "print(f\"Testing on: {test_page.title}\")\n",
    "\n",
    "content = fetch_page_content(test_page.title)\n",
    "if content:\n",
    "    chunks = chunk_article(test_page.title, content)\n",
    "    chunks = [c for c in chunks if len(c.text) >= MIN_CHUNK_LENGTH]\n",
    "    print(f\"  ‚Üí {len(chunks)} chunks (filtered by min_length={MIN_CHUNK_LENGTH})\")\n",
    "    if chunks:\n",
    "        print(f\"  First chunk preview: {chunks[0].text[:200]}...\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Could not fetch content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886bea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all pages - logic is in process_wikipedia_sample()\n",
    "# Handles: fetching, chunking, saving with CID provenance, incremental processing\n",
    "\n",
    "training_chunks, pages_processed = process_wikipedia_sample(\n",
    "    pages=wiki_pages,\n",
    "    output_dir=chunks_dir,\n",
    "    max_pages=MAX_PAGES,\n",
    "    min_chunk_length=MIN_CHUNK_LENGTH,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {pages_processed} pages\")\n",
    "print(f\"   Total training chunks: {len(training_chunks)}\")\n",
    "print(f\"   Chunks saved to: {chunks_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf50dd2",
   "metadata": {},
   "source": [
    "## 5. Initialize Extractor with Few-Shot Examples\n",
    "\n",
    "Create the statement extractor and provide Albert Einstein examples as demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e38b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best few-shot examples (ones with good variety of statements)\n",
    "NUM_FEWSHOT = 3\n",
    "\n",
    "# Sort by statement count to get diverse examples\n",
    "sorted_fewshot = sorted(fewshot_examples, key=lambda x: len(x.statements), reverse=True)\n",
    "selected_fewshot = sorted_fewshot[:NUM_FEWSHOT]\n",
    "\n",
    "print(f\"Selected {len(selected_fewshot)} few-shot examples:\")\n",
    "for i, ex in enumerate(selected_fewshot, 1):\n",
    "    print(f\"  {i}. {ex.section_context[:50]}... ({len(ex.statements)} statements)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9896eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extractor with few-shot demonstrations\n",
    "extractor = StatementExtractor()\n",
    "\n",
    "# In DSPy, we can provide demonstrations directly\n",
    "# The few-shot examples will be used by MIPROv2 for bootstrapping\n",
    "print(\"Extractor initialized\")\n",
    "print(f\"Few-shot examples available: {len(selected_fewshot)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4419625",
   "metadata": {},
   "source": [
    "## 6. Test Extraction on Training Sample\n",
    "\n",
    "Run the extractor on a few training chunks to verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a training chunk\n",
    "if training_chunks:\n",
    "    test_chunk = training_chunks[0]\n",
    "    \n",
    "    print(f\"Testing on: {test_chunk.section_context}\")\n",
    "    print(f\"Text: {test_chunk.text[:300]}...\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    result = extractor(\n",
    "        chunk_text=test_chunk.text,\n",
    "        section_context=test_chunk.section_context,\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracted {len(result.statements)} statements:\")\n",
    "    for i, stmt in enumerate(result.statements[:10], 1):\n",
    "        print(f\"  {i}. {stmt}\")\n",
    "    if len(result.statements) > 10:\n",
    "        print(f\"  ... and {len(result.statements) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a247a7a",
   "metadata": {},
   "source": [
    "## 7. Create Training Dataset\n",
    "\n",
    "Convert chunks into DSPy examples. For training, we need to generate initial extractions\n",
    "that can be scored and optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab9914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training examples (without labels - we'll generate and judge them)\n",
    "# For DSPy optimization, we just need the inputs\n",
    "\n",
    "random_seed(42)  # For reproducibility\n",
    "\n",
    "# Convert WikipediaChunk objects to DSPy examples\n",
    "trainset_chunks = list(training_chunks)  # Make a copy\n",
    "shuffle(trainset_chunks)\n",
    "\n",
    "trainset = []\n",
    "for chunk in trainset_chunks:\n",
    "    ex = dspy.Example(\n",
    "        chunk_text=chunk.text,\n",
    "        section_context=chunk.section_context,\n",
    "    ).with_inputs('chunk_text', 'section_context')\n",
    "    trainset.append(ex)\n",
    "\n",
    "# Split into train/dev\n",
    "split_idx = int(len(trainset) * 0.8)\n",
    "devset = trainset[split_idx:]\n",
    "trainset = trainset[:split_idx]\n",
    "\n",
    "print(f\"Training set: {len(trainset)} examples\")\n",
    "print(f\"Dev set: {len(devset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda75e24",
   "metadata": {},
   "source": [
    "## 8. Initialize Judge with Few-Shot Guidance\n",
    "\n",
    "The judge scores extraction quality. We use Albert Einstein examples to calibrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792d6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize judge\n",
    "judge = StatementQualityJudge()\n",
    "\n",
    "# Test judge on a known good example (Albert Einstein few-shot)\n",
    "if selected_fewshot:\n",
    "    test_ex = selected_fewshot[0]\n",
    "    \n",
    "    evaluation = judge(\n",
    "        chunk_text=test_ex.chunk_text,\n",
    "        section_context=test_ex.section_context,\n",
    "        statements=test_ex.statements,\n",
    "    )\n",
    "    \n",
    "    print(\"Judge calibration on few-shot example:\")\n",
    "    print(f\"  Completeness:      {evaluation.completeness:.2f}\")\n",
    "    print(f\"  Atomicity:         {evaluation.atomicity:.2f}\")\n",
    "    print(f\"  Accuracy:          {evaluation.accuracy:.2f}\")\n",
    "    print(f\"  Link preservation: {evaluation.link_preservation:.2f}\")\n",
    "    print(f\"  ---\")\n",
    "    print(f\"  Weighted score:    {evaluation.weighted_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649d2fc",
   "metadata": {},
   "source": [
    "## 9. Baseline Evaluation\n",
    "\n",
    "Evaluate the unoptimized extractor on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline on dev set\n",
    "EVAL_SIZE = min(10, len(devset))  # Limit for speed\n",
    "\n",
    "evaluator = dspy.Evaluate(\n",
    "    devset=devset[:EVAL_SIZE],\n",
    "    metric=statement_quality_metric,\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    ")\n",
    "\n",
    "baseline_extractor = StatementExtractor()\n",
    "baseline_result = evaluator(baseline_extractor)\n",
    "\n",
    "baseline_score = baseline_result.score if hasattr(baseline_result, 'score') else float(baseline_result)\n",
    "print(f\"\\nBaseline quality score: {baseline_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e36a0",
   "metadata": {},
   "source": [
    "## 9b. MLflow Observability Setup\n",
    "\n",
    "MLflow provides tracing, evaluation, and human feedback tools for DSPy pipelines.\n",
    "\n",
    "### Quick Setup (One-time)\n",
    "\n",
    "1. **Install MLflow** (already in requirements or run cell below)\n",
    "2. **Start the MLflow server** in a terminal:\n",
    "   ```bash\n",
    "   cd /workspaces/wiki3-kg-project\n",
    "   mlflow server \\\n",
    "     --backend-store-uri sqlite:///mlflow.sqlite \\\n",
    "     --default-artifact-root ./mlflow-artifacts \\\n",
    "     --host 0.0.0.0 \\\n",
    "     --port 5000\n",
    "   ```\n",
    "3. **Open the UI** at http://localhost:5000 (or via VS Code port forwarding)\n",
    "\n",
    "### What MLflow Provides\n",
    "- **Tracing**: See every LM call, inputs, outputs, latency\n",
    "- **Evaluation**: Compare model versions side-by-side\n",
    "- **Human Feedback**: Add labels/assessments directly in the UI\n",
    "- **Experiment Tracking**: Track metrics across optimization runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87efb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MLflow Setup\n",
    "# =============================================================================\n",
    "# Prerequisites:\n",
    "#   1. Install: pip install \"mlflow>=3.0\"\n",
    "#   2. Start server in terminal (or run: ./scripts/start_mlflow.sh):\n",
    "#      mlflow server --backend-store-uri sqlite:///mlflow.sqlite \\\n",
    "#                    --default-artifact-root ./mlflow-artifacts \\\n",
    "#                    --host 0.0.0.0 --port 5000\n",
    "#   3. Open UI: http://localhost:5000\n",
    "# =============================================================================\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Configure MLflow - use localhost for local server\n",
    "# For Docker: use host.docker.internal if MLflow runs on host\n",
    "MLFLOW_TRACKING_URI = \"http://127.0.0.1:5000\"\n",
    "\n",
    "try:\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(\"wiki3-kg-stage1-statements\")\n",
    "    \n",
    "    # Enable automatic DSPy tracing - captures all LM calls, modules, predictions\n",
    "    mlflow.dspy.autolog()\n",
    "    \n",
    "    print(f\"‚úÖ MLflow configured successfully\")\n",
    "    print(f\"   Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "    print(f\"   Experiment: wiki3-kg-stage1-statements\")\n",
    "    print(f\"\\nüìä Open MLflow UI: {MLFLOW_TRACKING_URI}\")\n",
    "    MLFLOW_ENABLED = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  MLflow not available: {e}\")\n",
    "    print(f\"\\nüí° To enable MLflow, start the server:\")\n",
    "    print(f\"   mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\")\n",
    "    MLFLOW_ENABLED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with MLflow tracing (if enabled)\n",
    "# Each prediction creates a trace viewable in the MLflow UI\n",
    "\n",
    "if MLFLOW_ENABLED:\n",
    "    with mlflow.start_run(run_name=\"baseline_evaluation\"):\n",
    "        # Log parameters for reproducibility\n",
    "        mlflow.log_param(\"eval_size\", EVAL_SIZE)\n",
    "        mlflow.log_param(\"model\", \"qwen/qwen3-coder-30b\")\n",
    "        mlflow.log_param(\"num_fewshot\", NUM_FEWSHOT)\n",
    "        \n",
    "        # Run extractions on dev set - each one is traced\n",
    "        results = []\n",
    "        for i, ex in enumerate(tqdm(devset[:EVAL_SIZE], desc=\"Evaluating\")):\n",
    "            with mlflow.start_span(name=f\"example_{i}\") as span:\n",
    "                # Run extraction\n",
    "                pred = baseline_extractor(\n",
    "                    chunk_text=ex.chunk_text,\n",
    "                    section_context=ex.section_context,\n",
    "                )\n",
    "                \n",
    "                # Run judge\n",
    "                eval_result = judge(\n",
    "                    chunk_text=ex.chunk_text,\n",
    "                    section_context=ex.section_context,\n",
    "                    statements=pred.statements,\n",
    "                )\n",
    "                \n",
    "                # Log to span for MLflow UI review\n",
    "                span.set_inputs({\n",
    "                    \"chunk_text\": ex.chunk_text[:500],\n",
    "                    \"section_context\": ex.section_context,\n",
    "                })\n",
    "                span.set_outputs({\n",
    "                    \"statements\": list(pred.statements),\n",
    "                    \"completeness\": float(eval_result.completeness),\n",
    "                    \"atomicity\": float(eval_result.atomicity),\n",
    "                    \"accuracy\": float(eval_result.accuracy),\n",
    "                    \"link_preservation\": float(eval_result.link_preservation),\n",
    "                    \"weighted_score\": float(eval_result.weighted_score),\n",
    "                    \"reasoning\": eval_result.reasoning,\n",
    "                })\n",
    "                \n",
    "                results.append({\n",
    "                    \"index\": i,\n",
    "                    \"score\": float(eval_result.weighted_score),\n",
    "                })\n",
    "        \n",
    "        # Log aggregate metrics\n",
    "        avg_score = sum(r[\"score\"] for r in results) / len(results)\n",
    "        mlflow.log_metric(\"avg_quality_score\", avg_score)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Evaluation complete!\")\n",
    "        print(f\"   Average score: {avg_score:.2f}\")\n",
    "        print(f\"   Traces logged: {len(results)}\")\n",
    "        print(f\"\\nüìä Review in MLflow UI: {MLFLOW_TRACKING_URI}\")\n",
    "        print(f\"   ‚Üí Click 'Traces' tab to see all predictions\")\n",
    "        print(f\"   ‚Üí Click individual traces to review inputs/outputs\")\n",
    "        print(f\"   ‚Üí Use 'Feedback' to add human labels\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping MLflow evaluation (server not running)\")\n",
    "    print(\"   Run baseline evaluation with dspy.Evaluate instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use dspy.inspect_history() for quick debugging\n",
    "# This shows recent LM calls without needing MLflow server\n",
    "\n",
    "print(\"Recent LM calls (use MLflow UI for full traces):\")\n",
    "print(\"=\" * 60)\n",
    "dspy.inspect_history(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d708ef8",
   "metadata": {},
   "source": [
    "### MLflow Evaluation with Human Feedback\n",
    "\n",
    "Use MLflow's evaluation API to systematically review predictions and collect human labels.\n",
    "The MLflow UI provides a proper interface for reviewing and annotating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de353ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation dataset for MLflow\n",
    "import pandas as pd\n",
    "\n",
    "eval_data = []\n",
    "for i, ex in enumerate(devset[:EVAL_SIZE]):\n",
    "    eval_data.append({\n",
    "        \"index\": i,\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"section_context\": ex.section_context,\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "print(f\"Created evaluation dataset with {len(eval_df)} examples\")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ec641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Per-Statement Classification for ALL Evaluation Examples\n",
    "# ============================================================================\n",
    "# Uses StatementClassifier to get GOOD/BAD verdicts per statement\n",
    "\n",
    "from ontological_engineer import StatementClassifier, StatementClassification\n",
    "\n",
    "classifier = StatementClassifier()\n",
    "\n",
    "# Store all results for summary\n",
    "all_classification_results = []\n",
    "\n",
    "print(f\"Classifying statements for {EVAL_SIZE} chunks...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx, ex in enumerate(devset[:EVAL_SIZE]):\n",
    "    print(f\"\\n[{idx+1}/{EVAL_SIZE}] {ex.section_context[:50]}...\")\n",
    "    \n",
    "    # Extract statements\n",
    "    pred = baseline_extractor(\n",
    "        chunk_text=ex.chunk_text,\n",
    "        section_context=ex.section_context,\n",
    "    )\n",
    "    \n",
    "    # Classify each statement\n",
    "    result = classifier(\n",
    "        chunk_text=ex.chunk_text,\n",
    "        section_context=ex.section_context,\n",
    "        statements=list(pred.statements),\n",
    "    )\n",
    "    \n",
    "    # Store result\n",
    "    all_classification_results.append({\n",
    "        \"idx\": idx,\n",
    "        \"section\": ex.section_context,\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"statements\": list(pred.statements),\n",
    "        \"score\": result.score,\n",
    "        \"classifications\": result.classifications,\n",
    "        \"missing_facts\": result.missing_facts,\n",
    "    })\n",
    "    \n",
    "    # Show quick summary\n",
    "    good = sum(1 for c in result.classifications if c.is_good)\n",
    "    total = len(result.classifications)\n",
    "    print(f\"   ‚Üí {good}/{total} GOOD ({result.score:.0%})\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "avg_score = sum(r[\"score\"] for r in all_classification_results) / len(all_classification_results)\n",
    "total_good = sum(sum(1 for c in r[\"classifications\"] if c.is_good) for r in all_classification_results)\n",
    "total_bad = sum(sum(1 for c in r[\"classifications\"] if not c.is_good) for r in all_classification_results)\n",
    "print(f\"Average score: {avg_score:.1%}\")\n",
    "print(f\"Total GOOD: {total_good}, Total BAD: {total_bad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results for each chunk\n",
    "print(\"DETAILED PER-CHUNK RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for r in all_classification_results:\n",
    "    good = sum(1 for c in r[\"classifications\"] if c.is_good)\n",
    "    bad = sum(1 for c in r[\"classifications\"] if not c.is_good)\n",
    "    total = len(r[\"classifications\"])\n",
    "    \n",
    "    print(f\"\\nüìÑ Chunk {r['idx']}: {r['section'][:60]}...\")\n",
    "    print(f\"   Score: {r['score']:.0%} ({good}/{total} GOOD)\")\n",
    "    \n",
    "    # Show BAD statements (these need attention)\n",
    "    bad_stmts = [c for c in r[\"classifications\"] if not c.is_good]\n",
    "    if bad_stmts:\n",
    "        print(f\"   ‚ùå BAD statements:\")\n",
    "        for c in bad_stmts:\n",
    "            print(f\"      [{c.index}] {c.statement[:80]}...\")\n",
    "            print(f\"          Reason: {c.reason}\")\n",
    "    \n",
    "    if r[\"missing_facts\"] and r[\"missing_facts\"].lower() != \"none\":\n",
    "        print(f\"   üìù Missing: {r['missing_facts'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c283f",
   "metadata": {},
   "source": [
    "### Export Annotations from MLflow\n",
    "\n",
    "After reviewing and labeling in the MLflow UI, export your annotations for judge improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from MLflow (after you've labeled them in the UI)\n",
    "# MLflow stores feedback as assessments on traces\n",
    "\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "# Get the latest evaluation run\n",
    "experiment = client.get_experiment_by_name(\"wiki3-kg-stage1-statements\")\n",
    "if experiment:\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"start_time DESC\"],\n",
    "        max_results=1,\n",
    "    )\n",
    "    \n",
    "    if runs:\n",
    "        latest_run = runs[0]\n",
    "        print(f\"Latest run: {latest_run.info.run_id}\")\n",
    "        print(f\"Metrics: {latest_run.data.metrics}\")\n",
    "        \n",
    "        # Get traces with assessments (human feedback)\n",
    "        try:\n",
    "            traces = client.search_traces(\n",
    "                experiment_ids=[experiment.experiment_id],\n",
    "                max_results=100,\n",
    "            )\n",
    "            print(f\"Found {len(traces)} traces\")\n",
    "        except Exception as e:\n",
    "            print(f\"Trace search error: {e}\")\n",
    "else:\n",
    "    print(\"No experiment found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52239b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use human feedback to improve the judge\n",
    "# After collecting labels in MLflow, create DSPy training examples\n",
    "\n",
    "# For now, save the evaluation data for later use\n",
    "output_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "eval_df.to_json(output_dir / \"eval_dataset.json\", orient=\"records\", indent=2)\n",
    "print(f\"Saved evaluation dataset to {output_dir / 'eval_dataset.json'}\")\n",
    "\n",
    "print(\"\"\"\n",
    "üìã Next steps for human feedback:\n",
    "\n",
    "1. Start MLflow server:\n",
    "   mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\n",
    "\n",
    "2. Open MLflow UI at http://127.0.0.1:5000\n",
    "\n",
    "3. Navigate to the experiment 'wiki3-kg-stage1-statements'\n",
    "\n",
    "4. Click on traces to review predictions\n",
    "\n",
    "5. Use the feedback/assessment features to label quality\n",
    "\n",
    "6. Export labeled data for judge improvement\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b175b",
   "metadata": {},
   "source": [
    "## 10. MIPROv2 Prompt Optimization\n",
    "\n",
    "Use DSPy's MIPROv2 optimizer to improve the extractor's prompts.\n",
    "This uses the few-shot examples to bootstrap better demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f711918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "# Configure optimizer\n",
    "optimizer = MIPROv2(\n",
    "    metric=statement_quality_metric,\n",
    "    num_candidates=3,  # Number of prompt candidates to try\n",
    "    init_temperature=0.7,\n",
    ")\n",
    "\n",
    "# Use few-shot examples for bootstrapping demonstrations\n",
    "# Use training set for optimization\n",
    "TRAIN_SIZE = min(20, len(trainset))  # Limit for speed\n",
    "\n",
    "print(f\"Optimizing with {TRAIN_SIZE} training examples...\")\n",
    "print(f\"Using {len(selected_fewshot)} few-shot demos for bootstrapping...\")\n",
    "\n",
    "optimized_extractor = optimizer.compile(\n",
    "    StatementExtractor(),\n",
    "    trainset=trainset[:TRAIN_SIZE],\n",
    "    num_batches=2,\n",
    "    max_bootstrapped_demos=NUM_FEWSHOT,\n",
    "    # Provide few-shot examples as initial demos\n",
    "    # demos=selected_fewshot,  # Uncomment if supported\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d59aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate optimized extractor\n",
    "optimized_result = evaluator(optimized_extractor)\n",
    "optimized_score = optimized_result.score if hasattr(optimized_result, 'score') else float(optimized_result)\n",
    "\n",
    "print(f\"Baseline score:  {baseline_score:.2f}\")\n",
    "print(f\"Optimized score: {optimized_score:.2f}\")\n",
    "print(f\"Improvement:     {optimized_score - baseline_score:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fab06",
   "metadata": {},
   "source": [
    "## 11. Inspect Optimized Prompts\n",
    "\n",
    "See what prompts MIPROv2 discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a08e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the optimized module\n",
    "print(\"Optimized extractor configuration:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to access the optimized signature/demos\n",
    "if hasattr(optimized_extractor, 'demos'):\n",
    "    print(f\"\\nDemonstrations: {len(optimized_extractor.demos)}\")\n",
    "    for i, demo in enumerate(optimized_extractor.demos[:2], 1):\n",
    "        print(f\"  Demo {i}: {demo.section_context[:50]}...\")\n",
    "\n",
    "# Check for any instruction changes\n",
    "if hasattr(optimized_extractor, 'signature'):\n",
    "    print(f\"\\nSignature: {optimized_extractor.signature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0c651",
   "metadata": {},
   "source": [
    "## 12. Save Results\n",
    "\n",
    "Save the optimized extractor and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e45006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training metadata\n",
    "output_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save training results\n",
    "results = {\n",
    "    \"baseline_score\": baseline_score,\n",
    "    \"optimized_score\": optimized_score,\n",
    "    \"train_size\": TRAIN_SIZE,\n",
    "    \"eval_size\": EVAL_SIZE,\n",
    "    \"num_fewshot\": NUM_FEWSHOT,\n",
    "    \"pages_processed\": pages_processed,\n",
    "    \"total_chunks\": len(training_chunks),\n",
    "}\n",
    "\n",
    "with open(output_dir / \"stage1_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Saved results to {output_dir / 'stage1_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized extractor state\n",
    "try:\n",
    "    optimized_extractor.save(output_dir / \"optimized_extractor\")\n",
    "    print(f\"Saved optimized extractor to {output_dir / 'optimized_extractor'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save extractor state: {e}\")\n",
    "    # Alternative: save as JSON\n",
    "    if hasattr(optimized_extractor, 'dump_state'):\n",
    "        state = optimized_extractor.dump_state()\n",
    "        with open(output_dir / \"optimized_extractor_state.json\", \"w\") as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "        print(\"Saved extractor state as JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec590ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save few-shot examples for reference\n",
    "fewshot_data = []\n",
    "for ex in selected_fewshot:\n",
    "    fewshot_data.append({\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"section_context\": ex.section_context,\n",
    "        \"statements\": list(ex.statements),\n",
    "    })\n",
    "\n",
    "with open(output_dir / \"fewshot_examples.json\", \"w\") as f:\n",
    "    json.dump(fewshot_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(fewshot_data)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e226c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. Loaded Albert Einstein as few-shot examples (seed/guidance)\n",
    "2. Fetched and chunked Wikipedia sample pages for training\n",
    "3. Established baseline extraction quality\n",
    "4. Ran MIPROv2 prompt optimization\n",
    "5. Saved the optimized extractor\n",
    "\n",
    "Next steps:\n",
    "- **Stage 2**: Schema matching with optimized statements\n",
    "- **Stage 3**: RDF generation training\n",
    "- **Arbor GRPO**: Fine-tune the full pipeline end-to-end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
