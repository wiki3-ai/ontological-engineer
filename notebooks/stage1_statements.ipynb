{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6df303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.insert(0, '/workspaces/wiki3-kg-project')\n",
    "\n",
    "import dspy\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from pathlib import Path\n",
    "from random import shuffle, seed as random_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ontological_engineer import (\n",
    "    configure_lm,\n",
    "    StatementExtractor,\n",
    "    StatementQualityJudge,\n",
    ")\n",
    "from ontological_engineer.judges import statement_quality_metric\n",
    "from ontological_engineer.training.bootstrap import (\n",
    "    load_chunks_from_notebook,\n",
    "    load_facts_from_notebook,\n",
    "    create_training_examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f86eb6",
   "metadata": {},
   "source": [
    "## 1. Configure Language Model\n",
    "\n",
    "Connect to LM Studio running Qwen-30B (or your preferred model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e839f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured LM: <dspy.clients.lm.LM object at 0xffffa46693c0>\n"
     ]
    }
   ],
   "source": [
    "# Configure the LM (defaults to Qwen-30B via LM Studio)\n",
    "lm = configure_lm(\n",
    "    model=\"qwen/qwen3-coder-30b\",\n",
    "    api_base=\"http://host.docker.internal:1234/v1\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(f\"Configured LM: {lm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185bc4f7",
   "metadata": {},
   "source": [
    "## 2. Load Few-Shot Examples (Albert Einstein)\n",
    "\n",
    "Albert Einstein is our gold-standard example. These chunks and their extracted facts\n",
    "serve as few-shot demonstrations for the extractor and judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b715a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 63 chunks from Albert Einstein\n",
      "Loaded 19 fact sets\n",
      "Created 19 few-shot examples\n"
     ]
    }
   ],
   "source": [
    "# Load Albert Einstein data for few-shot examples\n",
    "fewshot_dir = Path(\"/workspaces/wiki3-kg-project/data/albert_einstein/20251218_231446\")\n",
    "\n",
    "fewshot_chunks = load_chunks_from_notebook(fewshot_dir / \"chunks.ipynb\")\n",
    "fewshot_facts = load_facts_from_notebook(fewshot_dir / \"facts.ipynb\")\n",
    "\n",
    "print(f\"Loaded {len(fewshot_chunks)} chunks from Albert Einstein\")\n",
    "print(f\"Loaded {len(fewshot_facts)} fact sets\")\n",
    "\n",
    "# Create few-shot examples\n",
    "fewshot_examples = create_training_examples(fewshot_chunks, fewshot_facts)\n",
    "print(f\"Created {len(fewshot_examples)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5911ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample few-shot example:\n",
      "  Context: Albert Einstein > Introduction\n",
      "  Text: Albert Einstein (14 March 1879 ‚Äì 18 April 1955) was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory...\n",
      "  Statements: 28 items\n",
      "    - Albert Einstein was a German-born theoretical physicist.\n",
      "    - Albert Einstein developed the theory of relativity.\n",
      "    - Albert Einstein made important contributions to quantum theory.\n"
     ]
    }
   ],
   "source": [
    "# Show a few-shot example\n",
    "if fewshot_examples:\n",
    "    ex = fewshot_examples[0]\n",
    "    print(\"Sample few-shot example:\")\n",
    "    print(f\"  Context: {ex.section_context}\")\n",
    "    print(f\"  Text: {ex.chunk_text[:200]}...\")\n",
    "    print(f\"  Statements: {len(ex.statements)} items\")\n",
    "    for stmt in ex.statements[:3]:\n",
    "        print(f\"    - {stmt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06944",
   "metadata": {},
   "source": [
    "## 3. Load Wikipedia Sample for Training\n",
    "\n",
    "Load the 100-page Wikipedia sample. We'll fetch article content and chunk it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9640f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 Wikipedia pages\n",
      "Sampling method: power_law\n",
      "\n",
      "First 10 pages:\n",
      "  - Zohran Mamdani (9,344,963 views)\n",
      "  - ChatGPT (3,639,485 views)\n",
      "  - James A. Garfield (3,524,531 views)\n",
      "  - 1989 Tiananmen Square protests and massacre (2,867,005 views)\n",
      "  - 2025 Bihar Legislative Assembly election (2,555,071 views)\n",
      "  - Mira Nair (2,503,516 views)\n",
      "  - Dick Cheney (2,186,840 views)\n",
      "  - 2026 FIFA World Cup (2,155,565 views)\n",
      "  - 1xBet (1,831,684 views)\n",
      "  - Survivor Series: WarGames (2025) (1,590,263 views)\n"
     ]
    }
   ],
   "source": [
    "# Load the Wikipedia sample\n",
    "sample_file = Path(\"/workspaces/wiki3-kg-project/data/training/wikipedia_sample.json\")\n",
    "\n",
    "with open(sample_file) as f:\n",
    "    wiki_sample = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(wiki_sample['pages'])} Wikipedia pages\")\n",
    "print(f\"Sampling method: {wiki_sample['metadata']['sampling_method']}\")\n",
    "print(f\"\\nFirst 10 pages:\")\n",
    "for p in wiki_sample['pages'][:10]:\n",
    "    print(f\"  - {p['title']} ({p['views']:,} views)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab17fa",
   "metadata": {},
   "source": [
    "## 4. Fetch and Chunk Wikipedia Pages\n",
    "\n",
    "For each page in our sample, fetch the content and break it into chunks.\n",
    "We use the same chunking strategy as the original pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be01db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 'Zohran Mamdani' -> 20 chunks\n",
      "First chunk: Zohran Kwame Mamdani (born October 18, 1991) is an American politician who is the mayor-elect of New York City. A member of the Democratic Party and the Democratic Socialists of America, he is set to ...\n"
     ]
    }
   ],
   "source": [
    "def fetch_wikipedia_content(title):\n",
    "    \"\"\"\n",
    "    Fetch the plain text content of a Wikipedia article.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"Wiki3.ai OntologicalEngineer/0.1\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "        for page_id, page_data in pages.items():\n",
    "            if page_id == \"-1\":\n",
    "                return None\n",
    "            return page_data.get(\"extract\", \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error fetching {title}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def chunk_article(title, text, max_chunk_size=1500):\n",
    "    \"\"\"\n",
    "    Split article into chunks by section.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # Split by section headers (== Header ==)\n",
    "    sections = re.split(r'\\n(==+\\s+.+?\\s+==+)\\n', text)\n",
    "    \n",
    "    current_section = title\n",
    "    current_text = \"\"\n",
    "    \n",
    "    for i, part in enumerate(sections):\n",
    "        # Check if this is a header\n",
    "        if re.match(r'==+\\s+.+?\\s+==+', part):\n",
    "            # Save previous chunk if it has content\n",
    "            if current_text.strip():\n",
    "                chunks.append({\n",
    "                    \"text\": current_text.strip(),\n",
    "                    \"section_context\": f\"{title} > {current_section}\",\n",
    "                })\n",
    "            \n",
    "            # Update section name\n",
    "            current_section = part.strip('= \\n')\n",
    "            current_text = \"\"\n",
    "        else:\n",
    "            current_text += part\n",
    "            \n",
    "            # If chunk gets too large, split it\n",
    "            while len(current_text) > max_chunk_size:\n",
    "                # Find a good break point (paragraph)\n",
    "                break_point = current_text.rfind('\\n\\n', 0, max_chunk_size)\n",
    "                if break_point == -1:\n",
    "                    break_point = current_text.rfind('. ', 0, max_chunk_size)\n",
    "                if break_point == -1:\n",
    "                    break_point = max_chunk_size\n",
    "                \n",
    "                chunk_text = current_text[:break_point].strip()\n",
    "                if chunk_text:\n",
    "                    chunks.append({\n",
    "                        \"text\": chunk_text,\n",
    "                        \"section_context\": f\"{title} > {current_section}\",\n",
    "                    })\n",
    "                current_text = current_text[break_point:].strip()\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_text.strip():\n",
    "        chunks.append({\n",
    "            \"text\": current_text.strip(),\n",
    "            \"section_context\": f\"{title} > {current_section}\",\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Test on one page\n",
    "test_title = wiki_sample['pages'][0]['title']\n",
    "test_content = fetch_wikipedia_content(test_title)\n",
    "if test_content:\n",
    "    test_chunks = chunk_article(test_title, test_content)\n",
    "    print(f\"Test: '{test_title}' -> {len(test_chunks)} chunks\")\n",
    "    if test_chunks:\n",
    "        print(f\"First chunk: {test_chunks[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886bea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content for 20 pages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:09<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 pages\n",
      "Total training chunks: 879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch and chunk all pages in sample (limit to first N for faster iteration)\n",
    "MAX_PAGES = 20  # Increase for full training run\n",
    "MIN_CHUNK_LENGTH = 100  # Skip very short chunks\n",
    "\n",
    "training_chunks = []\n",
    "pages_processed = 0\n",
    "\n",
    "print(f\"Fetching content for {min(MAX_PAGES, len(wiki_sample['pages']))} pages...\")\n",
    "\n",
    "for page in tqdm(wiki_sample['pages'][:MAX_PAGES]):\n",
    "    content = fetch_wikipedia_content(page['title'])\n",
    "    if content:\n",
    "        chunks = chunk_article(page['title'], content)\n",
    "        # Filter short chunks\n",
    "        chunks = [c for c in chunks if len(c['text']) >= MIN_CHUNK_LENGTH]\n",
    "        training_chunks.extend(chunks)\n",
    "        pages_processed += 1\n",
    "\n",
    "print(f\"\\nProcessed {pages_processed} pages\")\n",
    "print(f\"Total training chunks: {len(training_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf50dd2",
   "metadata": {},
   "source": [
    "## 5. Initialize Extractor with Few-Shot Examples\n",
    "\n",
    "Create the statement extractor and provide Albert Einstein examples as demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e38b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 few-shot examples:\n",
      "  1. Albert Einstein > Introduction... (28 statements)\n",
      "  2. Albert Einstein > Life and career > Personal views... (28 statements)\n",
      "  3. Albert Einstein > Introduction... (27 statements)\n"
     ]
    }
   ],
   "source": [
    "# Select best few-shot examples (ones with good variety of statements)\n",
    "NUM_FEWSHOT = 3\n",
    "\n",
    "# Sort by statement count to get diverse examples\n",
    "sorted_fewshot = sorted(fewshot_examples, key=lambda x: len(x.statements), reverse=True)\n",
    "selected_fewshot = sorted_fewshot[:NUM_FEWSHOT]\n",
    "\n",
    "print(f\"Selected {len(selected_fewshot)} few-shot examples:\")\n",
    "for i, ex in enumerate(selected_fewshot, 1):\n",
    "    print(f\"  {i}. {ex.section_context[:50]}... ({len(ex.statements)} statements)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9896eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor initialized\n",
      "Few-shot examples available: 3\n"
     ]
    }
   ],
   "source": [
    "# Create extractor with few-shot demonstrations\n",
    "extractor = StatementExtractor()\n",
    "\n",
    "# In DSPy, we can provide demonstrations directly\n",
    "# The few-shot examples will be used by MIPROv2 for bootstrapping\n",
    "print(\"Extractor initialized\")\n",
    "print(f\"Few-shot examples available: {len(selected_fewshot)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4419625",
   "metadata": {},
   "source": [
    "## 6. Test Extraction on Training Sample\n",
    "\n",
    "Run the extractor on a few training chunks to verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ea088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on: Zohran Mamdani > Zohran Mamdani\n",
      "Text: Zohran Kwame Mamdani (born October 18, 1991) is an American politician who is the mayor-elect of New York City. A member of the Democratic Party and the Democratic Socialists of America, he is set to become New York's first Muslim and Asian American mayor. Mamdani has served as a member of the New Y...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Extracted 22 statements:\n",
      "  1. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) was born on October 18, 1991.\n",
      "  2. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is an American politician.\n",
      "  3. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is the mayor-elect of [New York City](/wiki/New_York_City).\n",
      "  4. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is a member of the [Democratic Party](/wiki/Democratic_Party).\n",
      "  5. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is a member of the [Democratic Socialists of America](/wiki/Democratic_Socialists_of_America).\n",
      "  6. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is set to become [New York City](/wiki/New_York_City)'s first Muslim mayor.\n",
      "  7. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is set to become [New York City](/wiki/New_York_City)'s first Asian American mayor.\n",
      "  8. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) has served as a member of the [New York State Assembly](/wiki/New_York_State_Assembly) for the 36th district since 2021.\n",
      "  9. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) represents the [Queens](/wiki/Queens) neighborhood of [Astoria](/wiki/Astoria,_New_York).\n",
      "  10. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) was born in [Kampala](/wiki/Kampala), [Uganda](/wiki/Uganda).\n",
      "  ... and 12 more\n"
     ]
    }
   ],
   "source": [
    "# Test on a training chunk\n",
    "if training_chunks:\n",
    "    test_chunk = training_chunks[0]\n",
    "    \n",
    "    print(f\"Testing on: {test_chunk['section_context']}\")\n",
    "    print(f\"Text: {test_chunk['text'][:300]}...\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    result = extractor(\n",
    "        chunk_text=test_chunk['text'],\n",
    "        section_context=test_chunk['section_context'],\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracted {len(result.statements)} statements:\")\n",
    "    for i, stmt in enumerate(result.statements[:10], 1):\n",
    "        print(f\"  {i}. {stmt}\")\n",
    "    if len(result.statements) > 10:\n",
    "        print(f\"  ... and {len(result.statements) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a247a7a",
   "metadata": {},
   "source": [
    "## 7. Create Training Dataset\n",
    "\n",
    "Convert chunks into DSPy examples. For training, we need to generate initial extractions\n",
    "that can be scored and optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fab9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 703 examples\n",
      "Dev set: 176 examples\n"
     ]
    }
   ],
   "source": [
    "# Create training examples (without labels - we'll generate and judge them)\n",
    "# For DSPy optimization, we just need the inputs\n",
    "\n",
    "random_seed(42)  # For reproducibility\n",
    "shuffle(training_chunks)\n",
    "\n",
    "# Create DSPy examples from chunks\n",
    "trainset = []\n",
    "for chunk in training_chunks:\n",
    "    ex = dspy.Example(\n",
    "        chunk_text=chunk['text'],\n",
    "        section_context=chunk['section_context'],\n",
    "    ).with_inputs('chunk_text', 'section_context')\n",
    "    trainset.append(ex)\n",
    "\n",
    "# Split into train/dev\n",
    "split_idx = int(len(trainset) * 0.8)\n",
    "devset = trainset[split_idx:]\n",
    "trainset = trainset[:split_idx]\n",
    "\n",
    "print(f\"Training set: {len(trainset)} examples\")\n",
    "print(f\"Dev set: {len(devset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda75e24",
   "metadata": {},
   "source": [
    "## 8. Initialize Judge with Few-Shot Guidance\n",
    "\n",
    "The judge scores extraction quality. We use Albert Einstein examples to calibrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "792d6e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge calibration on few-shot example:\n",
      "  Completeness:      0.95\n",
      "  Atomicity:         0.85\n",
      "  Accuracy:          0.95\n",
      "  Link preservation: 1.00\n",
      "  ---\n",
      "  Weighted score:    0.94\n"
     ]
    }
   ],
   "source": [
    "# Initialize judge\n",
    "judge = StatementQualityJudge()\n",
    "\n",
    "# Test judge on a known good example (Albert Einstein few-shot)\n",
    "if selected_fewshot:\n",
    "    test_ex = selected_fewshot[0]\n",
    "    \n",
    "    evaluation = judge(\n",
    "        chunk_text=test_ex.chunk_text,\n",
    "        section_context=test_ex.section_context,\n",
    "        statements=test_ex.statements,\n",
    "    )\n",
    "    \n",
    "    print(\"Judge calibration on few-shot example:\")\n",
    "    print(f\"  Completeness:      {evaluation.completeness:.2f}\")\n",
    "    print(f\"  Atomicity:         {evaluation.atomicity:.2f}\")\n",
    "    print(f\"  Accuracy:          {evaluation.accuracy:.2f}\")\n",
    "    print(f\"  Link preservation: {evaluation.link_preservation:.2f}\")\n",
    "    print(f\"  ---\")\n",
    "    print(f\"  Weighted score:    {evaluation.weighted_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649d2fc",
   "metadata": {},
   "source": [
    "## 9. Baseline Evaluation\n",
    "\n",
    "Evaluate the unoptimized extractor on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f230a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.72 / 10 (87.2%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:55<00:00, 11.51s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 01:29:59 INFO dspy.evaluate.evaluate: Average Metric: 8.724999999999998 / 10 (87.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Baseline quality score: 87.25\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline on dev set\n",
    "EVAL_SIZE = min(10, len(devset))  # Limit for speed\n",
    "\n",
    "evaluator = dspy.Evaluate(\n",
    "    devset=devset[:EVAL_SIZE],\n",
    "    metric=statement_quality_metric,\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    ")\n",
    "\n",
    "baseline_extractor = StatementExtractor()\n",
    "baseline_result = evaluator(baseline_extractor)\n",
    "\n",
    "baseline_score = baseline_result.score if hasattr(baseline_result, 'score') else float(baseline_result)\n",
    "print(f\"\\nBaseline quality score: {baseline_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e36a0",
   "metadata": {},
   "source": [
    "## 9b. Interactive Judgment Review\n",
    "\n",
    "Review the judge's outputs and provide corrections. Your feedback will be saved\n",
    "as labeled examples to improve the judge via DSPy's few-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87efb9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running extraction + judgment on dev set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 44.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ready to review 10 examples (sorted by score, worst first)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run detailed evaluation and capture individual results\n",
    "detailed_results = []\n",
    "\n",
    "print(\"Running extraction + judgment on dev set...\")\n",
    "for i, ex in enumerate(tqdm(devset[:EVAL_SIZE])):\n",
    "    # Run extraction\n",
    "    pred = baseline_extractor(\n",
    "        chunk_text=ex.chunk_text,\n",
    "        section_context=ex.section_context,\n",
    "    )\n",
    "    \n",
    "    # Run judge\n",
    "    eval_result = judge(\n",
    "        chunk_text=ex.chunk_text,\n",
    "        section_context=ex.section_context,\n",
    "        statements=pred.statements,\n",
    "    )\n",
    "    \n",
    "    detailed_results.append({\n",
    "        \"index\": i,\n",
    "        \"section_context\": ex.section_context,\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"statements\": list(pred.statements),\n",
    "        \"completeness\": float(eval_result.completeness),\n",
    "        \"atomicity\": float(eval_result.atomicity),\n",
    "        \"accuracy\": float(eval_result.accuracy),\n",
    "        \"link_preservation\": float(eval_result.link_preservation),\n",
    "        \"weighted_score\": float(eval_result.weighted_score),\n",
    "        \"reasoning\": eval_result.reasoning,\n",
    "        # Human annotations (to be filled)\n",
    "        \"human_approved\": None,\n",
    "        \"human_scores\": None,\n",
    "        \"human_notes\": None,\n",
    "    })\n",
    "\n",
    "# Sort by score (worst first for review)\n",
    "detailed_results = sorted(detailed_results, key=lambda x: x[\"weighted_score\"])\n",
    "print(f\"\\nReady to review {len(detailed_results)} examples (sorted by score, worst first)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive review widget\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Current review state\n",
    "review_state = {\"current_idx\": 0}\n",
    "\n",
    "def display_example(idx):\n",
    "    \"\"\"Display an example for review.\"\"\"\n",
    "    r = detailed_results[idx]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Example {r['index']} ({idx + 1}/{len(detailed_results)}) | Judge Score: {r['weighted_score']:.1f}\")\n",
    "    print(f\"Context: {r['section_context']}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nüìÑ CHUNK TEXT:\\n{'-' * 40}\")\n",
    "    print(r['chunk_text'][:800])\n",
    "    if len(r['chunk_text']) > 800:\n",
    "        print(f\"... ({len(r['chunk_text']) - 800} more chars)\")\n",
    "    \n",
    "    print(f\"\\nüìù EXTRACTED STATEMENTS ({len(r['statements'])}):\\n{'-' * 40}\")\n",
    "    for j, stmt in enumerate(r['statements'], 1):\n",
    "        print(f\"  {j}. {stmt}\")\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è JUDGE SCORES:\\n{'-' * 40}\")\n",
    "    print(f\"  Completeness:      {r['completeness']:.0f}\")\n",
    "    print(f\"  Atomicity:         {r['atomicity']:.0f}\")\n",
    "    print(f\"  Accuracy:          {r['accuracy']:.0f}\")\n",
    "    print(f\"  Link preservation: {r['link_preservation']:.0f}\")\n",
    "    print(f\"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "    print(f\"  Weighted Score:    {r['weighted_score']:.1f}\")\n",
    "    \n",
    "    print(f\"\\nüí≠ JUDGE REASONING:\\n{'-' * 40}\")\n",
    "    print(r['reasoning'])\n",
    "    \n",
    "    if r['human_approved'] is not None:\n",
    "        status = \"‚úÖ APPROVED\" if r['human_approved'] else \"‚ùå CORRECTED\"\n",
    "        print(f\"\\nüè∑Ô∏è Your verdict: {status}\")\n",
    "        if r['human_notes']:\n",
    "            print(f\"   Notes: {r['human_notes']}\")\n",
    "\n",
    "display_example(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "794a21f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review functions loaded. Use:\n",
      "  approve()           - Accept judge's assessment\n",
      "  approve('notes')    - Accept with notes\n",
      "  correct(accuracy=95, notes='...')  - Override specific scores\n",
      "  next_example()      - Skip to next\n",
      "  prev_example()      - Go back\n",
      "  goto(5)             - Jump to example 5\n",
      "  show_progress()     - See review stats\n"
     ]
    }
   ],
   "source": [
    "# Annotation functions - run these cells to provide feedback\n",
    "\n",
    "def approve(notes=\"\"):\n",
    "    \"\"\"Approve the judge's assessment for the current example.\"\"\"\n",
    "    idx = review_state[\"current_idx\"]\n",
    "    detailed_results[idx][\"human_approved\"] = True\n",
    "    detailed_results[idx][\"human_notes\"] = notes\n",
    "    print(f\"‚úÖ Approved example {idx}\")\n",
    "    next_example()\n",
    "\n",
    "def correct(completeness=None, atomicity=None, accuracy=None, \n",
    "            link_preservation=None, notes=\"\"):\n",
    "    \"\"\"Override the judge's scores with your own assessment.\"\"\"\n",
    "    idx = review_state[\"current_idx\"]\n",
    "    r = detailed_results[idx]\n",
    "    \n",
    "    # Use original scores where not overridden\n",
    "    r[\"human_approved\"] = False\n",
    "    r[\"human_scores\"] = {\n",
    "        \"completeness\": completeness if completeness is not None else r[\"completeness\"],\n",
    "        \"atomicity\": atomicity if atomicity is not None else r[\"atomicity\"],\n",
    "        \"accuracy\": accuracy if accuracy is not None else r[\"accuracy\"],\n",
    "        \"link_preservation\": link_preservation if link_preservation is not None else r[\"link_preservation\"],\n",
    "    }\n",
    "    r[\"human_notes\"] = notes\n",
    "    \n",
    "    print(f\"‚ùå Corrected example {idx}\")\n",
    "    print(f\"   Original: {r['weighted_score']:.0f}\")\n",
    "    hs = r[\"human_scores\"]\n",
    "    human_weighted = 0.3*hs[\"completeness\"] + 0.25*hs[\"atomicity\"] + 0.3*hs[\"accuracy\"] + 0.15*hs[\"link_preservation\"]\n",
    "    print(f\"   Corrected: {human_weighted:.0f}\")\n",
    "    next_example()\n",
    "\n",
    "def next_example():\n",
    "    \"\"\"Move to the next example.\"\"\"\n",
    "    review_state[\"current_idx\"] = min(review_state[\"current_idx\"] + 1, len(detailed_results) - 1)\n",
    "    clear_output(wait=True)\n",
    "    display_example(review_state[\"current_idx\"])\n",
    "\n",
    "def prev_example():\n",
    "    \"\"\"Move to the previous example.\"\"\"\n",
    "    review_state[\"current_idx\"] = max(review_state[\"current_idx\"] - 1, 0)\n",
    "    clear_output(wait=True)\n",
    "    display_example(review_state[\"current_idx\"])\n",
    "\n",
    "def goto(idx):\n",
    "    \"\"\"Jump to a specific example.\"\"\"\n",
    "    review_state[\"current_idx\"] = max(0, min(idx, len(detailed_results) - 1))\n",
    "    clear_output(wait=True)\n",
    "    display_example(review_state[\"current_idx\"])\n",
    "\n",
    "def show_progress():\n",
    "    \"\"\"Show review progress.\"\"\"\n",
    "    approved = sum(1 for r in detailed_results if r[\"human_approved\"] == True)\n",
    "    corrected = sum(1 for r in detailed_results if r[\"human_approved\"] == False)\n",
    "    pending = sum(1 for r in detailed_results if r[\"human_approved\"] is None)\n",
    "    print(f\"Progress: ‚úÖ {approved} approved, ‚ùå {corrected} corrected, ‚è≥ {pending} pending\")\n",
    "\n",
    "print(\"Review functions loaded. Use:\")\n",
    "print(\"  approve()           - Accept judge's assessment\")\n",
    "print(\"  approve('notes')    - Accept with notes\")\n",
    "print(\"  correct(accuracy=95, notes='...')  - Override specific scores\")\n",
    "print(\"  next_example()      - Skip to next\")\n",
    "print(\"  prev_example()      - Go back\")\n",
    "print(\"  goto(5)             - Jump to example 5\")\n",
    "print(\"  show_progress()     - See review stats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d708ef8",
   "metadata": {},
   "source": [
    "### Annotate Current Example\n",
    "\n",
    "Run one of these cells to provide your feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de353ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROVE: Judge got it right\n",
    "approve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ec641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECT: Override judge's scores (edit values as needed)\n",
    "correct(\n",
    "    completeness=90,   # Your score (0-100)\n",
    "    atomicity=85,      # Your score (0-100)  \n",
    "    accuracy=95,       # Your score (0-100)\n",
    "    link_preservation=80,  # Your score (0-100)\n",
    "    notes=\"Judge was too harsh on X because Y\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP: Move to next without annotating\n",
    "next_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c283f",
   "metadata": {},
   "source": [
    "### Save Annotations as DSPy Training Data\n",
    "\n",
    "After reviewing, save your corrections as labeled examples for the judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show review progress\n",
    "show_progress()\n",
    "\n",
    "# Save annotations as DSPy Examples for judge improvement\n",
    "judge_training_examples = []\n",
    "\n",
    "for r in detailed_results:\n",
    "    if r[\"human_approved\"] is None:\n",
    "        continue  # Skip unannotated\n",
    "    \n",
    "    # Use human scores if corrected, otherwise use original (approved)\n",
    "    if r[\"human_approved\"]:\n",
    "        scores = {\n",
    "            \"completeness\": r[\"completeness\"],\n",
    "            \"atomicity\": r[\"atomicity\"],\n",
    "            \"accuracy\": r[\"accuracy\"],\n",
    "            \"link_preservation\": r[\"link_preservation\"],\n",
    "        }\n",
    "    else:\n",
    "        scores = r[\"human_scores\"]\n",
    "    \n",
    "    # Create DSPy Example with inputs and outputs\n",
    "    ex = dspy.Example(\n",
    "        # Inputs\n",
    "        chunk_text=r[\"chunk_text\"],\n",
    "        section_context=r[\"section_context\"],\n",
    "        statements=r[\"statements\"],\n",
    "        # Outputs (what judge should produce)\n",
    "        completeness=scores[\"completeness\"],\n",
    "        atomicity=scores[\"atomicity\"],\n",
    "        accuracy=scores[\"accuracy\"],\n",
    "        link_preservation=scores[\"link_preservation\"],\n",
    "    ).with_inputs(\"chunk_text\", \"section_context\", \"statements\")\n",
    "    \n",
    "    judge_training_examples.append(ex)\n",
    "\n",
    "print(f\"\\nCreated {len(judge_training_examples)} training examples for judge improvement\")\n",
    "\n",
    "# Save to file\n",
    "output_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "annotations_file = output_dir / \"judge_annotations.json\"\n",
    "annotations_data = []\n",
    "for r in detailed_results:\n",
    "    if r[\"human_approved\"] is not None:\n",
    "        annotations_data.append({\n",
    "            \"chunk_text\": r[\"chunk_text\"],\n",
    "            \"section_context\": r[\"section_context\"],\n",
    "            \"statements\": r[\"statements\"],\n",
    "            \"judge_scores\": {\n",
    "                \"completeness\": r[\"completeness\"],\n",
    "                \"atomicity\": r[\"atomicity\"],\n",
    "                \"accuracy\": r[\"accuracy\"],\n",
    "                \"link_preservation\": r[\"link_preservation\"],\n",
    "            },\n",
    "            \"judge_reasoning\": r[\"reasoning\"],\n",
    "            \"human_approved\": r[\"human_approved\"],\n",
    "            \"human_scores\": r[\"human_scores\"],\n",
    "            \"human_notes\": r[\"human_notes\"],\n",
    "        })\n",
    "\n",
    "with open(annotations_file, \"w\") as f:\n",
    "    json.dump(annotations_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved annotations to {annotations_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52239b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use annotations to improve the judge via few-shot learning\n",
    "# DSPy's LabeledFewShot uses your corrections as demonstrations\n",
    "\n",
    "from dspy.teleprompt import LabeledFewShot\n",
    "\n",
    "if judge_training_examples:\n",
    "    fewshot_optimizer = LabeledFewShot(k=min(3, len(judge_training_examples)))\n",
    "    \n",
    "    improved_judge = fewshot_optimizer.compile(\n",
    "        StatementQualityJudge(),\n",
    "        trainset=judge_training_examples,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Judge improved with your annotations!\")\n",
    "    print(f\"   Using {len(judge_training_examples)} labeled examples as few-shot demos\")\n",
    "    \n",
    "    # Save the improved judge\n",
    "    try:\n",
    "        improved_judge.save(output_dir / \"improved_judge\")\n",
    "        print(f\"   Saved to {output_dir / 'improved_judge'}\")\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"No annotations yet. Review some examples first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b175b",
   "metadata": {},
   "source": [
    "## 10. MIPROv2 Prompt Optimization\n",
    "\n",
    "Use DSPy's MIPROv2 optimizer to improve the extractor's prompts.\n",
    "This uses the few-shot examples to bootstrap better demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f711918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "# Configure optimizer\n",
    "optimizer = MIPROv2(\n",
    "    metric=statement_quality_metric,\n",
    "    num_candidates=3,  # Number of prompt candidates to try\n",
    "    init_temperature=0.7,\n",
    ")\n",
    "\n",
    "# Use few-shot examples for bootstrapping demonstrations\n",
    "# Use training set for optimization\n",
    "TRAIN_SIZE = min(20, len(trainset))  # Limit for speed\n",
    "\n",
    "print(f\"Optimizing with {TRAIN_SIZE} training examples...\")\n",
    "print(f\"Using {len(selected_fewshot)} few-shot demos for bootstrapping...\")\n",
    "\n",
    "optimized_extractor = optimizer.compile(\n",
    "    StatementExtractor(),\n",
    "    trainset=trainset[:TRAIN_SIZE],\n",
    "    num_batches=2,\n",
    "    max_bootstrapped_demos=NUM_FEWSHOT,\n",
    "    # Provide few-shot examples as initial demos\n",
    "    # demos=selected_fewshot,  # Uncomment if supported\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d59aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate optimized extractor\n",
    "optimized_result = evaluator(optimized_extractor)\n",
    "optimized_score = optimized_result.score if hasattr(optimized_result, 'score') else float(optimized_result)\n",
    "\n",
    "print(f\"Baseline score:  {baseline_score:.2f}\")\n",
    "print(f\"Optimized score: {optimized_score:.2f}\")\n",
    "print(f\"Improvement:     {optimized_score - baseline_score:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fab06",
   "metadata": {},
   "source": [
    "## 11. Inspect Optimized Prompts\n",
    "\n",
    "See what prompts MIPROv2 discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a08e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the optimized module\n",
    "print(\"Optimized extractor configuration:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to access the optimized signature/demos\n",
    "if hasattr(optimized_extractor, 'demos'):\n",
    "    print(f\"\\nDemonstrations: {len(optimized_extractor.demos)}\")\n",
    "    for i, demo in enumerate(optimized_extractor.demos[:2], 1):\n",
    "        print(f\"  Demo {i}: {demo.section_context[:50]}...\")\n",
    "\n",
    "# Check for any instruction changes\n",
    "if hasattr(optimized_extractor, 'signature'):\n",
    "    print(f\"\\nSignature: {optimized_extractor.signature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0c651",
   "metadata": {},
   "source": [
    "## 12. Save Results\n",
    "\n",
    "Save the optimized extractor and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e45006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training metadata\n",
    "output_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save training results\n",
    "results = {\n",
    "    \"baseline_score\": baseline_score,\n",
    "    \"optimized_score\": optimized_score,\n",
    "    \"train_size\": TRAIN_SIZE,\n",
    "    \"eval_size\": EVAL_SIZE,\n",
    "    \"num_fewshot\": NUM_FEWSHOT,\n",
    "    \"pages_processed\": pages_processed,\n",
    "    \"total_chunks\": len(training_chunks),\n",
    "}\n",
    "\n",
    "with open(output_dir / \"stage1_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Saved results to {output_dir / 'stage1_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized extractor state\n",
    "try:\n",
    "    optimized_extractor.save(output_dir / \"optimized_extractor\")\n",
    "    print(f\"Saved optimized extractor to {output_dir / 'optimized_extractor'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save extractor state: {e}\")\n",
    "    # Alternative: save as JSON\n",
    "    if hasattr(optimized_extractor, 'dump_state'):\n",
    "        state = optimized_extractor.dump_state()\n",
    "        with open(output_dir / \"optimized_extractor_state.json\", \"w\") as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "        print(\"Saved extractor state as JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec590ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save few-shot examples for reference\n",
    "fewshot_data = []\n",
    "for ex in selected_fewshot:\n",
    "    fewshot_data.append({\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"section_context\": ex.section_context,\n",
    "        \"statements\": list(ex.statements),\n",
    "    })\n",
    "\n",
    "with open(output_dir / \"fewshot_examples.json\", \"w\") as f:\n",
    "    json.dump(fewshot_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(fewshot_data)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e226c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. Loaded Albert Einstein as few-shot examples (seed/guidance)\n",
    "2. Fetched and chunked Wikipedia sample pages for training\n",
    "3. Established baseline extraction quality\n",
    "4. Ran MIPROv2 prompt optimization\n",
    "5. Saved the optimized extractor\n",
    "\n",
    "Next steps:\n",
    "- **Stage 2**: Schema matching with optimized statements\n",
    "- **Stage 3**: RDF generation training\n",
    "- **Arbor GRPO**: Fine-tune the full pipeline end-to-end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
