{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6df303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 1: Statement Extraction with DSPy\n",
    "# =============================================================================\n",
    "# This notebook orchestrates statement extraction training/evaluation.\n",
    "# All application logic is in ontological_engineer - this notebook coordinates.\n",
    "#\n",
    "# Outputs (with CID provenance):\n",
    "#   - data/training/chunks/*.ipynb - Chunked Wikipedia pages\n",
    "#   - data/training/statements.ipynb - Extracted statements\n",
    "#   - data/training/classifications.ipynb - Per-statement judgments\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/workspaces/wiki3-kg-project')\n",
    "\n",
    "import dspy\n",
    "import json\n",
    "from pathlib import Path\n",
    "from random import shuffle, seed as random_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ontological_engineer import (\n",
    "    # LM Configuration\n",
    "    configure_lm,\n",
    "    # DSPy Modules\n",
    "    StatementExtractor,\n",
    "    StatementQualityJudge,\n",
    "    StatementClassifier,\n",
    "    StatementClassification,\n",
    "    # Data Loading (from provenance-tracked notebooks)\n",
    "    WikipediaPage,\n",
    "    WikipediaChunk,\n",
    "    load_sample_from_notebook,\n",
    "    load_chunks_from_notebook,\n",
    "    # Processing (all logic in module!)\n",
    "    process_wikipedia_sample,\n",
    "    fetch_page_content,\n",
    "    chunk_article,\n",
    "    # Provenance notebook generation\n",
    "    save_notebook,\n",
    "    get_processed_chunk_cids,\n",
    ")\n",
    "from ontological_engineer.judges import statement_quality_metric\n",
    "from ontological_engineer.training.bootstrap import (\n",
    "    load_chunks_from_notebook as load_albert_chunks,\n",
    "    load_facts_from_notebook,\n",
    "    create_training_examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f86eb6",
   "metadata": {},
   "source": [
    "## 1. Configure Language Model\n",
    "\n",
    "Connect to LM Studio running Qwen-30B (or your preferred model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e839f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured LM: <dspy.clients.lm.LM object at 0xffff8d3debf0>\n"
     ]
    }
   ],
   "source": [
    "# Configure the LM (defaults to Qwen-30B via LM Studio)\n",
    "lm = configure_lm(\n",
    "    model=\"qwen/qwen3-coder-30b\",\n",
    "    api_base=\"http://host.docker.internal:1234/v1\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(f\"Configured LM: {lm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185bc4f7",
   "metadata": {},
   "source": [
    "## 2. Load Few-Shot Examples (Albert Einstein)\n",
    "\n",
    "Albert Einstein is our gold-standard example. These chunks and their extracted facts\n",
    "serve as few-shot demonstrations for the extractor and judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b715a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 63 chunks from Albert Einstein\n",
      "Loaded 19 fact sets\n",
      "Created 19 few-shot examples\n"
     ]
    }
   ],
   "source": [
    "# Load Albert Einstein data for few-shot examples\n",
    "fewshot_dir = Path(\"/workspaces/wiki3-kg-project/data/albert_einstein/20251218_231446\")\n",
    "\n",
    "# Use load_albert_chunks (from bootstrap module) - NOT load_chunks_from_notebook\n",
    "fewshot_chunks = load_albert_chunks(fewshot_dir / \"chunks.ipynb\")\n",
    "fewshot_facts = load_facts_from_notebook(fewshot_dir / \"facts.ipynb\")\n",
    "\n",
    "print(f\"Loaded {len(fewshot_chunks)} chunks from Albert Einstein\")\n",
    "print(f\"Loaded {len(fewshot_facts)} fact sets\")\n",
    "\n",
    "# Create few-shot examples\n",
    "fewshot_examples = create_training_examples(fewshot_chunks, fewshot_facts)\n",
    "print(f\"Created {len(fewshot_examples)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5911ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample few-shot example:\n",
      "  Context: Albert Einstein > Introduction\n",
      "  Text: Albert Einstein (14 March 1879 ‚Äì 18 April 1955) was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory...\n",
      "  Statements: 28 items\n",
      "    - Albert Einstein was a German-born theoretical physicist.\n",
      "    - Albert Einstein developed the theory of relativity.\n",
      "    - Albert Einstein made important contributions to quantum theory.\n"
     ]
    }
   ],
   "source": [
    "# Show a few-shot example\n",
    "if fewshot_examples:\n",
    "    ex = fewshot_examples[0]\n",
    "    print(\"Sample few-shot example:\")\n",
    "    print(f\"  Context: {ex.section_context}\")\n",
    "    print(f\"  Text: {ex.chunk_text[:200]}...\")\n",
    "    print(f\"  Statements: {len(ex.statements)} items\")\n",
    "    for stmt in ex.statements[:3]:\n",
    "        print(f\"    - {stmt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06944",
   "metadata": {},
   "source": [
    "## 3. Load Wikipedia Sample for Training\n",
    "\n",
    "Load the 100-page Wikipedia sample from the provenance-tracked notebook.\n",
    "If the notebook doesn't exist, fall back to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9640f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 100 pages from provenance-tracked notebook\n",
      "   Source: /workspaces/wiki3-kg-project/data/training/wikipedia_sample.ipynb\n",
      "\n",
      "First 10 pages:\n",
      "  - Zohran Mamdani (9,344,963 views)\n",
      "  - ChatGPT (3,639,485 views)\n",
      "  - James A. Garfield (3,524,531 views)\n",
      "  - 1989 Tiananmen Square protests and massacre (2,867,005 views)\n",
      "  - 2025 Bihar Legislative Assembly election (2,555,071 views)\n",
      "  - Mira Nair (2,503,516 views)\n",
      "  - Dick Cheney (2,186,840 views)\n",
      "  - 2026 FIFA World Cup (2,155,565 views)\n",
      "  - 1xBet (1,831,684 views)\n",
      "  - Survivor Series: WarGames (2025) (1,590,263 views)\n"
     ]
    }
   ],
   "source": [
    "# Load the Wikipedia sample (prefer provenance-tracked notebook)\n",
    "sample_notebook = Path(\"/workspaces/wiki3-kg-project/data/training/wikipedia_sample.ipynb\")\n",
    "sample_json = Path(\"/workspaces/wiki3-kg-project/data/training/wikipedia_sample.json\")\n",
    "\n",
    "if sample_notebook.exists():\n",
    "    # Load from provenance-tracked notebook\n",
    "    wiki_pages = load_sample_from_notebook(sample_notebook)\n",
    "    print(f\"‚úÖ Loaded {len(wiki_pages)} pages from provenance-tracked notebook\")\n",
    "    print(f\"   Source: {sample_notebook}\")\n",
    "elif sample_json.exists():\n",
    "    # Fall back to JSON format\n",
    "    with open(sample_json) as f:\n",
    "        wiki_sample = json.load(f)\n",
    "    wiki_pages = [WikipediaPage(title=p['title'], views=p['views']) for p in wiki_sample['pages']]\n",
    "    print(f\"‚ö†Ô∏è  Loaded {len(wiki_pages)} pages from JSON (no provenance)\")\n",
    "    print(f\"   Run sample_wikipedia_pages.ipynb to generate provenance-tracked version\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No Wikipedia sample found. Run sample_wikipedia_pages.ipynb first.\")\n",
    "\n",
    "print(f\"\\nFirst 10 pages:\")\n",
    "for p in wiki_pages[:10]:\n",
    "    print(f\"  - {p.title} ({p.views:,} views)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab17fa",
   "metadata": {},
   "source": [
    "## 4. Fetch and Chunk Wikipedia Pages (with CID Provenance)\n",
    "\n",
    "Fetch page content and chunk it. Each page's chunks are saved to a \n",
    "provenance-tracked notebook with CID signatures.\n",
    "\n",
    "**Note**: Uses `fetch_page_content` and `chunk_article` from `ontological_engineer` - \n",
    "no application logic defined in this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be01db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on: Zohran Mamdani\n",
      "  ‚Üí 20 chunks (filtered by min_length=60)\n",
      "  First chunk preview: Zohran Kwame Mamdani (born October 18, 1991) is an American politician who is the mayor-elect of New York City. A member of the Democratic Party and the Democratic Socialists of America, he is set to ...\n"
     ]
    }
   ],
   "source": [
    "# Processing parameters\n",
    "chunks_dir = Path(\"/workspaces/wiki3-kg-project/data/training/chunks\")\n",
    "MAX_PAGES = len(wiki_pages)\n",
    "MIN_CHUNK_LENGTH = 60  # Skip very short chunks\n",
    "\n",
    "# Quick test on one page first\n",
    "test_page = wiki_pages[0]\n",
    "print(f\"Testing on: {test_page.title}\")\n",
    "\n",
    "content = fetch_page_content(test_page.title)\n",
    "if content:\n",
    "    chunks = chunk_article(test_page.title, content)\n",
    "    chunks = [c for c in chunks if len(c.text) >= MIN_CHUNK_LENGTH]\n",
    "    print(f\"  ‚Üí {len(chunks)} chunks (filtered by min_length={MIN_CHUNK_LENGTH})\")\n",
    "    if chunks:\n",
    "        print(f\"  First chunk preview: {chunks[0].text[:200]}...\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Could not fetch content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886bea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 261.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Processed 99 pages\n",
      "   Total training chunks: 3428\n",
      "   Chunks saved to: /workspaces/wiki3-kg-project/data/training/chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all pages - logic is in process_wikipedia_sample()\n",
    "# Handles: fetching, chunking, saving with CID provenance, incremental processing\n",
    "\n",
    "training_chunks, pages_processed = process_wikipedia_sample(\n",
    "    pages=wiki_pages,\n",
    "    output_dir=chunks_dir,\n",
    "    max_pages=MAX_PAGES,\n",
    "    min_chunk_length=MIN_CHUNK_LENGTH,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {pages_processed} pages\")\n",
    "print(f\"   Total training chunks: {len(training_chunks)}\")\n",
    "print(f\"   Chunks saved to: {chunks_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf50dd2",
   "metadata": {},
   "source": [
    "## 5. Initialize Extractor with Few-Shot Examples\n",
    "\n",
    "Create the statement extractor and provide Albert Einstein examples as demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e38b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 few-shot examples:\n",
      "  1. Albert Einstein > Introduction... (28 statements)\n",
      "  2. Albert Einstein > Life and career > Personal views... (28 statements)\n",
      "  3. Albert Einstein > Introduction... (27 statements)\n"
     ]
    }
   ],
   "source": [
    "# Select best few-shot examples (ones with good variety of statements)\n",
    "NUM_FEWSHOT = 3\n",
    "\n",
    "# Sort by statement count to get diverse examples\n",
    "sorted_fewshot = sorted(fewshot_examples, key=lambda x: len(x.statements), reverse=True)\n",
    "selected_fewshot = sorted_fewshot[:NUM_FEWSHOT]\n",
    "\n",
    "print(f\"Selected {len(selected_fewshot)} few-shot examples:\")\n",
    "for i, ex in enumerate(selected_fewshot, 1):\n",
    "    print(f\"  {i}. {ex.section_context[:50]}... ({len(ex.statements)} statements)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9896eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor initialized\n",
      "Few-shot examples available: 3\n"
     ]
    }
   ],
   "source": [
    "# Create extractor with few-shot demonstrations\n",
    "extractor = StatementExtractor()\n",
    "\n",
    "# In DSPy, we can provide demonstrations directly\n",
    "# The few-shot examples will be used by MIPROv2 for bootstrapping\n",
    "print(\"Extractor initialized\")\n",
    "print(f\"Few-shot examples available: {len(selected_fewshot)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4419625",
   "metadata": {},
   "source": [
    "## 6. Test Extraction on Training Sample\n",
    "\n",
    "Run the extractor on a few training chunks to verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ea088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on: Zohran Mamdani > Zohran Mamdani\n",
      "Text: Zohran Kwame Mamdani (born October 18, 1991) is an American politician who is the mayor-elect of New York City. A member of the Democratic Party and the Democratic Socialists of America, he is set to become New York's first Muslim and Asian American mayor. Mamdani has served as a member of the New Y...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Extracted 22 statements:\n",
      "  1. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) was born on October 18, 1991.\n",
      "  2. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is an American politician.\n",
      "  3. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is the mayor-elect of [New York City](/wiki/New_York_City).\n",
      "  4. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is a member of the [Democratic Party](/wiki/Democratic_Party).\n",
      "  5. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is a member of the [Democratic Socialists of America](/wiki/Democratic_Socialists_of_America).\n",
      "  6. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is set to become [New York City](/wiki/New_York_City)'s first Muslim mayor.\n",
      "  7. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is set to become [New York City](/wiki/New_York_City)'s first Asian American mayor.\n",
      "  8. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) has served as a member of the [New York State Assembly](/wiki/New_York_State_Assembly) for the 36th district since 2021.\n",
      "  9. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) represents the [Queens](/wiki/Queens) neighborhood of [Astoria](/wiki/Astoria,_New_York).\n",
      "  10. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) was born in [Kampala](/wiki/Kampala), [Uganda](/wiki/Uganda).\n",
      "  ... and 12 more\n"
     ]
    }
   ],
   "source": [
    "# Test on a training chunk\n",
    "if training_chunks:\n",
    "    test_chunk = training_chunks[0]\n",
    "    \n",
    "    print(f\"Testing on: {test_chunk.section_context}\")\n",
    "    print(f\"Text: {test_chunk.text[:300]}...\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    result = extractor(\n",
    "        chunk_text=test_chunk.text,\n",
    "        section_context=test_chunk.section_context,\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracted {len(result.statements)} statements:\")\n",
    "    for i, stmt in enumerate(result.statements[:10], 1):\n",
    "        print(f\"  {i}. {stmt}\")\n",
    "    if len(result.statements) > 10:\n",
    "        print(f\"  ... and {len(result.statements) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a247a7a",
   "metadata": {},
   "source": [
    "## 7. Create Training Dataset\n",
    "\n",
    "Convert chunks into DSPy examples. For training, we need to generate initial extractions\n",
    "that can be scored and optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fab9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 2742 examples\n",
      "Dev set: 686 examples\n"
     ]
    }
   ],
   "source": [
    "# Create training examples (without labels - we'll generate and judge them)\n",
    "# For DSPy optimization, we just need the inputs\n",
    "\n",
    "random_seed(42)  # For reproducibility\n",
    "\n",
    "# Convert WikipediaChunk objects to DSPy examples\n",
    "trainset_chunks = list(training_chunks)  # Make a copy\n",
    "shuffle(trainset_chunks)\n",
    "\n",
    "trainset = []\n",
    "for chunk in trainset_chunks:\n",
    "    ex = dspy.Example(\n",
    "        chunk_text=chunk.text,\n",
    "        section_context=chunk.section_context,\n",
    "    ).with_inputs('chunk_text', 'section_context')\n",
    "    trainset.append(ex)\n",
    "\n",
    "# Split into train/dev\n",
    "split_idx = int(len(trainset) * 0.8)\n",
    "devset = trainset[split_idx:]\n",
    "trainset = trainset[:split_idx]\n",
    "\n",
    "print(f\"Training set: {len(trainset)} examples\")\n",
    "print(f\"Dev set: {len(devset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda75e24",
   "metadata": {},
   "source": [
    "## 8. Initialize Judge with Few-Shot Guidance\n",
    "\n",
    "The judge scores extraction quality. We use Albert Einstein examples to calibrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "792d6e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge calibration on few-shot example:\n",
      "  Completeness:      0.95\n",
      "  Atomicity:         0.85\n",
      "  Accuracy:          0.95\n",
      "  Link preservation: 1.00\n",
      "  ---\n",
      "  Weighted score:    0.94\n"
     ]
    }
   ],
   "source": [
    "# Initialize judge\n",
    "judge = StatementQualityJudge()\n",
    "\n",
    "# Test judge on a known good example (Albert Einstein few-shot)\n",
    "if selected_fewshot:\n",
    "    test_ex = selected_fewshot[0]\n",
    "    \n",
    "    evaluation = judge(\n",
    "        chunk_text=test_ex.chunk_text,\n",
    "        section_context=test_ex.section_context,\n",
    "        statements=test_ex.statements,\n",
    "    )\n",
    "    \n",
    "    print(\"Judge calibration on few-shot example:\")\n",
    "    print(f\"  Completeness:      {evaluation.completeness:.2f}\")\n",
    "    print(f\"  Atomicity:         {evaluation.atomicity:.2f}\")\n",
    "    print(f\"  Accuracy:          {evaluation.accuracy:.2f}\")\n",
    "    print(f\"  Link preservation: {evaluation.link_preservation:.2f}\")\n",
    "    print(f\"  ---\")\n",
    "    print(f\"  Weighted score:    {evaluation.weighted_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649d2fc",
   "metadata": {},
   "source": [
    "## 9. Baseline Evaluation\n",
    "\n",
    "Evaluate the unoptimized extractor on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f230a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.24 / 10 (82.4%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 240.28it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:01:27 INFO dspy.evaluate.evaluate: Average Metric: 8.2375 / 10 (82.4%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Baseline quality score: 82.38\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline on dev set\n",
    "EVAL_SIZE = min(10, len(devset))  # Limit for speed\n",
    "\n",
    "evaluator = dspy.Evaluate(\n",
    "    devset=devset[:EVAL_SIZE],\n",
    "    metric=statement_quality_metric,\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    ")\n",
    "\n",
    "baseline_extractor = StatementExtractor()\n",
    "baseline_result = evaluator(baseline_extractor)\n",
    "\n",
    "baseline_score = baseline_result.score if hasattr(baseline_result, 'score') else float(baseline_result)\n",
    "print(f\"\\nBaseline quality score: {baseline_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e36a0",
   "metadata": {},
   "source": [
    "## 9b. MLflow Observability Setup\n",
    "\n",
    "MLflow provides tracing, evaluation, and human feedback tools for DSPy pipelines.\n",
    "\n",
    "### Quick Setup (One-time)\n",
    "\n",
    "1. **Install MLflow** (already in requirements or run cell below)\n",
    "2. **Start the MLflow server** in a terminal:\n",
    "   ```bash\n",
    "   cd /workspaces/wiki3-kg-project\n",
    "   mlflow server \\\n",
    "     --backend-store-uri sqlite:///mlflow.sqlite \\\n",
    "     --default-artifact-root ./mlflow-artifacts \\\n",
    "     --host 0.0.0.0 \\\n",
    "     --port 5000\n",
    "   ```\n",
    "3. **Open the UI** at http://localhost:5000 (or via VS Code port forwarding)\n",
    "\n",
    "### What MLflow Provides\n",
    "- **Tracing**: See every LM call, inputs, outputs, latency\n",
    "- **Evaluation**: Compare model versions side-by-side\n",
    "- **Human Feedback**: Add labels/assessments directly in the UI\n",
    "- **Experiment Tracking**: Track metrics across optimization runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87efb9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MLflow configured successfully\n",
      "   Tracking URI: http://127.0.0.1:5000\n",
      "   Experiment: wiki3-kg-stage1-statements\n",
      "\n",
      "üìä Open MLflow UI: http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MLflow Setup for DSPy Optimization Tracking\n",
    "# =============================================================================\n",
    "# Reference: https://dspy.ai/tutorials/optimizer_tracking/\n",
    "#\n",
    "# Prerequisites:\n",
    "#   1. Install: pip install \"mlflow>=2.21.1\"\n",
    "#   2. Start server (use SQL store for tracing):\n",
    "#      mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\n",
    "#   3. Open UI: http://localhost:5000\n",
    "# =============================================================================\n",
    "\n",
    "import mlflow\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"http://127.0.0.1:5000\"\n",
    "\n",
    "try:\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(\"wiki3-kg-stage1-statements\")\n",
    "    \n",
    "    # Enable autologging with full optimizer tracking (per DSPy docs)\n",
    "    # https://dspy.ai/tutorials/optimizer_tracking/\n",
    "    mlflow.dspy.autolog(\n",
    "        log_compiles=True,           # Track optimization process\n",
    "        log_evals=True,              # Track evaluation results  \n",
    "        log_traces_from_compile=True # Track program traces during optimization\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ MLflow configured successfully\")\n",
    "    print(f\"   Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "    print(f\"   Experiment: wiki3-kg-stage1-statements\")\n",
    "    print(f\"\\nüìä Open MLflow UI: {MLFLOW_TRACKING_URI}\")\n",
    "    MLFLOW_ENABLED = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  MLflow not available: {e}\")\n",
    "    print(f\"\\nüí° To enable MLflow, start the server:\")\n",
    "    print(f\"   mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\")\n",
    "    MLFLOW_ENABLED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de34b2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:01<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation complete!\n",
      "   Average score: 0.82\n",
      "   Traces logged: 10\n",
      "\n",
      "üìä Review in MLflow UI: http://127.0.0.1:5000\n",
      "   ‚Üí Click 'Traces' tab to see all predictions\n",
      "   ‚Üí Click individual traces to review inputs/outputs\n",
      "   ‚Üí Use 'Feedback' to add human labels\n",
      "üèÉ View run baseline_evaluation at: http://127.0.0.1:5000/#/experiments/1/runs/3b98426ce0954065a119d35246a740b2\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-d0afb235ade68f51e67093a0fc48f615&amp;experiment_id=1&amp;trace_id=tr-a1e518a293d228d424655c397d4d2add&amp;experiment_id=1&amp;trace_id=tr-17d821b8bb93cd40d8dff535c62dbaea&amp;experiment_id=1&amp;trace_id=tr-c9ce44bd7c3f1addda385db590eeb067&amp;experiment_id=1&amp;trace_id=tr-4d470ed6a6663769e1d46f8221c50e41&amp;experiment_id=1&amp;trace_id=tr-36a2c52b00473139185df528b4258e19&amp;experiment_id=1&amp;trace_id=tr-5e1885c35037cd6b1a6b6a7d1a38aad6&amp;experiment_id=1&amp;trace_id=tr-9db7284b6965d1d1eb7e2e6426c0747e&amp;experiment_id=1&amp;trace_id=tr-48f091f09b8b38aeb89f3a92999a1d86&amp;experiment_id=1&amp;trace_id=tr-a3595d1f7fe4a42e3db788a61cdcc4c2&amp;experiment_id=1&amp;version=3.7.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-d0afb235ade68f51e67093a0fc48f615), Trace(trace_id=tr-a1e518a293d228d424655c397d4d2add), Trace(trace_id=tr-17d821b8bb93cd40d8dff535c62dbaea), Trace(trace_id=tr-c9ce44bd7c3f1addda385db590eeb067), Trace(trace_id=tr-4d470ed6a6663769e1d46f8221c50e41), Trace(trace_id=tr-36a2c52b00473139185df528b4258e19), Trace(trace_id=tr-5e1885c35037cd6b1a6b6a7d1a38aad6), Trace(trace_id=tr-9db7284b6965d1d1eb7e2e6426c0747e), Trace(trace_id=tr-48f091f09b8b38aeb89f3a92999a1d86), Trace(trace_id=tr-a3595d1f7fe4a42e3db788a61cdcc4c2)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run evaluation with MLflow tracing (if enabled)\n",
    "# Each prediction creates a trace viewable in the MLflow UI\n",
    "\n",
    "if MLFLOW_ENABLED:\n",
    "    with mlflow.start_run(run_name=\"baseline_evaluation\"):\n",
    "        # Log parameters for reproducibility\n",
    "        mlflow.log_param(\"eval_size\", EVAL_SIZE)\n",
    "        mlflow.log_param(\"model\", \"qwen/qwen3-coder-30b\")\n",
    "        mlflow.log_param(\"num_fewshot\", NUM_FEWSHOT)\n",
    "        \n",
    "        # Run extractions on dev set - each one is traced\n",
    "        results = []\n",
    "        for i, ex in enumerate(tqdm(devset[:EVAL_SIZE], desc=\"Evaluating\")):\n",
    "            with mlflow.start_span(name=f\"example_{i}\") as span:\n",
    "                # Run extraction\n",
    "                pred = baseline_extractor(\n",
    "                    chunk_text=ex.chunk_text,\n",
    "                    section_context=ex.section_context,\n",
    "                )\n",
    "                \n",
    "                # Run judge\n",
    "                eval_result = judge(\n",
    "                    chunk_text=ex.chunk_text,\n",
    "                    section_context=ex.section_context,\n",
    "                    statements=pred.statements,\n",
    "                )\n",
    "                \n",
    "                # Log to span for MLflow UI review\n",
    "                span.set_inputs({\n",
    "                    \"chunk_text\": ex.chunk_text[:500],\n",
    "                    \"section_context\": ex.section_context,\n",
    "                })\n",
    "                span.set_outputs({\n",
    "                    \"statements\": list(pred.statements),\n",
    "                    \"completeness\": float(eval_result.completeness),\n",
    "                    \"atomicity\": float(eval_result.atomicity),\n",
    "                    \"accuracy\": float(eval_result.accuracy),\n",
    "                    \"link_preservation\": float(eval_result.link_preservation),\n",
    "                    \"weighted_score\": float(eval_result.weighted_score),\n",
    "                    \"reasoning\": eval_result.reasoning,\n",
    "                })\n",
    "                \n",
    "                results.append({\n",
    "                    \"index\": i,\n",
    "                    \"score\": float(eval_result.weighted_score),\n",
    "                })\n",
    "        \n",
    "        # Log aggregate metrics\n",
    "        avg_score = sum(r[\"score\"] for r in results) / len(results)\n",
    "        mlflow.log_metric(\"avg_quality_score\", avg_score)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Evaluation complete!\")\n",
    "        print(f\"   Average score: {avg_score:.2f}\")\n",
    "        print(f\"   Traces logged: {len(results)}\")\n",
    "        print(f\"\\nüìä Review in MLflow UI: {MLFLOW_TRACKING_URI}\")\n",
    "        print(f\"   ‚Üí Click 'Traces' tab to see all predictions\")\n",
    "        print(f\"   ‚Üí Click individual traces to review inputs/outputs\")\n",
    "        print(f\"   ‚Üí Use 'Feedback' to add human labels\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping MLflow evaluation (server not running)\")\n",
    "    print(\"   Run baseline evaluation with dspy.Evaluate instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "794a21f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent LM calls (use MLflow UI for full traces):\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-12-20T07:01:30.404005]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `chunk_text` (str): Wikipedia article chunk with markdown links preserved\n",
      "2. `section_context` (str): Breadcrumb showing location: Article > Section > Subsection\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `statements` (list[str]): List of atomic statements, each preserving [Entity](/wiki/...) links\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## chunk_text ## ]]\n",
      "{chunk_text}\n",
      "\n",
      "[[ ## section_context ## ]]\n",
      "{section_context}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## statements ## ]]\n",
      "{statements}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Extract atomic, verifiable statements from Wikipedia text.\n",
      "        \n",
      "        Each statement must:\n",
      "        - Be self-contained (understandable without the original text)\n",
      "        - Preserve markdown links: [Entity Name](/wiki/Entity_Name)\n",
      "        - Contain exactly one verifiable claim\n",
      "        - Not editorialize or interpret beyond what's stated\n",
      "        \n",
      "        Example input chunk:\n",
      "            \"Albert Einstein was born in Ulm, in the Kingdom of W√ºrttemberg \n",
      "            in the German Empire, on 14 March 1879.\"\n",
      "        \n",
      "        Example output statements:\n",
      "            - \"[Albert Einstein](/wiki/Albert_Einstein) was born in [Ulm](/wiki/Ulm).\"\n",
      "            - \"[Albert Einstein](/wiki/Albert_Einstein) was born on 14 March 1879.\"\n",
      "            - \"[Ulm](/wiki/Ulm) was in the [Kingdom of W√ºrttemberg](/wiki/Kingdom_of_W√ºrttemberg).\"\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## chunk_text ## ]]\n",
      "In August 1967, Gacy sexually assaulted 15-year-old Donald Voorhees Jr., the son of Donald Edwin Voorhees, a local politician and fellow Jaycee. Gacy lured Voorhees to his house with the promise of showing him heterosexual stag films regularly played at Jaycee events. Gacy plied Voorhees with alcohol, allowed him to watch a stag movie, then persuaded him to engage in mutual oral sex, adding, \"You have to have sex with a man before you start having sex with women.\" Over the following months, Gacy abused several other youths, including one whom he encouraged to have sex with his own wife before blackmailing him into performing oral sex on him. Gacy also tricked several teenagers into believing he was commissioned to conduct homosexual experiments for scientific research and paid them up to $50 each.\n",
      "In March 1968, Voorhees told his father that Gacy had sexually assaulted him. Voorhees Sr. immediately informed the police, who arrested Gacy and charged him with performing oral sodomy on Voorhees and the attempted assault of 16-year-old Edward Lynch. Gacy vehemently denied the charges and demanded to take a polygraph test. The results of these tests were \"indicative of deception\". Gacy publicly denied any wrongdoing and insisted the charges were politically motivated‚ÄîVoorhees Sr. had opposed Gacy's nomination for appointment as president of the Iowa Jaycees. Several fellow Jaycees found Gacy's story credible and rallied to his support\n",
      "\n",
      "[[ ## section_context ## ]]\n",
      "John Wayne Gacy > Assault of Donald Voorhees\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## statements ## ]]` (must be formatted as a valid Python list[str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The chunk describes a sequence of events involving John Wayne Gacy's sexual assault of Donald Voorhees Jr. and subsequent legal consequences. The text contains several verifiable claims that can be broken down into atomic statements:\n",
      "\n",
      "1. Gacy sexually assaulted Donald Voorhees Jr. in August 1967\n",
      "2. Voorhees was 15 years old at the time\n",
      "3. Voorhees was the son of Donald Edwin Voorhees, a local politician and fellow Jaycee\n",
      "4. Gacy lured Voorhees to his house with the promise of showing heterosexual stag films\n",
      "5. Gacy plied Voorhees with alcohol\n",
      "6. Gacy persuaded Voorhees to engage in mutual oral sex\n",
      "7. Gacy told Voorhees, \"You have to have sex with a man before you start having sex with women\"\n",
      "8. Gacy abused several other youths over the following months\n",
      "9. Gacy encouraged one youth to have sex with his own wife before blackmailing him\n",
      "10. Gacy tricked teenagers into believing he was conducting homosexual experiments for scientific research\n",
      "11. Gacy paid teenagers up to $50 each for these experiments\n",
      "12. In March 1968, Voorhees told his father about the assault\n",
      "13. Voorhees Sr. informed the police\n",
      "14. Gacy was arrested and charged with oral sodomy and attempted assault\n",
      "15. Gacy denied the charges and demanded a polygraph test\n",
      "16. Polygraph results were \"indicative of deception\"\n",
      "17. Gacy claimed the charges were politically motivated due to Voorhees Sr.'s opposition to his Jaycees nomination\n",
      "18. Several fellow Jaycees supported Gacy's story\n",
      "\n",
      "Each of these claims is self-contained and can be verified independently, with appropriate links to entities where relevant.\n",
      "\n",
      "[[ ## statements ## ]]\n",
      "[\n",
      "    \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) sexually assaulted [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) in [August 1967](/wiki/August_1967).\",\n",
      "    \"[Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) was 15 years old at the time of the assault.\",\n",
      "    \"[Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) was the son of [Donald Edwin Voorhees](/wiki/Donald_Edwin_Voorhees), a local politician and fellow [Jaycee](/wiki/Jaycees).\",\n",
      "    \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) lured [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) to his house with the promise of showing him heterosexual stag films.\",\n",
      "    \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) plied [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) with alcohol.\",\n",
      "    \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) persuaded [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) to engage in mutual oral sex.\",\n",
      "    \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) told [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.), \\\"You have to have sex with a man before you start having sex with women.\\\"\",\n",
      "    \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) abused several other youths over the following months.\",\n",
      "    \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) encouraged one youth to have sex with his own wife before blackmailing him into performing oral sex on him.\",\n",
      "    \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) tricked several teenagers into believing he was commissioned to conduct homosexual experiments for scientific research.\",\n",
      "    \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) paid teenagers up to $50 each for these experiments.\",\n",
      "    \"In [March 1968](/wiki/March_1968), [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) told his father about the assault.\",\n",
      "    \"[Donald Voorhees Sr.](/wiki/Donald_Edwin_Voorhees) informed the police.\",\n",
      "    \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) was arrested and charged with performing oral sodomy on [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) and the attempted assault of 16-year-old [Edward Lynch](/wiki/Edward_Lynch).\",\n",
      "    \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) denied the charges and demanded to take a polygraph test.\",\n",
      "    \"The results of the polygraph tests were \\\"indicative of deception\\\".\",\n",
      "    \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) claimed the charges were politically motivated‚Äî[Donald Voorhees Sr.](/wiki/Donald_Edwin_Voorhees) had opposed Gacy's nomination for appointment as president of the [Iowa Jaycees](/wiki/Iowa_Jaycees).\",\n",
      "    \"Several fellow [Jaycees](/wiki/Jaycees) found [John Wayne Gacy](/wiki/John_Wayne_Gacy)'s story credible and rallied to his support.\"\n",
      "]\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-12-20T07:01:30.456334]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `chunk_text` (str): Original Wikipedia chunk\n",
      "2. `section_context` (str): Section breadcrumb for context\n",
      "3. `statements` (list[str]): Extracted statements to evaluate\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): Brief explanation of the scores given\n",
      "2. `completeness` (float): Score 0-1: Are all key facts from the chunk captured?\n",
      "3. `atomicity` (float): Score 0-1: Is each statement truly atomic (one claim)?\n",
      "4. `accuracy` (float): Score 0-1: Do statements faithfully represent the source?\n",
      "5. `link_preservation` (float): Score 0-1: Are [Entity](/wiki/...) links preserved correctly?\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## chunk_text ## ]]\n",
      "{chunk_text}\n",
      "\n",
      "[[ ## section_context ## ]]\n",
      "{section_context}\n",
      "\n",
      "[[ ## statements ## ]]\n",
      "{statements}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## completeness ## ]]\n",
      "{completeness}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## atomicity ## ]]\n",
      "{atomicity}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## accuracy ## ]]\n",
      "{accuracy}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## link_preservation ## ]]\n",
      "{link_preservation}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Judge the quality of extracted statements from a Wikipedia chunk.\n",
      "        \n",
      "        Evaluate on four dimensions:\n",
      "        1. Completeness: Are all key facts from the chunk captured?\n",
      "        2. Atomicity: Is each statement truly atomic (one verifiable claim)?\n",
      "        3. Accuracy: Do statements faithfully represent the source without adding info?\n",
      "        4. Link preservation: Are [Entity](/wiki/...) links preserved correctly?\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## chunk_text ## ]]\n",
      "In August 1967, Gacy sexually assaulted 15-year-old Donald Voorhees Jr., the son of Donald Edwin Voorhees, a local politician and fellow Jaycee. Gacy lured Voorhees to his house with the promise of showing him heterosexual stag films regularly played at Jaycee events. Gacy plied Voorhees with alcohol, allowed him to watch a stag movie, then persuaded him to engage in mutual oral sex, adding, \"You have to have sex with a man before you start having sex with women.\" Over the following months, Gacy abused several other youths, including one whom he encouraged to have sex with his own wife before blackmailing him into performing oral sex on him. Gacy also tricked several teenagers into believing he was commissioned to conduct homosexual experiments for scientific research and paid them up to $50 each.\n",
      "In March 1968, Voorhees told his father that Gacy had sexually assaulted him. Voorhees Sr. immediately informed the police, who arrested Gacy and charged him with performing oral sodomy on Voorhees and the attempted assault of 16-year-old Edward Lynch. Gacy vehemently denied the charges and demanded to take a polygraph test. The results of these tests were \"indicative of deception\". Gacy publicly denied any wrongdoing and insisted the charges were politically motivated‚ÄîVoorhees Sr. had opposed Gacy's nomination for appointment as president of the Iowa Jaycees. Several fellow Jaycees found Gacy's story credible and rallied to his support\n",
      "\n",
      "[[ ## section_context ## ]]\n",
      "John Wayne Gacy > Assault of Donald Voorhees\n",
      "\n",
      "[[ ## statements ## ]]\n",
      "[\"[John Wayne Gacy](/wiki/John_Wayne_Gacy) sexually assaulted [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) in [August 1967](/wiki/August_1967).\", \"[Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) was 15 years old at the time of the assault.\", \"[Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) was the son of [Donald Edwin Voorhees](/wiki/Donald_Edwin_Voorhees), a local politician and fellow [Jaycee](/wiki/Jaycees).\", \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) lured [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) to his house with the promise of showing him heterosexual stag films.\", \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) plied [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) with alcohol.\", \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) persuaded [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) to engage in mutual oral sex.\", \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) told [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.), \\\"You have to have sex with a man before you start having sex with women.\\\"\", \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) abused several other youths over the following months.\", \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) encouraged one youth to have sex with his own wife before blackmailing him into performing oral sex on him.\", \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) tricked several teenagers into believing he was commissioned to conduct homosexual experiments for scientific research.\", \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) paid teenagers up to $50 each for these experiments.\", \"In [March 1968](/wiki/March_1968), [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) told his father about the assault.\", \"[Donald Voorhees Sr.](/wiki/Donald_Edwin_Voorhees) informed the police.\", \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) was arrested and charged with performing oral sodomy on [Donald Voorhees Jr.](/wiki/Donald_Voorhees_Jr.) and the attempted assault of 16-year-old [Edward Lynch](/wiki/Edward_Lynch).\", \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) denied the charges and demanded to take a polygraph test.\", \"The results of the polygraph tests were \\\"indicative of deception\\\".\", \"[John Wayne Gacy](/wiki/John_Wayne_Gacy) claimed the charges were politically motivated‚Äî[Donald Voorhees Sr.](/wiki/Donald_Edwin_Voorhees) had opposed Gacy's nomination for appointment as president of the [Iowa Jaycees](/wiki/Iowa_Jaycees).\", \"Several fellow [Jaycees](/wiki/Jaycees) found [John Wayne Gacy](/wiki/John_Wayne_Gacy)'s story credible and rallied to his support.\"]\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## completeness ## ]]` (must be formatted as a valid Python float), then `[[ ## atomicity ## ]]` (must be formatted as a valid Python float), then `[[ ## accuracy ## ]]` (must be formatted as a valid Python float), then `[[ ## link_preservation ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The statements accurately reflect the key facts from the chunk text, capturing the assault details, Gacy's methods, subsequent abuse of other youths, the disclosure to his father, arrest, charges, polygraph results, and political motivations. Each statement is atomic, focusing on one verifiable claim without combining multiple ideas. All [Entity](/wiki/...) links are preserved correctly and consistently with the source text. Completeness is high as all major points from the chunk are included, though a few minor details like specific ages or exact monetary amounts are not mentioned but are not critical to the main narrative.\n",
      "\n",
      "[[ ## completeness ## ]]\n",
      "0.9\n",
      "\n",
      "[[ ## atomicity ## ]]\n",
      "1.0\n",
      "\n",
      "[[ ## accuracy ## ]]\n",
      "1.0\n",
      "\n",
      "[[ ## link_preservation ## ]]\n",
      "1.0\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Use dspy.inspect_history() for quick debugging\n",
    "# This shows recent LM calls without needing MLflow server\n",
    "\n",
    "print(\"Recent LM calls (use MLflow UI for full traces):\")\n",
    "print(\"=\" * 60)\n",
    "dspy.inspect_history(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d708ef8",
   "metadata": {},
   "source": [
    "### MLflow Evaluation with Human Feedback\n",
    "\n",
    "Use MLflow's evaluation API to systematically review predictions and collect human labels.\n",
    "The MLflow UI provides a proper interface for reviewing and annotating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de353ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation dataset with 10 examples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>section_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Abbey Laurel-Smith, businesswoman\\nAdam Oremla...</td>\n",
       "      <td>2021 New York City mayoral election &gt; Failed t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>At the time of Gacy's arrest, he had claimed t...</td>\n",
       "      <td>John Wayne Gacy &gt; Possible additional victims</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>. Kumar's government also announced a scheme o...</td>\n",
       "      <td>Nitish Kumar &gt; Consolidation of Extremely Back...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Queen Elizabeth II died on 8 September 2022, a...</td>\n",
       "      <td>William, Prince of Wales &gt; Prince of Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\"The Life and Death of Robin Williams\". ABC Ne...</td>\n",
       "      <td>Robin Williams &gt; Further reading</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         chunk_text  \\\n",
       "0      0  Abbey Laurel-Smith, businesswoman\\nAdam Oremla...   \n",
       "1      1  At the time of Gacy's arrest, he had claimed t...   \n",
       "2      2  . Kumar's government also announced a scheme o...   \n",
       "3      3  Queen Elizabeth II died on 8 September 2022, a...   \n",
       "4      4  \"The Life and Death of Robin Williams\". ABC Ne...   \n",
       "\n",
       "                                     section_context  \n",
       "0  2021 New York City mayoral election > Failed t...  \n",
       "1      John Wayne Gacy > Possible additional victims  \n",
       "2  Nitish Kumar > Consolidation of Extremely Back...  \n",
       "3         William, Prince of Wales > Prince of Wales  \n",
       "4                   Robin Williams > Further reading  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create evaluation dataset for MLflow\n",
    "import pandas as pd\n",
    "\n",
    "eval_data = []\n",
    "for i, ex in enumerate(devset[:EVAL_SIZE]):\n",
    "    eval_data.append({\n",
    "        \"index\": i,\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"section_context\": ex.section_context,\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "print(f\"Created evaluation dataset with {len(eval_df)} examples\")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b02ec641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying statements for 10 chunks...\n",
      "======================================================================\n",
      "\n",
      "[1/10] 2021 New York City mayoral election > Failed to qu...\n",
      "   ‚Üí 0/0 GOOD (0%)\n",
      "\n",
      "[2/10] John Wayne Gacy > Possible additional victims...\n",
      "   ‚Üí 12/12 GOOD (100%)\n",
      "\n",
      "[3/10] Nitish Kumar > Consolidation of Extremely Backward...\n",
      "   ‚Üí 9/9 GOOD (100%)\n",
      "\n",
      "[4/10] William, Prince of Wales > Prince of Wales...\n",
      "   ‚Üí 9/9 GOOD (100%)\n",
      "\n",
      "[5/10] Robin Williams > Further reading...\n",
      "   ‚Üí 0/0 GOOD (0%)\n",
      "\n",
      "[6/10] Shah Rukh Khan > 2004‚Äì2009: Comeback...\n",
      "   ‚Üí 11/11 GOOD (100%)\n",
      "\n",
      "[7/10] John Wayne Gacy > Cited works...\n",
      "   ‚Üí 10/10 GOOD (100%)\n",
      "\n",
      "[8/10] The Black Phone > Plot...\n",
      "   ‚Üí 23/23 GOOD (100%)\n",
      "\n",
      "[9/10] Thanksgiving > Australia...\n",
      "   ‚Üí 5/5 GOOD (100%)\n",
      "\n",
      "[10/10] John Wayne Gacy > Assault of Donald Voorhees...\n",
      "   ‚Üí 18/18 GOOD (100%)\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Average score: 80.0%\n",
      "Total GOOD: 97, Total BAD: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-6fff15ccb2dc6c344f67448e10eceb1f&amp;experiment_id=1&amp;trace_id=tr-aa8430f84f2da2330e165b129bdc70d1&amp;experiment_id=1&amp;trace_id=tr-8468b74d85e3ade558c1aad6f3f8dea4&amp;experiment_id=1&amp;trace_id=tr-e65fad87c57c74981e38f711e2ec13f4&amp;experiment_id=1&amp;trace_id=tr-82c299f07487d7727f0386fa13badc91&amp;experiment_id=1&amp;trace_id=tr-1d8680cbb90800f3ac93281a92ebeab0&amp;experiment_id=1&amp;trace_id=tr-2dd0622e266bf2eaeb93f78c620af158&amp;experiment_id=1&amp;trace_id=tr-ea75752d6a8fcacb3f8e2d0d52619dec&amp;experiment_id=1&amp;trace_id=tr-8409c5eacde918b33922a7e42de982f8&amp;experiment_id=1&amp;trace_id=tr-74ed51458d9765c7161d5c8753c9fe59&amp;experiment_id=1&amp;version=3.7.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-6fff15ccb2dc6c344f67448e10eceb1f), Trace(trace_id=tr-aa8430f84f2da2330e165b129bdc70d1), Trace(trace_id=tr-8468b74d85e3ade558c1aad6f3f8dea4), Trace(trace_id=tr-e65fad87c57c74981e38f711e2ec13f4), Trace(trace_id=tr-82c299f07487d7727f0386fa13badc91), Trace(trace_id=tr-1d8680cbb90800f3ac93281a92ebeab0), Trace(trace_id=tr-2dd0622e266bf2eaeb93f78c620af158), Trace(trace_id=tr-ea75752d6a8fcacb3f8e2d0d52619dec), Trace(trace_id=tr-8409c5eacde918b33922a7e42de982f8), Trace(trace_id=tr-74ed51458d9765c7161d5c8753c9fe59)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Per-Statement Classification for ALL Evaluation Examples\n",
    "# ============================================================================\n",
    "# Uses StatementClassifier to get GOOD/BAD verdicts per statement\n",
    "\n",
    "from ontological_engineer import StatementClassifier, StatementClassification\n",
    "\n",
    "classifier = StatementClassifier()\n",
    "\n",
    "# Store all results for summary\n",
    "all_classification_results = []\n",
    "\n",
    "print(f\"Classifying statements for {EVAL_SIZE} chunks...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx, ex in enumerate(devset[:EVAL_SIZE]):\n",
    "    print(f\"\\n[{idx+1}/{EVAL_SIZE}] {ex.section_context[:50]}...\")\n",
    "    \n",
    "    # Extract statements\n",
    "    pred = baseline_extractor(\n",
    "        chunk_text=ex.chunk_text,\n",
    "        section_context=ex.section_context,\n",
    "    )\n",
    "    \n",
    "    # Classify each statement\n",
    "    result = classifier(\n",
    "        chunk_text=ex.chunk_text,\n",
    "        section_context=ex.section_context,\n",
    "        statements=list(pred.statements),\n",
    "    )\n",
    "    \n",
    "    # Store result\n",
    "    all_classification_results.append({\n",
    "        \"idx\": idx,\n",
    "        \"section\": ex.section_context,\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"statements\": list(pred.statements),\n",
    "        \"score\": result.score,\n",
    "        \"classifications\": result.classifications,\n",
    "        \"missing_facts\": result.missing_facts,\n",
    "    })\n",
    "    \n",
    "    # Show quick summary\n",
    "    good = sum(1 for c in result.classifications if c.is_good)\n",
    "    total = len(result.classifications)\n",
    "    print(f\"   ‚Üí {good}/{total} GOOD ({result.score:.0%})\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "avg_score = sum(r[\"score\"] for r in all_classification_results) / len(all_classification_results)\n",
    "total_good = sum(sum(1 for c in r[\"classifications\"] if c.is_good) for r in all_classification_results)\n",
    "total_bad = sum(sum(1 for c in r[\"classifications\"] if not c.is_good) for r in all_classification_results)\n",
    "print(f\"Average score: {avg_score:.1%}\")\n",
    "print(f\"Total GOOD: {total_good}, Total BAD: {total_bad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f56db83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAILED PER-CHUNK RESULTS\n",
      "======================================================================\n",
      "\n",
      "üìÑ Chunk 0: 2021 New York City mayoral election > Failed to qualify for ...\n",
      "   Score: 0% (0/0 GOOD)\n",
      "\n",
      "üìÑ Chunk 1: John Wayne Gacy > Possible additional victims...\n",
      "   Score: 100% (12/12 GOOD)\n",
      "\n",
      "üìÑ Chunk 2: Nitish Kumar > Consolidation of Extremely Backward Castes...\n",
      "   Score: 100% (9/9 GOOD)\n",
      "\n",
      "üìÑ Chunk 3: William, Prince of Wales > Prince of Wales...\n",
      "   Score: 100% (9/9 GOOD)\n",
      "\n",
      "üìÑ Chunk 4: Robin Williams > Further reading...\n",
      "   Score: 0% (0/0 GOOD)\n",
      "\n",
      "üìÑ Chunk 5: Shah Rukh Khan > 2004‚Äì2009: Comeback...\n",
      "   Score: 100% (11/11 GOOD)\n",
      "\n",
      "üìÑ Chunk 6: John Wayne Gacy > Cited works...\n",
      "   Score: 100% (10/10 GOOD)\n",
      "\n",
      "üìÑ Chunk 7: The Black Phone > Plot...\n",
      "   Score: 100% (23/23 GOOD)\n",
      "\n",
      "üìÑ Chunk 8: Thanksgiving > Australia...\n",
      "   Score: 100% (5/5 GOOD)\n",
      "\n",
      "üìÑ Chunk 9: John Wayne Gacy > Assault of Donald Voorhees...\n",
      "   Score: 100% (18/18 GOOD)\n"
     ]
    }
   ],
   "source": [
    "# Display detailed results for each chunk\n",
    "print(\"DETAILED PER-CHUNK RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for r in all_classification_results:\n",
    "    good = sum(1 for c in r[\"classifications\"] if c.is_good)\n",
    "    bad = sum(1 for c in r[\"classifications\"] if not c.is_good)\n",
    "    total = len(r[\"classifications\"])\n",
    "    \n",
    "    print(f\"\\nüìÑ Chunk {r['idx']}: {r['section'][:60]}...\")\n",
    "    print(f\"   Score: {r['score']:.0%} ({good}/{total} GOOD)\")\n",
    "    \n",
    "    # Show BAD statements (these need attention)\n",
    "    bad_stmts = [c for c in r[\"classifications\"] if not c.is_good]\n",
    "    if bad_stmts:\n",
    "        print(f\"   ‚ùå BAD statements:\")\n",
    "        for c in bad_stmts:\n",
    "            print(f\"      [{c.index}] {c.statement[:80]}...\")\n",
    "            print(f\"          Reason: {c.reason}\")\n",
    "    \n",
    "    if r[\"missing_facts\"] and r[\"missing_facts\"].lower() != \"none\":\n",
    "        print(f\"   üìù Missing: {r['missing_facts'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c283f",
   "metadata": {},
   "source": [
    "### Export Annotations from MLflow\n",
    "\n",
    "After reviewing and labeling in the MLflow UI, export your annotations for judge improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e23dcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest run: 3b98426ce0954065a119d35246a740b2\n",
      "Metrics: {'avg_quality_score': 0.8237500000000001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75668/2900277544.py:22: FutureWarning: Parameter 'experiment_ids' is deprecated. Please use 'locations' instead.\n",
      "  traces = client.search_traces(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 traces\n"
     ]
    }
   ],
   "source": [
    "# Load annotations from MLflow (after you've labeled them in the UI)\n",
    "# MLflow stores feedback as assessments on traces\n",
    "\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "# Get the latest evaluation run\n",
    "experiment = client.get_experiment_by_name(\"wiki3-kg-stage1-statements\")\n",
    "if experiment:\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"start_time DESC\"],\n",
    "        max_results=1,\n",
    "    )\n",
    "    \n",
    "    if runs:\n",
    "        latest_run = runs[0]\n",
    "        print(f\"Latest run: {latest_run.info.run_id}\")\n",
    "        print(f\"Metrics: {latest_run.data.metrics}\")\n",
    "        \n",
    "        # Get traces with assessments (human feedback)\n",
    "        try:\n",
    "            traces = client.search_traces(\n",
    "                experiment_ids=[experiment.experiment_id],\n",
    "                max_results=100,\n",
    "            )\n",
    "            print(f\"Found {len(traces)} traces\")\n",
    "        except Exception as e:\n",
    "            print(f\"Trace search error: {e}\")\n",
    "else:\n",
    "    print(\"No experiment found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52239b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use human feedback to improve the judge\n",
    "# After collecting labels in MLflow, create DSPy training examples\n",
    "\n",
    "# For now, save the evaluation data for later use\n",
    "output_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "eval_df.to_json(output_dir / \"eval_dataset.json\", orient=\"records\", indent=2)\n",
    "print(f\"Saved evaluation dataset to {output_dir / 'eval_dataset.json'}\")\n",
    "\n",
    "print(\"\"\"\n",
    "üìã Next steps for human feedback:\n",
    "\n",
    "1. Start MLflow server:\n",
    "   mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\n",
    "\n",
    "2. Open MLflow UI at http://127.0.0.1:5000\n",
    "\n",
    "3. Navigate to the experiment 'wiki3-kg-stage1-statements'\n",
    "\n",
    "4. Click on traces to review predictions\n",
    "\n",
    "5. Use the feedback/assessment features to label quality\n",
    "\n",
    "6. Export labeled data for judge improvement\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b175b",
   "metadata": {},
   "source": [
    "## 10. MIPROv2 Prompt Optimization\n",
    "\n",
    "Use DSPy's MIPROv2 optimizer to improve the extractor's prompts.\n",
    "This uses the few-shot examples to bootstrap better demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f711918",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:06:52 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'cdf4428219734698a0708c4dd0003049', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current dspy workflow\n",
      "2025/12/20 07:06:52 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "RUNNING WITH THE FOLLOWING LIGHT AUTO RUN SETTINGS:\n",
      "num_trials: 10\n",
      "minibatch: False\n",
      "num_fewshot_candidates: 6\n",
      "num_instruct_candidates: 3\n",
      "valset size: 16\n",
      "\n",
      "2025/12/20 07:06:52 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
      "2025/12/20 07:06:52 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
      "\n",
      "2025/12/20 07:06:52 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=6 sets of demonstrations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing with 20 training examples...\n",
      "Using 3 few-shot demos for bootstrapping...\n",
      "MIPROv2 mode: auto='light'\n",
      "Bootstrapping set 1/6\n",
      "Bootstrapping set 2/6\n",
      "Bootstrapping set 3/6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd17651ecfc4c08a60475a51d9dd116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:10<00:03,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35ed918186a4092929f8a6d6a8898da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 4/6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84da3ba7658e45babe3053cf04da6f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba0410314aa47ca8ef339597dafb3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 5/6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4c722effaf4136b4b05cc928233fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 1/4 [00:09<00:27,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4212feb7f8924601b142f8482b953dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapping set 6/6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a97a54fb3f4adaa778def63c62bb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:00<00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 3 examples for up to 1 rounds, amounting to 3 attempts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab9ed0dcd0848c498d1cb710e756f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:07:17 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
      "2025/12/20 07:07:17 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n",
      "2025/12/20 07:07:22 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "Proposing N=3 instructions...\n",
      "\n",
      "2025/12/20 07:08:11 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
      "\n",
      "2025/12/20 07:08:11 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Extract atomic, verifiable statements from Wikipedia text.\n",
      "\n",
      "Each statement must:\n",
      "- Be self-contained (understandable without the original text)\n",
      "- Preserve markdown links: [Entity Name](/wiki/Entity_Name)\n",
      "- Contain exactly one verifiable claim\n",
      "- Not editorialize or interpret beyond what's stated\n",
      "\n",
      "Example input chunk:\n",
      "    \"Albert Einstein was born in Ulm, in the Kingdom of W√ºrttemberg \n",
      "    in the German Empire, on 14 March 1879.\"\n",
      "    \n",
      "Example output statements:\n",
      "    - \"[Albert Einstein](/wiki/Albert_Einstein) was born in [Ulm](/wiki/Ulm).\"\n",
      "    - \"[Albert Einstein](/wiki/Albert_Einstein) was born on 14 March 1879.\"\n",
      "    - \"[Ulm](/wiki/Ulm) was in the [Kingdom of W√ºrttemberg](/wiki/Kingdom_of_W√ºrttemberg).\"\n",
      "\n",
      "2025/12/20 07:08:11 INFO dspy.teleprompt.mipro_optimizer_v2: 1: You are a meticulous knowledge extraction specialist tasked with transforming Wikipedia text into structured, atomic statements. Your job is to parse provided text chunks and extract individual, verifiable facts while preserving markdown links in the format [Entity Name](/wiki/Entity_Name). Each statement must be self-contained, containing only one claim that can stand alone without referring back to the original text. Avoid editorializing or adding interpretation beyond what is explicitly stated in the source.\n",
      "\n",
      "When processing a chunk of text:\n",
      "1. Identify all verifiable claims\n",
      "2. Break each claim into atomic statements\n",
      "3. Preserve any existing markdown links\n",
      "4. Ensure each statement is clear and standalone\n",
      "5. Return a list of these statements in the format shown in the examples\n",
      "\n",
      "Example input:\n",
      "    \"Albert Einstein was born in Ulm, in the Kingdom of W√ºrttemberg \n",
      "    in the German Empire, on 14 March 1879.\"\n",
      "    \n",
      "Example output:\n",
      "    - \"[Albert Einstein](/wiki/Albert_Einstein) was born in [Ulm](/wiki/Ulm).\"\n",
      "    - \"[Albert Einstein](/wiki/Albert_Einstein) was born on 14 March 1879.\"\n",
      "    - \"[Ulm](/wiki/Ulm) was in the [Kingdom of W√ºrttemberg](/wiki/Kingdom_of_W√ºrttemberg).\"\n",
      "\n",
      "Apply this same rigorous approach to any Wikipedia text provided, ensuring each extracted statement is accurate, atomic, and properly formatted.\n",
      "\n",
      "2025/12/20 07:08:11 INFO dspy.teleprompt.mipro_optimizer_v2: 2: Extract atomic, verifiable statements from text chunks, particularly from Wikipedia articles or biographical content.\n",
      "\n",
      "Each statement must:\n",
      "- Be self-contained (understandable without the original text)\n",
      "- Preserve markdown links: [Entity Name](/wiki/Entity_Name) \n",
      "- Contain exactly one verifiable claim\n",
      "- Not editorialize or interpret beyond what's stated\n",
      "\n",
      "When processing the text:\n",
      "1. Identify distinct factual claims\n",
      "2. Decompose compound sentences into individual statements\n",
      "3. Ensure each statement is atomic and verifiable\n",
      "4. Maintain entity references using markdown link format\n",
      "5. Focus on public figures, political events, or institutional changes where applicable\n",
      "\n",
      "Example input chunk:\n",
      "    \"Albert Einstein was born in Ulm, in the Kingdom of W√ºrttemberg \n",
      "    in the German Empire, on 14 March 1879.\"\n",
      "    \n",
      "Example output statements:\n",
      "    - \"[Albert Einstein](/wiki/Albert_Einstein) was born in [Ulm](/wiki/Ulm).\"\n",
      "    - \"[Albert Einstein](/wiki/Albert_Einstein) was born on 14 March 1879.\"\n",
      "    - \"[Ulm](/wiki/Ulm) was in the [Kingdom of W√ºrttemberg](/wiki/Kingdom_of_W√ºrttemberg).\"\n",
      "\n",
      "Apply this same logic to extract statements from biographical, political, or historical content, ensuring that each claim is isolated and verifiable.\n",
      "\n",
      "2025/12/20 07:08:11 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
      "\n",
      "2025/12/20 07:08:11 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
      "2025/12/20 07:08:11 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
      "\n",
      "2025/12/20 07:08:11 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 10 - Full Evaluation of Default Program ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 15.31 / 16 (95.7%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [02:39<00:00,  9.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:10:50 INFO dspy.evaluate.evaluate: Average Metric: 15.3125 / 16 (95.7%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:10:51 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 95.7\n",
      "\n",
      "/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/optuna/_experimental.py:32: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "2025/12/20 07:10:51 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 2 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run eval_full_0 at: http://127.0.0.1:5000/#/experiments/1/runs/e3ab7818eb3548a48f4feefad4ba41e4\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Average Metric: 14.26 / 16 (89.1%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [03:07<00:00, 11.74s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:13:59 INFO dspy.evaluate.evaluate: Average Metric: 14.255000000000003 / 16 (89.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:13:59 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 89.09 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 3'].\n",
      "2025/12/20 07:13:59 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [95.7, 89.09]\n",
      "2025/12/20 07:13:59 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 95.7\n",
      "2025/12/20 07:13:59 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/12/20 07:13:59 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 3 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run eval_full_1 at: http://127.0.0.1:5000/#/experiments/1/runs/bd7afe8afb444ad2bcbb617e1b16f6ce\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Average Metric: 14.81 / 16 (92.6%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [03:16<00:00, 12.26s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:17:16 INFO dspy.evaluate.evaluate: Average Metric: 14.812500000000002 / 16 (92.6%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:17:16 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 92.58 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 0'].\n",
      "2025/12/20 07:17:16 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [95.7, 89.09, 92.58]\n",
      "2025/12/20 07:17:16 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 95.7\n",
      "2025/12/20 07:17:16 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/12/20 07:17:16 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 4 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run eval_full_2 at: http://127.0.0.1:5000/#/experiments/1/runs/775e8f2f3e524cb59adc9bad3680b0eb\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "Average Metric: 13.32 / 16 (83.3%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [03:09<00:00, 11.86s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:20:26 INFO dspy.evaluate.evaluate: Average Metric: 13.325 / 16 (83.3%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:20:26 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 83.28 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 5'].\n",
      "2025/12/20 07:20:26 INFO dspy.teleprompt.mipro_optimizer_v2: Scores so far: [95.7, 89.09, 92.58, 83.28]\n",
      "2025/12/20 07:20:26 INFO dspy.teleprompt.mipro_optimizer_v2: Best score so far: 95.7\n",
      "2025/12/20 07:20:26 INFO dspy.teleprompt.mipro_optimizer_v2: ========================\n",
      "\n",
      "\n",
      "2025/12/20 07:20:26 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 5 / 10 =====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run eval_full_3 at: http://127.0.0.1:5000/#/experiments/1/runs/a8ddc52f4c2840f78cff091161799153\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:21:09 ERROR dspy.utils.parallelizer: Error for Example({'chunk_text': \"Halloween, also known as All Hallows' Eve, or All Saints' Eve, is a celebration observed in many countries  on 31 October, the eve of the Western Christian feast of All Hallows' Day. It is at the beginning of the observance of Allhallowtide, the time in the Christian liturgical year dedicated to remembering the dead, including saints (hallows), martyrs, and all the faithful departed. In popular culture, Halloween has become a celebration of horror and is associated with the macabre and the supernatural.\\nOne theory holds that many Halloween traditions were influenced by Celtic harvest festivals, particularly the Gaelic festival Samhain, which are believed to have pagan roots. Some theories go further and suggest that Samhain may have been Christianized as All Hallows' Day, along with its eve, by the Church. Other academics say Halloween began independently as a Christian holiday, being the vigil  of All Hallows' Day\", 'section_context': 'Halloween > Halloween'}) (input_keys={'chunk_text', 'section_context'}): litellm.BadRequestError: OpenAIException - Error code: 400 - {'error': \"'response_format.type' must be 'json_schema' or 'text'\"}. Set `provide_traceback=True` for traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 0.00 / 0 (0%):   6%|‚ñã         | 1/16 [00:42<10:41, 42.80s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 07:21:50 WARNING dspy.utils.parallelizer: SIGINT received. Cancelling.\n",
      "[W 2025-12-20 07:21:50,968] Trial 4 failed with parameters: {'0_predictor_instruction': 2, '0_predictor_demos': 2} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 205, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/teleprompt/mipro_optimizer_v2.py\", line 510, in objective\n",
      "    score = eval_candidate_program(batch_size, valset, candidate_program, evaluate, self.rng).score\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/teleprompt/utils.py\", line 53, in eval_candidate_program\n",
      "    return evaluate(candidate_program, devset=trainset, callback_metadata={\"metric_key\": \"eval_full\"})\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 484, in safe_patch_function\n",
      "    patch_function(call_original, *args, **kwargs)\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/dspy/autolog.py\", line 234, in _patched_evaluate\n",
      "    return original(self, *new_args, **new_kwargs)\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 475, in call_original\n",
      "    return call_original_fn_with_event_logging(_original_fn, og_args, og_kwargs)\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 426, in call_original_fn_with_event_logging\n",
      "    original_fn_result = original_fn(*og_args, **og_kwargs)\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py\", line 472, in _original_fn\n",
      "    original_result = original(*_og_args, **_og_kwargs)\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/utils/callback.py\", line 339, in sync_wrapper\n",
      "    results = fn(instance, *args, **kwargs)\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/evaluate/evaluate.py\", line 175, in __call__\n",
      "    results = executor.execute(process_item, devset)\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/utils/parallelizer.py\", line 50, in execute\n",
      "    return self._execute_parallel(wrapped, data)\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/utils/parallelizer.py\", line 151, in _execute_parallel\n",
      "    done, not_done = wait(futures_set, timeout=1, return_when=FIRST_COMPLETED)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 307, in wait\n",
      "    waiter.event.wait(timeout)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 607, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 324, in wait\n",
      "    gotit = waiter.acquire(True, timeout)\n",
      "  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/utils/parallelizer.py\", line 111, in handler\n",
      "    orig_handler(sig, frame)\n",
      "KeyboardInterrupt\n",
      "[W 2025-12-20 07:21:50,970] Trial 4 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run eval_full_4 at: http://127.0.0.1:5000/#/experiments/1/runs/90620f9d3d884ca5acf2c33c0d427590\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
      "üèÉ View run welcoming-toad-10 at: http://127.0.0.1:5000/#/experiments/1/runs/cdf4428219734698a0708c4dd0003049\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMIPROv2 mode: auto=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# MIPROv2.compile() - MLflow automatically tracks via autolog()\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Parent run: overall optimization, child runs: each intermediate program\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m optimized_extractor \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mStatementExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mTRAIN_SIZE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_bootstrapped_demos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_FEWSHOT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOptimization complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìä View optimization traces in MLflow UI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMLFLOW_TRACKING_URI\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:484\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m call_original \u001b[38;5;241m=\u001b[39m update_wrapper_extended(call_original, original)\n\u001b[1;32m    482\u001b[0m event_logger\u001b[38;5;241m.\u001b[39mlog_patch_function_start(args, kwargs)\n\u001b[0;32m--> 484\u001b[0m \u001b[43mpatch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m session\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msucceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m event_logger\u001b[38;5;241m.\u001b[39mlog_patch_function_success(args, kwargs)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:182\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001b[0;34m(original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     managed_run \u001b[38;5;241m=\u001b[39m create_managed_run()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpatch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m managed_run:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/dspy/autolog.py:186\u001b[0m, in \u001b[0;36m_patched_compile\u001b[0;34m(original, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# NB: Log a dummy run outputs such that \"Run\" tab is shown in the UI. Currently, the\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# GenAI experiment does not show the \"Run\" tab without this, which is critical gap for\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# DSPy users. This should be done BEFORE the compile call, because Run page is used\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# for tracking the compile progress, not only after finishing the compile.\u001b[39;00m\n\u001b[1;32m    184\u001b[0m log_dummy_model_outputs()\n\u001b[0;32m--> 186\u001b[0m program \u001b[38;5;241m=\u001b[39m \u001b[43m_compile_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Save the state of the best model in json format\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# so that users can see the demonstrations and instructions.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m save_dspy_module_state(program, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/dspy/autolog.py:166\u001b[0m, in \u001b[0;36m_patched_compile.<locals>._compile_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m get_autologging_config(FLAVOR_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_traces_from_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 166\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m         result \u001b[38;5;241m=\u001b[39m _trace_disabled_fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:475\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m         original_result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;241m*\u001b[39m_og_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_og_kwargs)\n\u001b[1;32m    473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_original_fn_with_event_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_original_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:426\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    424\u001b[0m     event_logger\u001b[38;5;241m.\u001b[39mlog_original_function_start(og_args, og_kwargs)\n\u001b[0;32m--> 426\u001b[0m     original_fn_result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m     event_logger\u001b[38;5;241m.\u001b[39mlog_original_function_success(og_args, og_kwargs)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:472\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m NonMlflowWarningsBehaviorForCurrentThread(\n\u001b[1;32m    469\u001b[0m     disable_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    470\u001b[0m     reroute_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    471\u001b[0m ):\n\u001b[0;32m--> 472\u001b[0m     original_result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_og_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_og_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/teleprompt/mipro_optimizer_v2.py:206\u001b[0m, in \u001b[0;36mMIPROv2.compile\u001b[0;34m(self, student, trainset, teacher, valset, num_trials, max_bootstrapped_demos, max_labeled_demos, seed, minibatch, minibatch_size, minibatch_full_eval_steps, program_aware_proposer, data_aware_proposer, view_data_batch_size, tip_aware_proposer, fewshot_aware_proposer, requires_permission_to_run, provide_traceback)\u001b[0m\n\u001b[1;32m    202\u001b[0m     demo_candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_model):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Step 3: Find optimal prompt parameters\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     best_program \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimize_prompt_parameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstruction_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdemo_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminibatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminibatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminibatch_full_eval_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_program\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/teleprompt/mipro_optimizer_v2.py:594\u001b[0m, in \u001b[0;36mMIPROv2._optimize_prompt_parameters\u001b[0;34m(self, program, instruction_candidates, demo_candidates, evaluate, valset, num_trials, minibatch, minibatch_size, minibatch_full_eval_steps, seed)\u001b[0m\n\u001b[1;32m    588\u001b[0m trial \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mtrial\u001b[38;5;241m.\u001b[39mcreate_trial(\n\u001b[1;32m    589\u001b[0m     params\u001b[38;5;241m=\u001b[39mdefault_params,\n\u001b[1;32m    590\u001b[0m     distributions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_distributions(program, instruction_candidates, demo_candidates),\n\u001b[1;32m    591\u001b[0m     value\u001b[38;5;241m=\u001b[39mdefault_score,\n\u001b[1;32m    592\u001b[0m )\n\u001b[1;32m    593\u001b[0m study\u001b[38;5;241m.\u001b[39madd_trial(trial)\n\u001b[0;32m--> 594\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;66;03m# Attach logs to best program\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_program \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_stats:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/optuna/study/study.py:490\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    390\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:67\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 67\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:164\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     frozen_trial_id \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:262\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    258\u001b[0m     updated_state \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    261\u001b[0m ):\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trial\u001b[38;5;241m.\u001b[39m_trial_id\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:205\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/teleprompt/mipro_optimizer_v2.py:510\u001b[0m, in \u001b[0;36mMIPROv2._optimize_prompt_parameters.<locals>.objective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# Evaluate the candidate program (on minibatch if minibatch=True)\u001b[39;00m\n\u001b[1;32m    509\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m minibatch_size \u001b[38;5;28;01mif\u001b[39;00m minibatch \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valset)\n\u001b[0;32m--> 510\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43meval_candidate_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_program\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscore\n\u001b[1;32m    511\u001b[0m total_eval_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m    513\u001b[0m \u001b[38;5;66;03m# Update best score and program\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/teleprompt/utils.py:53\u001b[0m, in \u001b[0;36meval_candidate_program\u001b[0;34m(batch_size, trainset, candidate_program, evaluate, rng)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Evaluate on the full trainset\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(trainset):\n\u001b[0;32m---> 53\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidate_program\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetric_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_full\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Or evaluate on a minibatch\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m evaluate(\n\u001b[1;32m     57\u001b[0m             candidate_program,\n\u001b[1;32m     58\u001b[0m             devset\u001b[38;5;241m=\u001b[39mcreate_minibatch(trainset, batch_size, rng),\n\u001b[1;32m     59\u001b[0m             callback_metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_minibatch\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     60\u001b[0m         )\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:484\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m call_original \u001b[38;5;241m=\u001b[39m update_wrapper_extended(call_original, original)\n\u001b[1;32m    482\u001b[0m event_logger\u001b[38;5;241m.\u001b[39mlog_patch_function_start(args, kwargs)\n\u001b[0;32m--> 484\u001b[0m \u001b[43mpatch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m session\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msucceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m event_logger\u001b[38;5;241m.\u001b[39mlog_patch_function_success(args, kwargs)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/dspy/autolog.py:234\u001b[0m, in \u001b[0;36m_patched_evaluate\u001b[0;34m(original, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args_passed_positional:\n\u001b[1;32m    232\u001b[0m     new_args\u001b[38;5;241m.\u001b[39mappend(new_kwargs\u001b[38;5;241m.\u001b[39mpop(arg))\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:475\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    472\u001b[0m         original_result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;241m*\u001b[39m_og_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_og_kwargs)\n\u001b[1;32m    473\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_original_fn_with_event_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_original_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:426\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    424\u001b[0m     event_logger\u001b[38;5;241m.\u001b[39mlog_original_function_start(og_args, og_kwargs)\n\u001b[0;32m--> 426\u001b[0m     original_fn_result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m     event_logger\u001b[38;5;241m.\u001b[39mlog_original_function_success(og_args, og_kwargs)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:472\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m NonMlflowWarningsBehaviorForCurrentThread(\n\u001b[1;32m    469\u001b[0m     disable_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    470\u001b[0m     reroute_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    471\u001b[0m ):\n\u001b[0;32m--> 472\u001b[0m     original_result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_og_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_og_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/utils/callback.py:339\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.sync_wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:175\u001b[0m, in \u001b[0;36mEvaluate.__call__\u001b[0;34m(self, program, metric, devset, num_threads, display_progress, display_table, callback_metadata, save_as_csv, save_as_json)\u001b[0m\n\u001b[1;32m    172\u001b[0m     score \u001b[38;5;241m=\u001b[39m metric(example, prediction)\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction, score\n\u001b[0;32m--> 175\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devset) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(results)\n\u001b[1;32m    178\u001b[0m results \u001b[38;5;241m=\u001b[39m [((dspy\u001b[38;5;241m.\u001b[39mPrediction(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfailure_score) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/utils/parallelizer.py:50\u001b[0m, in \u001b[0;36mParallelExecutor.execute\u001b[0;34m(self, function, data)\u001b[0m\n\u001b[1;32m     48\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mtqdm\u001b[38;5;241m.\u001b[39m_instances\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     49\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_function(function)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/utils/parallelizer.py:151\u001b[0m, in \u001b[0;36mParallelExecutor._execute_parallel\u001b[0;34m(self, function, data)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_done():\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m done, not_done \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFIRST_COMPLETED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m done:\n\u001b[1;32m    153\u001b[0m     futures_set\u001b[38;5;241m.\u001b[39mremove(f)\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:307\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[1;32m    305\u001b[0m     waiter \u001b[38;5;241m=\u001b[39m _create_and_install_waiters(fs, return_when)\n\u001b[0;32m--> 307\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m f\u001b[38;5;241m.\u001b[39m_condition:\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/utils/parallelizer.py:111\u001b[0m, in \u001b[0;36mParallelExecutor._execute_parallel.<locals>.interrupt_manager.<locals>.handler\u001b[0;34m(sig, frame)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_jobs\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m    110\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSIGINT received. Cancelling.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m \u001b[43morig_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-cb371d97cdcc2d6a1a16998c7648776e&amp;experiment_id=1&amp;trace_id=tr-15839b99a50a13dcf004e897a15b2e18&amp;experiment_id=1&amp;trace_id=tr-178be96820f87ba2cf2e32967184b713&amp;experiment_id=1&amp;trace_id=tr-7f0223bc651ea252b9744cded259f960&amp;experiment_id=1&amp;trace_id=tr-9d3616a108dec921260cb547e2280fe3&amp;experiment_id=1&amp;trace_id=tr-a484e035330faaa36b52f93c2321e64e&amp;experiment_id=1&amp;trace_id=tr-7acc99705beec570b69f75c5c82ba1be&amp;experiment_id=1&amp;trace_id=tr-cf89569de550d73607806e713811b519&amp;experiment_id=1&amp;trace_id=tr-3f738f5019a2676f28dd2bfdf956be95&amp;experiment_id=1&amp;trace_id=tr-636fa890cc21cbdbfb8fe8c4d8b48ead&amp;experiment_id=1&amp;version=3.7.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-cb371d97cdcc2d6a1a16998c7648776e), Trace(trace_id=tr-15839b99a50a13dcf004e897a15b2e18), Trace(trace_id=tr-178be96820f87ba2cf2e32967184b713), Trace(trace_id=tr-7f0223bc651ea252b9744cded259f960), Trace(trace_id=tr-9d3616a108dec921260cb547e2280fe3), Trace(trace_id=tr-a484e035330faaa36b52f93c2321e64e), Trace(trace_id=tr-7acc99705beec570b69f75c5c82ba1be), Trace(trace_id=tr-cf89569de550d73607806e713811b519), Trace(trace_id=tr-3f738f5019a2676f28dd2bfdf956be95), Trace(trace_id=tr-636fa890cc21cbdbfb8fe8c4d8b48ead)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "# Configure optimizer\n",
    "# Reference: https://dspy.ai/tutorials/optimizer_tracking/\n",
    "optimizer = MIPROv2(\n",
    "    metric=statement_quality_metric,\n",
    "    auto=\"light\",  # Use \"light\" preset (fast), \"medium\", or \"heavy\" for more trials\n",
    ")\n",
    "\n",
    "# Use training set for optimization\n",
    "TRAIN_SIZE = len(trainset)\n",
    "\n",
    "print(f\"Optimizing with {TRAIN_SIZE} training examples...\")\n",
    "print(f\"Using {len(selected_fewshot)} few-shot demos for bootstrapping...\")\n",
    "print(f\"MIPROv2 mode: auto='light'\")\n",
    "\n",
    "# MIPROv2.compile() - MLflow automatically tracks via autolog()\n",
    "# Parent run: overall optimization, child runs: each intermediate program\n",
    "optimized_extractor = optimizer.compile(\n",
    "    StatementExtractor(),\n",
    "    trainset=trainset[:TRAIN_SIZE],\n",
    "    max_bootstrapped_demos=NUM_FEWSHOT,\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization complete!\")\n",
    "print(f\"üìä View optimization traces in MLflow UI: {MLFLOW_TRACKING_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d59aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate optimized extractor\n",
    "optimized_result = evaluator(optimized_extractor)\n",
    "optimized_score = optimized_result.score if hasattr(optimized_result, 'score') else float(optimized_result)\n",
    "\n",
    "print(f\"Baseline score:  {baseline_score:.2f}\")\n",
    "print(f\"Optimized score: {optimized_score:.2f}\")\n",
    "print(f\"Improvement:     {optimized_score - baseline_score:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fab06",
   "metadata": {},
   "source": [
    "## 11. Inspect Optimized Prompts\n",
    "\n",
    "See what prompts MIPROv2 discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a08e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the optimized module\n",
    "print(\"Optimized extractor configuration:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to access the optimized signature/demos\n",
    "if hasattr(optimized_extractor, 'demos'):\n",
    "    print(f\"\\nDemonstrations: {len(optimized_extractor.demos)}\")\n",
    "    for i, demo in enumerate(optimized_extractor.demos[:2], 1):\n",
    "        print(f\"  Demo {i}: {demo.section_context[:50]}...\")\n",
    "\n",
    "# Check for any instruction changes\n",
    "if hasattr(optimized_extractor, 'signature'):\n",
    "    print(f\"\\nSignature: {optimized_extractor.signature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0c651",
   "metadata": {},
   "source": [
    "## 12. Save Results\n",
    "\n",
    "Save the optimized extractor and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e45006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training metadata\n",
    "output_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save training results\n",
    "results = {\n",
    "    \"baseline_score\": baseline_score,\n",
    "    \"optimized_score\": optimized_score,\n",
    "    \"train_size\": TRAIN_SIZE,\n",
    "    \"eval_size\": EVAL_SIZE,\n",
    "    \"num_fewshot\": NUM_FEWSHOT,\n",
    "    \"pages_processed\": pages_processed,\n",
    "    \"total_chunks\": len(training_chunks),\n",
    "}\n",
    "\n",
    "with open(output_dir / \"stage1_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Saved results to {output_dir / 'stage1_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized extractor state\n",
    "try:\n",
    "    optimized_extractor.save(output_dir / \"optimized_extractor\")\n",
    "    print(f\"Saved optimized extractor to {output_dir / 'optimized_extractor'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save extractor state: {e}\")\n",
    "    # Alternative: save as JSON\n",
    "    if hasattr(optimized_extractor, 'dump_state'):\n",
    "        state = optimized_extractor.dump_state()\n",
    "        with open(output_dir / \"optimized_extractor_state.json\", \"w\") as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "        print(\"Saved extractor state as JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec590ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save few-shot examples for reference\n",
    "fewshot_data = []\n",
    "for ex in selected_fewshot:\n",
    "    fewshot_data.append({\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"section_context\": ex.section_context,\n",
    "        \"statements\": list(ex.statements),\n",
    "    })\n",
    "\n",
    "with open(output_dir / \"fewshot_examples.json\", \"w\") as f:\n",
    "    json.dump(fewshot_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(fewshot_data)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e226c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. Loaded Albert Einstein as few-shot examples (seed/guidance)\n",
    "2. Fetched and chunked Wikipedia sample pages for training\n",
    "3. Established baseline extraction quality\n",
    "4. Ran MIPROv2 prompt optimization\n",
    "5. Saved the optimized extractor\n",
    "\n",
    "Next steps:\n",
    "- **Stage 2**: Schema matching with optimized statements\n",
    "- **Stage 3**: RDF generation training\n",
    "- **Arbor GRPO**: Fine-tune the full pipeline end-to-end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
