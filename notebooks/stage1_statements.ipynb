{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6df303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "sys.path.insert(0, '/workspaces/wiki3-kg-project')\n",
    "\n",
    "import dspy\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from pathlib import Path\n",
    "from random import shuffle, seed as random_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ontological_engineer import (\n",
    "    configure_lm,\n",
    "    StatementExtractor,\n",
    "    StatementQualityJudge,\n",
    ")\n",
    "from ontological_engineer.judges import statement_quality_metric\n",
    "from ontological_engineer.training.bootstrap import (\n",
    "    load_chunks_from_notebook,\n",
    "    load_facts_from_notebook,\n",
    "    create_training_examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f86eb6",
   "metadata": {},
   "source": [
    "## 1. Configure Language Model\n",
    "\n",
    "Connect to LM Studio running Qwen-30B (or your preferred model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e839f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured LM: <dspy.clients.lm.LM object at 0xffff7c0dc2b0>\n"
     ]
    }
   ],
   "source": [
    "# Configure the LM (defaults to Qwen-30B via LM Studio)\n",
    "lm = configure_lm(\n",
    "    model=\"qwen/qwen3-coder-30b\",\n",
    "    api_base=\"http://host.docker.internal:1234/v1\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(f\"Configured LM: {lm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185bc4f7",
   "metadata": {},
   "source": [
    "## 2. Load Few-Shot Examples (Albert Einstein)\n",
    "\n",
    "Albert Einstein is our gold-standard example. These chunks and their extracted facts\n",
    "serve as few-shot demonstrations for the extractor and judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b715a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 63 chunks from Albert Einstein\n",
      "Loaded 19 fact sets\n",
      "Created 19 few-shot examples\n"
     ]
    }
   ],
   "source": [
    "# Load Albert Einstein data for few-shot examples\n",
    "fewshot_dir = Path(\"/workspaces/wiki3-kg-project/data/albert_einstein/20251218_231446\")\n",
    "\n",
    "fewshot_chunks = load_chunks_from_notebook(fewshot_dir / \"chunks.ipynb\")\n",
    "fewshot_facts = load_facts_from_notebook(fewshot_dir / \"facts.ipynb\")\n",
    "\n",
    "print(f\"Loaded {len(fewshot_chunks)} chunks from Albert Einstein\")\n",
    "print(f\"Loaded {len(fewshot_facts)} fact sets\")\n",
    "\n",
    "# Create few-shot examples\n",
    "fewshot_examples = create_training_examples(fewshot_chunks, fewshot_facts)\n",
    "print(f\"Created {len(fewshot_examples)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5911ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample few-shot example:\n",
      "  Context: Albert Einstein > Introduction\n",
      "  Text: Albert Einstein (14 March 1879 ‚Äì 18 April 1955) was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory...\n",
      "  Statements: 28 items\n",
      "    - Albert Einstein was a German-born theoretical physicist.\n",
      "    - Albert Einstein developed the theory of relativity.\n",
      "    - Albert Einstein made important contributions to quantum theory.\n"
     ]
    }
   ],
   "source": [
    "# Show a few-shot example\n",
    "if fewshot_examples:\n",
    "    ex = fewshot_examples[0]\n",
    "    print(\"Sample few-shot example:\")\n",
    "    print(f\"  Context: {ex.section_context}\")\n",
    "    print(f\"  Text: {ex.chunk_text[:200]}...\")\n",
    "    print(f\"  Statements: {len(ex.statements)} items\")\n",
    "    for stmt in ex.statements[:3]:\n",
    "        print(f\"    - {stmt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06944",
   "metadata": {},
   "source": [
    "## 3. Load Wikipedia Sample for Training\n",
    "\n",
    "Load the 100-page Wikipedia sample. We'll fetch article content and chunk it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9640f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 Wikipedia pages\n",
      "Sampling method: power_law\n",
      "\n",
      "First 10 pages:\n",
      "  - Zohran Mamdani (9,344,963 views)\n",
      "  - ChatGPT (3,639,485 views)\n",
      "  - James A. Garfield (3,524,531 views)\n",
      "  - 1989 Tiananmen Square protests and massacre (2,867,005 views)\n",
      "  - 2025 Bihar Legislative Assembly election (2,555,071 views)\n",
      "  - Mira Nair (2,503,516 views)\n",
      "  - Dick Cheney (2,186,840 views)\n",
      "  - 2026 FIFA World Cup (2,155,565 views)\n",
      "  - 1xBet (1,831,684 views)\n",
      "  - Survivor Series: WarGames (2025) (1,590,263 views)\n"
     ]
    }
   ],
   "source": [
    "# Load the Wikipedia sample\n",
    "sample_file = Path(\"/workspaces/wiki3-kg-project/data/training/wikipedia_sample.json\")\n",
    "\n",
    "with open(sample_file) as f:\n",
    "    wiki_sample = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(wiki_sample['pages'])} Wikipedia pages\")\n",
    "print(f\"Sampling method: {wiki_sample['metadata']['sampling_method']}\")\n",
    "print(f\"\\nFirst 10 pages:\")\n",
    "for p in wiki_sample['pages'][:10]:\n",
    "    print(f\"  - {p['title']} ({p['views']:,} views)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab17fa",
   "metadata": {},
   "source": [
    "## 4. Fetch and Chunk Wikipedia Pages\n",
    "\n",
    "For each page in our sample, fetch the content and break it into chunks.\n",
    "We use the same chunking strategy as the original pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be01db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 'Zohran Mamdani' -> 20 chunks\n",
      "First chunk: Zohran Kwame Mamdani (born October 18, 1991) is an American politician who is the mayor-elect of New York City. A member of the Democratic Party and the Democratic Socialists of America, he is set to ...\n"
     ]
    }
   ],
   "source": [
    "def fetch_wikipedia_content(title):\n",
    "    \"\"\"\n",
    "    Fetch the plain text content of a Wikipedia article.\n",
    "    \"\"\"\n",
    "    url = \"https://en.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "    headers = {\"User-Agent\": \"Wiki3.ai OntologicalEngineer/0.1\"}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        pages = data.get(\"query\", {}).get(\"pages\", {})\n",
    "        for page_id, page_data in pages.items():\n",
    "            if page_id == \"-1\":\n",
    "                return None\n",
    "            return page_data.get(\"extract\", \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error fetching {title}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def chunk_article(title, text, max_chunk_size=1500):\n",
    "    \"\"\"\n",
    "    Split article into chunks by section.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    # Split by section headers (== Header ==)\n",
    "    sections = re.split(r'\\n(==+\\s+.+?\\s+==+)\\n', text)\n",
    "    \n",
    "    current_section = title\n",
    "    current_text = \"\"\n",
    "    \n",
    "    for i, part in enumerate(sections):\n",
    "        # Check if this is a header\n",
    "        if re.match(r'==+\\s+.+?\\s+==+', part):\n",
    "            # Save previous chunk if it has content\n",
    "            if current_text.strip():\n",
    "                chunks.append({\n",
    "                    \"text\": current_text.strip(),\n",
    "                    \"section_context\": f\"{title} > {current_section}\",\n",
    "                })\n",
    "            \n",
    "            # Update section name\n",
    "            current_section = part.strip('= \\n')\n",
    "            current_text = \"\"\n",
    "        else:\n",
    "            current_text += part\n",
    "            \n",
    "            # If chunk gets too large, split it\n",
    "            while len(current_text) > max_chunk_size:\n",
    "                # Find a good break point (paragraph)\n",
    "                break_point = current_text.rfind('\\n\\n', 0, max_chunk_size)\n",
    "                if break_point == -1:\n",
    "                    break_point = current_text.rfind('. ', 0, max_chunk_size)\n",
    "                if break_point == -1:\n",
    "                    break_point = max_chunk_size\n",
    "                \n",
    "                chunk_text = current_text[:break_point].strip()\n",
    "                if chunk_text:\n",
    "                    chunks.append({\n",
    "                        \"text\": chunk_text,\n",
    "                        \"section_context\": f\"{title} > {current_section}\",\n",
    "                    })\n",
    "                current_text = current_text[break_point:].strip()\n",
    "    \n",
    "    # Don't forget the last chunk\n",
    "    if current_text.strip():\n",
    "        chunks.append({\n",
    "            \"text\": current_text.strip(),\n",
    "            \"section_context\": f\"{title} > {current_section}\",\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Test on one page\n",
    "test_title = wiki_sample['pages'][0]['title']\n",
    "test_content = fetch_wikipedia_content(test_title)\n",
    "if test_content:\n",
    "    test_chunks = chunk_article(test_title, test_content)\n",
    "    print(f\"Test: '{test_title}' -> {len(test_chunks)} chunks\")\n",
    "    if test_chunks:\n",
    "        print(f\"First chunk: {test_chunks[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886bea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content for 20 pages...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:07<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed 20 pages\n",
      "Total training chunks: 879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch and chunk all pages in sample (limit to first N for faster iteration)\n",
    "MAX_PAGES = 20  # Increase for full training run\n",
    "MIN_CHUNK_LENGTH = 100  # Skip very short chunks\n",
    "\n",
    "training_chunks = []\n",
    "pages_processed = 0\n",
    "\n",
    "print(f\"Fetching content for {min(MAX_PAGES, len(wiki_sample['pages']))} pages...\")\n",
    "\n",
    "for page in tqdm(wiki_sample['pages'][:MAX_PAGES]):\n",
    "    content = fetch_wikipedia_content(page['title'])\n",
    "    if content:\n",
    "        chunks = chunk_article(page['title'], content)\n",
    "        # Filter short chunks\n",
    "        chunks = [c for c in chunks if len(c['text']) >= MIN_CHUNK_LENGTH]\n",
    "        training_chunks.extend(chunks)\n",
    "        pages_processed += 1\n",
    "\n",
    "print(f\"\\nProcessed {pages_processed} pages\")\n",
    "print(f\"Total training chunks: {len(training_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf50dd2",
   "metadata": {},
   "source": [
    "## 5. Initialize Extractor with Few-Shot Examples\n",
    "\n",
    "Create the statement extractor and provide Albert Einstein examples as demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e38b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 few-shot examples:\n",
      "  1. Albert Einstein > Introduction... (28 statements)\n",
      "  2. Albert Einstein > Life and career > Personal views... (28 statements)\n",
      "  3. Albert Einstein > Introduction... (27 statements)\n"
     ]
    }
   ],
   "source": [
    "# Select best few-shot examples (ones with good variety of statements)\n",
    "NUM_FEWSHOT = 3\n",
    "\n",
    "# Sort by statement count to get diverse examples\n",
    "sorted_fewshot = sorted(fewshot_examples, key=lambda x: len(x.statements), reverse=True)\n",
    "selected_fewshot = sorted_fewshot[:NUM_FEWSHOT]\n",
    "\n",
    "print(f\"Selected {len(selected_fewshot)} few-shot examples:\")\n",
    "for i, ex in enumerate(selected_fewshot, 1):\n",
    "    print(f\"  {i}. {ex.section_context[:50]}... ({len(ex.statements)} statements)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9896eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor initialized\n",
      "Few-shot examples available: 3\n"
     ]
    }
   ],
   "source": [
    "# Create extractor with few-shot demonstrations\n",
    "extractor = StatementExtractor()\n",
    "\n",
    "# In DSPy, we can provide demonstrations directly\n",
    "# The few-shot examples will be used by MIPROv2 for bootstrapping\n",
    "print(\"Extractor initialized\")\n",
    "print(f\"Few-shot examples available: {len(selected_fewshot)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4419625",
   "metadata": {},
   "source": [
    "## 6. Test Extraction on Training Sample\n",
    "\n",
    "Run the extractor on a few training chunks to verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ea088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on: Zohran Mamdani > Zohran Mamdani\n",
      "Text: Zohran Kwame Mamdani (born October 18, 1991) is an American politician who is the mayor-elect of New York City. A member of the Democratic Party and the Democratic Socialists of America, he is set to become New York's first Muslim and Asian American mayor. Mamdani has served as a member of the New Y...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Extracted 22 statements:\n",
      "  1. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) was born on October 18, 1991.\n",
      "  2. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is an American politician.\n",
      "  3. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is the mayor-elect of [New York City](/wiki/New_York_City).\n",
      "  4. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is a member of the [Democratic Party](/wiki/Democratic_Party).\n",
      "  5. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is a member of the [Democratic Socialists of America](/wiki/Democratic_Socialists_of_America).\n",
      "  6. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is set to become [New York City](/wiki/New_York_City)'s first Muslim mayor.\n",
      "  7. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is set to become [New York City](/wiki/New_York_City)'s first Asian American mayor.\n",
      "  8. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) has served as a member of the [New York State Assembly](/wiki/New_York_State_Assembly) for the 36th district since 2021.\n",
      "  9. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) represents the [Queens](/wiki/Queens) neighborhood of [Astoria](/wiki/Astoria,_New_York).\n",
      "  10. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) was born in [Kampala](/wiki/Kampala), [Uganda](/wiki/Uganda).\n",
      "  ... and 12 more\n"
     ]
    }
   ],
   "source": [
    "# Test on a training chunk\n",
    "if training_chunks:\n",
    "    test_chunk = training_chunks[0]\n",
    "    \n",
    "    print(f\"Testing on: {test_chunk['section_context']}\")\n",
    "    print(f\"Text: {test_chunk['text'][:300]}...\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    result = extractor(\n",
    "        chunk_text=test_chunk['text'],\n",
    "        section_context=test_chunk['section_context'],\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracted {len(result.statements)} statements:\")\n",
    "    for i, stmt in enumerate(result.statements[:10], 1):\n",
    "        print(f\"  {i}. {stmt}\")\n",
    "    if len(result.statements) > 10:\n",
    "        print(f\"  ... and {len(result.statements) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a247a7a",
   "metadata": {},
   "source": [
    "## 7. Create Training Dataset\n",
    "\n",
    "Convert chunks into DSPy examples. For training, we need to generate initial extractions\n",
    "that can be scored and optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fab9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 703 examples\n",
      "Dev set: 176 examples\n"
     ]
    }
   ],
   "source": [
    "# Create training examples (without labels - we'll generate and judge them)\n",
    "# For DSPy optimization, we just need the inputs\n",
    "\n",
    "random_seed(42)  # For reproducibility\n",
    "shuffle(training_chunks)\n",
    "\n",
    "# Create DSPy examples from chunks\n",
    "trainset = []\n",
    "for chunk in training_chunks:\n",
    "    ex = dspy.Example(\n",
    "        chunk_text=chunk['text'],\n",
    "        section_context=chunk['section_context'],\n",
    "    ).with_inputs('chunk_text', 'section_context')\n",
    "    trainset.append(ex)\n",
    "\n",
    "# Split into train/dev\n",
    "split_idx = int(len(trainset) * 0.8)\n",
    "devset = trainset[split_idx:]\n",
    "trainset = trainset[:split_idx]\n",
    "\n",
    "print(f\"Training set: {len(trainset)} examples\")\n",
    "print(f\"Dev set: {len(devset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda75e24",
   "metadata": {},
   "source": [
    "## 8. Initialize Judge with Few-Shot Guidance\n",
    "\n",
    "The judge scores extraction quality. We use Albert Einstein examples to calibrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "792d6e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge calibration on few-shot example:\n",
      "  Completeness:      0.95\n",
      "  Atomicity:         0.85\n",
      "  Accuracy:          0.95\n",
      "  Link preservation: 1.00\n",
      "  ---\n",
      "  Weighted score:    0.94\n"
     ]
    }
   ],
   "source": [
    "# Initialize judge\n",
    "judge = StatementQualityJudge()\n",
    "\n",
    "# Test judge on a known good example (Albert Einstein few-shot)\n",
    "if selected_fewshot:\n",
    "    test_ex = selected_fewshot[0]\n",
    "    \n",
    "    evaluation = judge(\n",
    "        chunk_text=test_ex.chunk_text,\n",
    "        section_context=test_ex.section_context,\n",
    "        statements=test_ex.statements,\n",
    "    )\n",
    "    \n",
    "    print(\"Judge calibration on few-shot example:\")\n",
    "    print(f\"  Completeness:      {evaluation.completeness:.2f}\")\n",
    "    print(f\"  Atomicity:         {evaluation.atomicity:.2f}\")\n",
    "    print(f\"  Accuracy:          {evaluation.accuracy:.2f}\")\n",
    "    print(f\"  Link preservation: {evaluation.link_preservation:.2f}\")\n",
    "    print(f\"  ---\")\n",
    "    print(f\"  Weighted score:    {evaluation.weighted_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649d2fc",
   "metadata": {},
   "source": [
    "## 9. Baseline Evaluation\n",
    "\n",
    "Evaluate the unoptimized extractor on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f230a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.72 / 10 (87.2%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 261.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 01:50:20 INFO dspy.evaluate.evaluate: Average Metric: 8.724999999999998 / 10 (87.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Baseline quality score: 87.25\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline on dev set\n",
    "EVAL_SIZE = min(10, len(devset))  # Limit for speed\n",
    "\n",
    "evaluator = dspy.Evaluate(\n",
    "    devset=devset[:EVAL_SIZE],\n",
    "    metric=statement_quality_metric,\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    ")\n",
    "\n",
    "baseline_extractor = StatementExtractor()\n",
    "baseline_result = evaluator(baseline_extractor)\n",
    "\n",
    "baseline_score = baseline_result.score if hasattr(baseline_result, 'score') else float(baseline_result)\n",
    "print(f\"\\nBaseline quality score: {baseline_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e36a0",
   "metadata": {},
   "source": [
    "## 9b. MLflow Observability Setup\n",
    "\n",
    "MLflow provides tracing, evaluation, and human feedback tools for DSPy pipelines.\n",
    "\n",
    "### Quick Setup (One-time)\n",
    "\n",
    "1. **Install MLflow** (already in requirements or run cell below)\n",
    "2. **Start the MLflow server** in a terminal:\n",
    "   ```bash\n",
    "   cd /workspaces/wiki3-kg-project\n",
    "   mlflow server \\\n",
    "     --backend-store-uri sqlite:///mlflow.sqlite \\\n",
    "     --default-artifact-root ./mlflow-artifacts \\\n",
    "     --host 0.0.0.0 \\\n",
    "     --port 5000\n",
    "   ```\n",
    "3. **Open the UI** at http://localhost:5000 (or via VS Code port forwarding)\n",
    "\n",
    "### What MLflow Provides\n",
    "- **Tracing**: See every LM call, inputs, outputs, latency\n",
    "- **Evaluation**: Compare model versions side-by-side\n",
    "- **Human Feedback**: Add labels/assessments directly in the UI\n",
    "- **Experiment Tracking**: Track metrics across optimization runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87efb9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 01:50:21 INFO mlflow.tracking.fluent: Experiment with name 'wiki3-kg-stage1-statements' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MLflow configured successfully\n",
      "   Tracking URI: http://127.0.0.1:5000\n",
      "   Experiment: wiki3-kg-stage1-statements\n",
      "\n",
      "üìä Open MLflow UI: http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MLflow Setup\n",
    "# =============================================================================\n",
    "# Prerequisites:\n",
    "#   1. Install: pip install \"mlflow>=3.0\"\n",
    "#   2. Start server in terminal (or run: ./scripts/start_mlflow.sh):\n",
    "#      mlflow server --backend-store-uri sqlite:///mlflow.sqlite \\\n",
    "#                    --default-artifact-root ./mlflow-artifacts \\\n",
    "#                    --host 0.0.0.0 --port 5000\n",
    "#   3. Open UI: http://localhost:5000\n",
    "# =============================================================================\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Configure MLflow - use localhost for local server\n",
    "# For Docker: use host.docker.internal if MLflow runs on host\n",
    "MLFLOW_TRACKING_URI = \"http://127.0.0.1:5000\"\n",
    "\n",
    "try:\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(\"wiki3-kg-stage1-statements\")\n",
    "    \n",
    "    # Enable automatic DSPy tracing - captures all LM calls, modules, predictions\n",
    "    mlflow.dspy.autolog()\n",
    "    \n",
    "    print(f\"‚úÖ MLflow configured successfully\")\n",
    "    print(f\"   Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "    print(f\"   Experiment: wiki3-kg-stage1-statements\")\n",
    "    print(f\"\\nüìä Open MLflow UI: {MLFLOW_TRACKING_URI}\")\n",
    "    MLFLOW_ENABLED = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  MLflow not available: {e}\")\n",
    "    print(f\"\\nüí° To enable MLflow, start the server:\")\n",
    "    print(f\"   mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\")\n",
    "    MLFLOW_ENABLED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de34b2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation complete!\n",
      "   Average score: 0.87\n",
      "   Traces logged: 10\n",
      "\n",
      "üìä Review in MLflow UI: http://127.0.0.1:5000\n",
      "   ‚Üí Click 'Traces' tab to see all predictions\n",
      "   ‚Üí Click individual traces to review inputs/outputs\n",
      "   ‚Üí Use 'Feedback' to add human labels\n",
      "üèÉ View run baseline_evaluation at: http://127.0.0.1:5000/#/experiments/1/runs/aa12844cba7a46b28725ae1e0207729a\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-6694c34310ba58e3d2762bdc1d34d08e&amp;experiment_id=1&amp;trace_id=tr-715629eee893be3d7354ea6f61607459&amp;experiment_id=1&amp;trace_id=tr-735435ea68949b8d00af5b3a2812859a&amp;experiment_id=1&amp;trace_id=tr-a5cb63a2398d1ca68b6870b51d61fac3&amp;experiment_id=1&amp;trace_id=tr-1497d6587010f7197e695d0d8a3c3b5e&amp;experiment_id=1&amp;trace_id=tr-7872bdeb2cd94cbbc19ad58cc35b1c8c&amp;experiment_id=1&amp;trace_id=tr-f1f8343ea99f131849c8a43f7ed70ed7&amp;experiment_id=1&amp;trace_id=tr-c1a6423b9f64eeed5c9d927d84b871bb&amp;experiment_id=1&amp;trace_id=tr-17dc8eff687213f98d60593603802b70&amp;experiment_id=1&amp;trace_id=tr-ebff8d1530cbd7556232b17a25074181&amp;experiment_id=1&amp;version=3.7.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-6694c34310ba58e3d2762bdc1d34d08e), Trace(trace_id=tr-715629eee893be3d7354ea6f61607459), Trace(trace_id=tr-735435ea68949b8d00af5b3a2812859a), Trace(trace_id=tr-a5cb63a2398d1ca68b6870b51d61fac3), Trace(trace_id=tr-1497d6587010f7197e695d0d8a3c3b5e), Trace(trace_id=tr-7872bdeb2cd94cbbc19ad58cc35b1c8c), Trace(trace_id=tr-f1f8343ea99f131849c8a43f7ed70ed7), Trace(trace_id=tr-c1a6423b9f64eeed5c9d927d84b871bb), Trace(trace_id=tr-17dc8eff687213f98d60593603802b70), Trace(trace_id=tr-ebff8d1530cbd7556232b17a25074181)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run evaluation with MLflow tracing (if enabled)\n",
    "# Each prediction creates a trace viewable in the MLflow UI\n",
    "\n",
    "if MLFLOW_ENABLED:\n",
    "    with mlflow.start_run(run_name=\"baseline_evaluation\"):\n",
    "        # Log parameters for reproducibility\n",
    "        mlflow.log_param(\"eval_size\", EVAL_SIZE)\n",
    "        mlflow.log_param(\"model\", \"qwen/qwen3-coder-30b\")\n",
    "        mlflow.log_param(\"num_fewshot\", NUM_FEWSHOT)\n",
    "        \n",
    "        # Run extractions on dev set - each one is traced\n",
    "        results = []\n",
    "        for i, ex in enumerate(tqdm(devset[:EVAL_SIZE], desc=\"Evaluating\")):\n",
    "            with mlflow.start_span(name=f\"example_{i}\") as span:\n",
    "                # Run extraction\n",
    "                pred = baseline_extractor(\n",
    "                    chunk_text=ex.chunk_text,\n",
    "                    section_context=ex.section_context,\n",
    "                )\n",
    "                \n",
    "                # Run judge\n",
    "                eval_result = judge(\n",
    "                    chunk_text=ex.chunk_text,\n",
    "                    section_context=ex.section_context,\n",
    "                    statements=pred.statements,\n",
    "                )\n",
    "                \n",
    "                # Log to span for MLflow UI review\n",
    "                span.set_inputs({\n",
    "                    \"chunk_text\": ex.chunk_text[:500],\n",
    "                    \"section_context\": ex.section_context,\n",
    "                })\n",
    "                span.set_outputs({\n",
    "                    \"statements\": list(pred.statements),\n",
    "                    \"completeness\": float(eval_result.completeness),\n",
    "                    \"atomicity\": float(eval_result.atomicity),\n",
    "                    \"accuracy\": float(eval_result.accuracy),\n",
    "                    \"link_preservation\": float(eval_result.link_preservation),\n",
    "                    \"weighted_score\": float(eval_result.weighted_score),\n",
    "                    \"reasoning\": eval_result.reasoning,\n",
    "                })\n",
    "                \n",
    "                results.append({\n",
    "                    \"index\": i,\n",
    "                    \"score\": float(eval_result.weighted_score),\n",
    "                })\n",
    "        \n",
    "        # Log aggregate metrics\n",
    "        avg_score = sum(r[\"score\"] for r in results) / len(results)\n",
    "        mlflow.log_metric(\"avg_quality_score\", avg_score)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Evaluation complete!\")\n",
    "        print(f\"   Average score: {avg_score:.2f}\")\n",
    "        print(f\"   Traces logged: {len(results)}\")\n",
    "        print(f\"\\nüìä Review in MLflow UI: {MLFLOW_TRACKING_URI}\")\n",
    "        print(f\"   ‚Üí Click 'Traces' tab to see all predictions\")\n",
    "        print(f\"   ‚Üí Click individual traces to review inputs/outputs\")\n",
    "        print(f\"   ‚Üí Use 'Feedback' to add human labels\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping MLflow evaluation (server not running)\")\n",
    "    print(\"   Run baseline evaluation with dspy.Evaluate instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "794a21f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent LM calls (use MLflow UI for full traces):\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-12-20T01:50:23.782466]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `chunk_text` (str): Wikipedia article chunk with markdown links preserved\n",
      "2. `section_context` (str): Breadcrumb showing location: Article > Section > Subsection\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `statements` (list[str]): List of atomic statements, each preserving [Entity](/wiki/...) links\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## chunk_text ## ]]\n",
      "{chunk_text}\n",
      "\n",
      "[[ ## section_context ## ]]\n",
      "{section_context}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## statements ## ]]\n",
      "{statements}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Extract atomic, verifiable statements from Wikipedia text.\n",
      "        \n",
      "        Each statement must:\n",
      "        - Be self-contained (understandable without the original text)\n",
      "        - Preserve markdown links: [Entity Name](/wiki/Entity_Name)\n",
      "        - Contain exactly one verifiable claim\n",
      "        - Not editorialize or interpret beyond what's stated\n",
      "        \n",
      "        Example input chunk:\n",
      "            \"Albert Einstein was born in Ulm, in the Kingdom of W√ºrttemberg \n",
      "            in the German Empire, on 14 March 1879.\"\n",
      "        \n",
      "        Example output statements:\n",
      "            - \"[Albert Einstein](/wiki/Albert_Einstein) was born in [Ulm](/wiki/Ulm).\"\n",
      "            - \"[Albert Einstein](/wiki/Albert_Einstein) was born on 14 March 1879.\"\n",
      "            - \"[Ulm](/wiki/Ulm) was in the [Kingdom of W√ºrttemberg](/wiki/Kingdom_of_W√ºrttemberg).\"\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## chunk_text ## ]]\n",
      "Kirk was the author of several books. Along with Brent Hamachek, he co-wrote the 2016 book Time for a Turning Point: Setting a Course Toward Free Markets and Limited Government for Future Generations, which was published by Post Hill Press, a subsidiary of Simon & Schuster. Under the same publisher, Kirk wrote the 2018 book Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters. Donald Trump Jr. wrote the foreword for the book. In a review for The Weekly Standard, Adam Rubenstein described the book as a \"hot mess\", \"nothing more than a marketing pitch for TPUSA\", and said the \"thin\" book was \"stuffed with reprintings of his tweets and quotes from others\".\n",
      "In 2020, Kirk wrote The MAGA Doctrine: The Only Ideas That Will Win the Future, which was published by Harper Collins. In its review for The New York Times, Gabriel Debenedetti wrote that \"Kirk's musing about whether 'The Art of The Deal' might one day be considered a 'religious tract' comes just nine chapters after the book highlights the importance of 'a healthy dose of skepticism about authority figures and experts who think they knew best.' And that's just pages after its dedication to Donald Trump, which is five chapters before Kirk wonders whether Trump might 'be remembered as the president who brought about world peace.'\" In Open Letters Review, Steve Donoghue said of the book that \"On every page, Kirk writes something that's either trivially, casually wrong ..\n",
      "\n",
      "[[ ## section_context ## ]]\n",
      "Charlie Kirk > Books\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## statements ## ]]` (must be formatted as a valid Python list[str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The chunk text contains several verifiable claims about Charlie Kirk's books, their authors, publishers, reviews, and content. Each sentence or group of sentences forms a distinct atomic statement that can stand alone and be verified independently. The statements must preserve markdown links if present (though none are in this case), and each must contain exactly one verifiable claim.\n",
      "\n",
      "The key claims to extract include:\n",
      "1. Kirk authored several books.\n",
      "2. He co-wrote a 2016 book with Brent Hamachek titled \"Time for a Turning Point...\" published by Post Hill Press.\n",
      "3. He wrote another 2018 book titled \"Campus Battlefield...\" under the same publisher.\n",
      "4. Donald Trump Jr. wrote the foreword for \"Campus Battlefield...\".\n",
      "5. Adam Rubenstein reviewed \"Campus Battlefield...\" and called it a \"hot mess\" and criticized its content.\n",
      "6. In 2020, Kirk wrote \"The MAGA Doctrine...\" published by Harper Collins.\n",
      "7. Gabriel Debenedetti reviewed \"The MAGA Doctrine...\" for The New York Times and criticized its inconsistencies.\n",
      "8. Steve Donoghue reviewed \"The MAGA Doctrine...\" for Open Letters Review and criticized its accuracy.\n",
      "\n",
      "[[ ## statements ## ]]\n",
      "[\n",
      "    \"[Charlie Kirk](/wiki/Charlie_Kirk) was the author of several books.\",\n",
      "    \"[Charlie Kirk](/wiki/Charlie_Kirk) co-wrote the 2016 book [Time for a Turning Point: Setting a Course Toward Free Markets and Limited Government for Future Generations](/wiki/Time_for_a_Turning_Point) with [Brent Hamachek](/wiki/Brent_Hamachek).\",\n",
      "    \"The 2016 book [Time for a Turning Point: Setting a Course Toward Free Markets and Limited Government for Future Generations](/wiki/Time_for_a_Turning_Point) was published by [Post Hill Press](/wiki/Post_Hill_Press).\",\n",
      "    \"[Post Hill Press](/wiki/Post_Hill_Press) is a subsidiary of [Simon & Schuster](/wiki/Simon_%26_Schuster).\",\n",
      "    \"[Charlie Kirk](/wiki/Charlie_Kirk) wrote the 2018 book [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield).\",\n",
      "    \"[Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) was published by [Post Hill Press](/wiki/Post_Hill_Press).\",\n",
      "    \"[Donald Trump Jr.](/wiki/Donald_Trump_Jr.) wrote the foreword for [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield).\",\n",
      "    \"In a review for [The Weekly Standard](/wiki/The_Weekly_Standard), [Adam Rubenstein](/wiki/Adam_Rubenstein) described [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) as a 'hot mess'.\",\n",
      "    \"In a review for [The Weekly Standard](/wiki/The_Weekly_Standard), [Adam Rubenstein](/wiki/Adam_Rubenstein) said [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) was 'nothing more than a marketing pitch for TPUSA'.\",\n",
      "    \"In a review for [The Weekly Standard](/wiki/The_Weekly_Standard), [Adam Rubenstein](/wiki/Adam_Rubenstein) said the 'thin' book [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) was 'stuffed with reprintings of his tweets and quotes from others'.\",\n",
      "    \"[Charlie Kirk](/wiki/Charlie_Kirk) wrote the 2020 book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine).\",\n",
      "    \"[The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) was published by [Harper Collins](/wiki/Harper_Collins).\",\n",
      "    \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that [Charlie Kirk](/wiki/Charlie_Kirk)'s musing about whether '[The Art of The Deal](/wiki/The_Art_of_the_Deal)' might one day be considered a 'religious tract' comes just nine chapters after the book highlights the importance of 'a healthy dose of skepticism about authority figures and experts who think they knew best'.\",\n",
      "    \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) highlights the importance of 'a healthy dose of skepticism about authority figures and experts who think they knew best.'\",\n",
      "    \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) dedicates itself to [Donald Trump](/wiki/Donald_Trump).\",\n",
      "    \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) highlights that [Donald Trump](/wiki/Donald_Trump) might 'be remembered as the president who brought about world peace.'\",\n",
      "    \"In [Open Letters Review](/wiki/Open_Letters_Review), [Steve Donoghue](/wiki/Steve_Donoghue) said of the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) that 'On every page, [Charlie Kirk](/wiki/Charlie_Kirk) writes something that's either trivially, casually wrong.'\"\n",
      "]\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-12-20T01:50:23.847275]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `chunk_text` (str): Original Wikipedia chunk\n",
      "2. `section_context` (str): Section breadcrumb for context\n",
      "3. `statements` (list[str]): Extracted statements to evaluate\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): Brief explanation of the scores given\n",
      "2. `completeness` (float): Score 0-1: Are all key facts from the chunk captured?\n",
      "3. `atomicity` (float): Score 0-1: Is each statement truly atomic (one claim)?\n",
      "4. `accuracy` (float): Score 0-1: Do statements faithfully represent the source?\n",
      "5. `link_preservation` (float): Score 0-1: Are [Entity](/wiki/...) links preserved correctly?\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## chunk_text ## ]]\n",
      "{chunk_text}\n",
      "\n",
      "[[ ## section_context ## ]]\n",
      "{section_context}\n",
      "\n",
      "[[ ## statements ## ]]\n",
      "{statements}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## completeness ## ]]\n",
      "{completeness}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## atomicity ## ]]\n",
      "{atomicity}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## accuracy ## ]]\n",
      "{accuracy}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## link_preservation ## ]]\n",
      "{link_preservation}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Judge the quality of extracted statements from a Wikipedia chunk.\n",
      "        \n",
      "        Evaluate on four dimensions:\n",
      "        1. Completeness: Are all key facts from the chunk captured?\n",
      "        2. Atomicity: Is each statement truly atomic (one verifiable claim)?\n",
      "        3. Accuracy: Do statements faithfully represent the source without adding info?\n",
      "        4. Link preservation: Are [Entity](/wiki/...) links preserved correctly?\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## chunk_text ## ]]\n",
      "Kirk was the author of several books. Along with Brent Hamachek, he co-wrote the 2016 book Time for a Turning Point: Setting a Course Toward Free Markets and Limited Government for Future Generations, which was published by Post Hill Press, a subsidiary of Simon & Schuster. Under the same publisher, Kirk wrote the 2018 book Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters. Donald Trump Jr. wrote the foreword for the book. In a review for The Weekly Standard, Adam Rubenstein described the book as a \"hot mess\", \"nothing more than a marketing pitch for TPUSA\", and said the \"thin\" book was \"stuffed with reprintings of his tweets and quotes from others\".\n",
      "In 2020, Kirk wrote The MAGA Doctrine: The Only Ideas That Will Win the Future, which was published by Harper Collins. In its review for The New York Times, Gabriel Debenedetti wrote that \"Kirk's musing about whether 'The Art of The Deal' might one day be considered a 'religious tract' comes just nine chapters after the book highlights the importance of 'a healthy dose of skepticism about authority figures and experts who think they knew best.' And that's just pages after its dedication to Donald Trump, which is five chapters before Kirk wonders whether Trump might 'be remembered as the president who brought about world peace.'\" In Open Letters Review, Steve Donoghue said of the book that \"On every page, Kirk writes something that's either trivially, casually wrong ..\n",
      "\n",
      "[[ ## section_context ## ]]\n",
      "Charlie Kirk > Books\n",
      "\n",
      "[[ ## statements ## ]]\n",
      "[\"[Charlie Kirk](/wiki/Charlie_Kirk) was the author of several books.\", \"[Charlie Kirk](/wiki/Charlie_Kirk) co-wrote the 2016 book [Time for a Turning Point: Setting a Course Toward Free Markets and Limited Government for Future Generations](/wiki/Time_for_a_Turning_Point) with [Brent Hamachek](/wiki/Brent_Hamachek).\", \"The 2016 book [Time for a Turning Point: Setting a Course Toward Free Markets and Limited Government for Future Generations](/wiki/Time_for_a_Turning_Point) was published by [Post Hill Press](/wiki/Post_Hill_Press).\", \"[Post Hill Press](/wiki/Post_Hill_Press) is a subsidiary of [Simon & Schuster](/wiki/Simon_%26_Schuster).\", \"[Charlie Kirk](/wiki/Charlie_Kirk) wrote the 2018 book [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield).\", \"[Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) was published by [Post Hill Press](/wiki/Post_Hill_Press).\", \"[Donald Trump Jr.](/wiki/Donald_Trump_Jr.) wrote the foreword for [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield).\", \"In a review for [The Weekly Standard](/wiki/The_Weekly_Standard), [Adam Rubenstein](/wiki/Adam_Rubenstein) described [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) as a 'hot mess'.\", \"In a review for [The Weekly Standard](/wiki/The_Weekly_Standard), [Adam Rubenstein](/wiki/Adam_Rubenstein) said [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) was 'nothing more than a marketing pitch for TPUSA'.\", \"In a review for [The Weekly Standard](/wiki/The_Weekly_Standard), [Adam Rubenstein](/wiki/Adam_Rubenstein) said the 'thin' book [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) was 'stuffed with reprintings of his tweets and quotes from others'.\", \"[Charlie Kirk](/wiki/Charlie_Kirk) wrote the 2020 book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine).\", \"[The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) was published by [Harper Collins](/wiki/Harper_Collins).\", \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that [Charlie Kirk](/wiki/Charlie_Kirk)'s musing about whether '[The Art of The Deal](/wiki/The_Art_of_the_Deal)' might one day be considered a 'religious tract' comes just nine chapters after the book highlights the importance of 'a healthy dose of skepticism about authority figures and experts who think they knew best'.\", \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) highlights the importance of 'a healthy dose of skepticism about authority figures and experts who think they knew best.'\", \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) dedicates itself to [Donald Trump](/wiki/Donald_Trump).\", \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) highlights that [Donald Trump](/wiki/Donald_Trump) might 'be remembered as the president who brought about world peace.'\", \"In [Open Letters Review](/wiki/Open_Letters_Review), [Steve Donoghue](/wiki/Steve_Donoghue) said of the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) that 'On every page, [Charlie Kirk](/wiki/Charlie_Kirk) writes something that's either trivially, casually wrong.'\"]\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## completeness ## ]]` (must be formatted as a valid Python float), then `[[ ## atomicity ## ]]` (must be formatted as a valid Python float), then `[[ ## accuracy ## ]]` (must be formatted as a valid Python float), then `[[ ## link_preservation ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The extracted statements capture most of the key facts from the chunk, including the books authored by Charlie Kirk, their publication details, publishers, forewords, and reviews. However, some nuances are lost in translation, particularly in the review excerpts where the full context of the criticism is not fully represented. The statements are generally atomic, each containing a single verifiable claim. Links are preserved correctly for all entities mentioned. Accuracy is high, though some statements slightly paraphrase the original text, which may affect precision.\n",
      "\n",
      "[[ ## completeness ## ]]\n",
      "0.9\n",
      "\n",
      "[[ ## atomicity ## ]]\n",
      "0.95\n",
      "\n",
      "[[ ## accuracy ## ]]\n",
      "0.9\n",
      "\n",
      "[[ ## link_preservation ## ]]\n",
      "1.0\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Use dspy.inspect_history() for quick debugging\n",
    "# This shows recent LM calls without needing MLflow server\n",
    "\n",
    "print(\"Recent LM calls (use MLflow UI for full traces):\")\n",
    "print(\"=\" * 60)\n",
    "dspy.inspect_history(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d708ef8",
   "metadata": {},
   "source": [
    "### MLflow Evaluation with Human Feedback\n",
    "\n",
    "Use MLflow's evaluation API to systematically review predictions and collect human labels.\n",
    "The MLflow UI provides a proper interface for reviewing and annotating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de353ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation dataset with 10 examples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>section_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>In an interview with Wired magazine during the...</td>\n",
       "      <td>Charlie Kirk &gt; Republican and pro-Trump activism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>. He has described his upbringing as \"privileg...</td>\n",
       "      <td>Zohran Mamdani &gt; Early life and education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Wikipedia has spawned several sister projects,...</td>\n",
       "      <td>Wikipedia &gt; Sister projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>. Throughout the demonstrations, broadcasts by...</td>\n",
       "      <td>1989 Tiananmen Square protests and massacre &gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The visual effects for Wicked were made by Ind...</td>\n",
       "      <td>Wicked (2024 film) &gt; Post-production and visua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         chunk_text  \\\n",
       "0      0  In an interview with Wired magazine during the...   \n",
       "1      1  . He has described his upbringing as \"privileg...   \n",
       "2      2  Wikipedia has spawned several sister projects,...   \n",
       "3      3  . Throughout the demonstrations, broadcasts by...   \n",
       "4      4  The visual effects for Wicked were made by Ind...   \n",
       "\n",
       "                                     section_context  \n",
       "0   Charlie Kirk > Republican and pro-Trump activism  \n",
       "1          Zohran Mamdani > Early life and education  \n",
       "2                        Wikipedia > Sister projects  \n",
       "3  1989 Tiananmen Square protests and massacre > ...  \n",
       "4  Wicked (2024 film) > Post-production and visua...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create evaluation dataset for MLflow\n",
    "import pandas as pd\n",
    "\n",
    "eval_data = []\n",
    "for i, ex in enumerate(devset[:EVAL_SIZE]):\n",
    "    eval_data.append({\n",
    "        \"index\": i,\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"section_context\": ex.section_context,\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "print(f\"Created evaluation dataset with {len(eval_df)} examples\")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b02ec641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/deprecated.py:9: FutureWarning: The `mlflow.evaluate` API has been deprecated as of MLflow 3.0.0. Please use these new alternatives:\n",
      "\n",
      " - For traditional ML or deep learning models: Use `mlflow.models.evaluate`, which maintains full compatibility with the original `mlflow.evaluate` API.\n",
      "\n",
      " - For LLMs or GenAI applications: Use the new `mlflow.genai.evaluate` API, which offers enhanced features specifically designed for evaluating LLMs and GenAI applications.\n",
      "\n",
      "  warnings.warn(\n",
      "/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/12/20 01:50:24 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "2025/12/20 01:50:24 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n",
      "2025/12/20 01:50:32 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run mlflow_evaluation at: http://127.0.0.1:5000/#/experiments/1/runs/059f8afc91b2416d81056c83f9c6071e\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "Metric 'quality': Error:\nKeyError('chunk_text')\nTraceback (most recent call last):\n  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7096, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'chunk_text'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py\", line 709, in _test_first_row\n    metric_value = metric.evaluate(eval_fn_args)\n  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/utils/metric.py\", line 64, in evaluate\n    metric: MetricValue = self.function(*eval_fn_args)\n  File \"/tmp/ipykernel_92526/898853016.py\", line 15, in quality_metric\n    chunk_text=row[\"chunk_text\"],\n  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/pandas/core/frame.py\", line 4113, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3819, in get_loc\n    raise KeyError(key) from err\nKeyError: 'chunk_text'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 28\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleteness\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(eval_result\u001b[38;5;241m.\u001b[39mcompleteness),\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124matomicity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(eval_result\u001b[38;5;241m.\u001b[39matomicity),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted_score\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(eval_result\u001b[38;5;241m.\u001b[39mweighted_score),\n\u001b[1;32m     25\u001b[0m     }\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlflow_evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_statements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_metric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                \u001b[49m\u001b[43meval_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquality_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ MLflow evaluation complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   View results in MLflow UI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMLFLOW_TRACKING_URI\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/deprecated.py:19\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(model_evaluate)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      9\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `mlflow.evaluate` API has been deprecated as of MLflow 3.0.0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use these new alternatives:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m     )\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/base.py:1791\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data, model_type, targets, predictions, dataset_path, feature_names, evaluators, evaluator_config, extra_metrics, custom_artifacts, env_manager, model_config, inference_params, model_id, _called_from_genai_evaluate)\u001b[0m\n\u001b[1;32m   1788\u001b[0m predictions_expected_in_model_output \u001b[38;5;241m=\u001b[39m predictions \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1791\u001b[0m     evaluate_result \u001b[38;5;241m=\u001b[39m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_name_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions_expected_in_model_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, _ServedPyFuncModel):\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/telemetry/track.py:30\u001b[0m, in \u001b[0;36mrecord_usage_event.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result  \u001b[38;5;66;03m# noqa: RET504\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/base.py:1032\u001b[0m, in \u001b[0;36m_evaluate\u001b[0;34m(model, model_type, model_id, dataset, run_id, evaluator_name_list, evaluator_name_to_conf_map, extra_metrics, custom_artifacts, predictions, evaluators)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mcan_evaluate(model_type\u001b[38;5;241m=\u001b[39mmodel_type, evaluator_config\u001b[38;5;241m=\u001b[39meval_\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m configure_autologging_for_evaluation(enable_tracing\u001b[38;5;241m=\u001b[39mshould_enable_tracing):\n\u001b[0;32m-> 1032\u001b[0m         eval_result \u001b[38;5;241m=\u001b[39m \u001b[43meval_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m         eval_results\u001b[38;5;241m.\u001b[39mappend(eval_result)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py:943\u001b[0m, in \u001b[0;36mBuiltInEvaluator.evaluate\u001b[0;34m(self, model_type, dataset, run_id, evaluator_config, model, extra_metrics, custom_artifacts, predictions, model_id, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TempDir() \u001b[38;5;28;01mas\u001b[39;00m temp_dir, matplotlib\u001b[38;5;241m.\u001b[39mrc_context(_matplotlib_config):\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemp_dir \u001b[38;5;241m=\u001b[39m temp_dir\n\u001b[0;32m--> 943\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/evaluators/default.py:77\u001b[0m, in \u001b[0;36mDefaultEvaluator._evaluate\u001b[0;34m(self, model, extra_metrics, custom_artifacts, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m y_true \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mlabels_data\n\u001b[1;32m     76\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_builtin_metrics() \u001b[38;5;241m+\u001b[39m extra_metrics\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mother_output_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mother_model_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_and_log_custom_artifacts(custom_artifacts, prediction\u001b[38;5;241m=\u001b[39my_pred, target\u001b[38;5;241m=\u001b[39my_true)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Log metrics and artifacts\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py:751\u001b[0m, in \u001b[0;36mBuiltInEvaluator.evaluate_metrics\u001b[0;34m(self, metrics, prediction, target, other_output_df)\u001b[0m\n\u001b[1;32m    746\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    747\u001b[0m     MetricDefinition\u001b[38;5;241m.\u001b[39mfrom_index_and_metric(i, metric) \u001b[38;5;28;01mfor\u001b[39;00m i, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(metrics)\n\u001b[1;32m    748\u001b[0m ]\n\u001b[1;32m    749\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_order_metrics(metrics, eval_df, other_output_df)\n\u001b[0;32m--> 751\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_first_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_output_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# calculate metrics for the full eval_df\u001b[39;00m\n\u001b[1;32m    754\u001b[0m input_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mcopy_to_avoid_mutation()\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py:723\u001b[0m, in \u001b[0;36mBuiltInEvaluator._test_first_row\u001b[0;34m(self, metrics, eval_df, other_output_df)\u001b[0m\n\u001b[1;32m    720\u001b[0m             exceptions\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: Error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstacktrace_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exceptions) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 723\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(exceptions))\n",
      "\u001b[0;31mMlflowException\u001b[0m: Metric 'quality': Error:\nKeyError('chunk_text')\nTraceback (most recent call last):\n  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7096, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'chunk_text'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/default_evaluator.py\", line 709, in _test_first_row\n    metric_value = metric.evaluate(eval_fn_args)\n  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/models/evaluation/utils/metric.py\", line 64, in evaluate\n    metric: MetricValue = self.function(*eval_fn_args)\n  File \"/tmp/ipykernel_92526/898853016.py\", line 15, in quality_metric\n    chunk_text=row[\"chunk_text\"],\n  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/pandas/core/frame.py\", line 4113, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 3819, in get_loc\n    raise KeyError(key) from err\nKeyError: 'chunk_text'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-260a5962dd81b7f57d5911c6a8f1e091&amp;experiment_id=1&amp;version=3.7.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-260a5962dd81b7f57d5911c6a8f1e091)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run MLflow evaluation with custom metrics\n",
    "# This creates a structured evaluation run with results viewable in UI\n",
    "\n",
    "def extract_statements(row):\n",
    "    \"\"\"Model function for MLflow evaluate.\"\"\"\n",
    "    pred = baseline_extractor(\n",
    "        chunk_text=row[\"chunk_text\"],\n",
    "        section_context=row[\"section_context\"],\n",
    "    )\n",
    "    return {\"statements\": list(pred.statements)}\n",
    "\n",
    "def quality_metric(row, output):\n",
    "    \"\"\"Custom metric using our judge.\"\"\"\n",
    "    eval_result = judge(\n",
    "        chunk_text=row[\"chunk_text\"],\n",
    "        section_context=row[\"section_context\"],\n",
    "        statements=output[\"statements\"],\n",
    "    )\n",
    "    return {\n",
    "        \"completeness\": float(eval_result.completeness),\n",
    "        \"atomicity\": float(eval_result.atomicity),\n",
    "        \"accuracy\": float(eval_result.accuracy),\n",
    "        \"link_preservation\": float(eval_result.link_preservation),\n",
    "        \"weighted_score\": float(eval_result.weighted_score),\n",
    "    }\n",
    "\n",
    "with mlflow.start_run(run_name=\"mlflow_evaluation\"):\n",
    "    eval_results = mlflow.evaluate(\n",
    "        model=extract_statements,\n",
    "        data=eval_df,\n",
    "        extra_metrics=[\n",
    "            mlflow.metrics.make_metric(\n",
    "                eval_fn=quality_metric,\n",
    "                name=\"quality\",\n",
    "                greater_is_better=True,\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ MLflow evaluation complete!\")\n",
    "    print(f\"   View results in MLflow UI: {MLFLOW_TRACKING_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View evaluation results summary\n",
    "if eval_results:\n",
    "    print(\"Evaluation Metrics:\")\n",
    "    print(\"=\" * 40)\n",
    "    for metric_name, value in eval_results.metrics.items():\n",
    "        print(f\"  {metric_name}: {value}\")\n",
    "    \n",
    "    print(f\"\\nüìä Full results table available in MLflow UI\")\n",
    "    print(f\"   - Click on the run to see detailed traces\")\n",
    "    print(f\"   - Use the Evaluation tab for side-by-side comparison\")\n",
    "    print(f\"   - Add human labels directly in the UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c283f",
   "metadata": {},
   "source": [
    "### Export Annotations from MLflow\n",
    "\n",
    "After reviewing and labeling in the MLflow UI, export your annotations for judge improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from MLflow (after you've labeled them in the UI)\n",
    "# MLflow stores feedback as assessments on traces\n",
    "\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "# Get the latest evaluation run\n",
    "experiment = client.get_experiment_by_name(\"wiki3-kg-stage1-statements\")\n",
    "if experiment:\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"start_time DESC\"],\n",
    "        max_results=1,\n",
    "    )\n",
    "    \n",
    "    if runs:\n",
    "        latest_run = runs[0]\n",
    "        print(f\"Latest run: {latest_run.info.run_id}\")\n",
    "        print(f\"Metrics: {latest_run.data.metrics}\")\n",
    "        \n",
    "        # Get traces with assessments (human feedback)\n",
    "        try:\n",
    "            traces = client.search_traces(\n",
    "                experiment_ids=[experiment.experiment_id],\n",
    "                max_results=100,\n",
    "            )\n",
    "            print(f\"Found {len(traces)} traces\")\n",
    "        except Exception as e:\n",
    "            print(f\"Trace search error: {e}\")\n",
    "else:\n",
    "    print(\"No experiment found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52239b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use human feedback to improve the judge\n",
    "# After collecting labels in MLflow, create DSPy training examples\n",
    "\n",
    "# For now, save the evaluation data for later use\n",
    "output_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "eval_df.to_json(output_dir / \"eval_dataset.json\", orient=\"records\", indent=2)\n",
    "print(f\"Saved evaluation dataset to {output_dir / 'eval_dataset.json'}\")\n",
    "\n",
    "print(\"\"\"\n",
    "üìã Next steps for human feedback:\n",
    "\n",
    "1. Start MLflow server:\n",
    "   mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\n",
    "\n",
    "2. Open MLflow UI at http://127.0.0.1:5000\n",
    "\n",
    "3. Navigate to the experiment 'wiki3-kg-stage1-statements'\n",
    "\n",
    "4. Click on traces to review predictions\n",
    "\n",
    "5. Use the feedback/assessment features to label quality\n",
    "\n",
    "6. Export labeled data for judge improvement\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b175b",
   "metadata": {},
   "source": [
    "## 10. MIPROv2 Prompt Optimization\n",
    "\n",
    "Use DSPy's MIPROv2 optimizer to improve the extractor's prompts.\n",
    "This uses the few-shot examples to bootstrap better demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f711918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "# Configure optimizer\n",
    "optimizer = MIPROv2(\n",
    "    metric=statement_quality_metric,\n",
    "    num_candidates=3,  # Number of prompt candidates to try\n",
    "    init_temperature=0.7,\n",
    ")\n",
    "\n",
    "# Use few-shot examples for bootstrapping demonstrations\n",
    "# Use training set for optimization\n",
    "TRAIN_SIZE = min(20, len(trainset))  # Limit for speed\n",
    "\n",
    "print(f\"Optimizing with {TRAIN_SIZE} training examples...\")\n",
    "print(f\"Using {len(selected_fewshot)} few-shot demos for bootstrapping...\")\n",
    "\n",
    "optimized_extractor = optimizer.compile(\n",
    "    StatementExtractor(),\n",
    "    trainset=trainset[:TRAIN_SIZE],\n",
    "    num_batches=2,\n",
    "    max_bootstrapped_demos=NUM_FEWSHOT,\n",
    "    # Provide few-shot examples as initial demos\n",
    "    # demos=selected_fewshot,  # Uncomment if supported\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d59aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate optimized extractor\n",
    "optimized_result = evaluator(optimized_extractor)\n",
    "optimized_score = optimized_result.score if hasattr(optimized_result, 'score') else float(optimized_result)\n",
    "\n",
    "print(f\"Baseline score:  {baseline_score:.2f}\")\n",
    "print(f\"Optimized score: {optimized_score:.2f}\")\n",
    "print(f\"Improvement:     {optimized_score - baseline_score:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fab06",
   "metadata": {},
   "source": [
    "## 11. Inspect Optimized Prompts\n",
    "\n",
    "See what prompts MIPROv2 discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a08e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the optimized module\n",
    "print(\"Optimized extractor configuration:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to access the optimized signature/demos\n",
    "if hasattr(optimized_extractor, 'demos'):\n",
    "    print(f\"\\nDemonstrations: {len(optimized_extractor.demos)}\")\n",
    "    for i, demo in enumerate(optimized_extractor.demos[:2], 1):\n",
    "        print(f\"  Demo {i}: {demo.section_context[:50]}...\")\n",
    "\n",
    "# Check for any instruction changes\n",
    "if hasattr(optimized_extractor, 'signature'):\n",
    "    print(f\"\\nSignature: {optimized_extractor.signature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0c651",
   "metadata": {},
   "source": [
    "## 12. Save Results\n",
    "\n",
    "Save the optimized extractor and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e45006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training metadata\n",
    "output_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save training results\n",
    "results = {\n",
    "    \"baseline_score\": baseline_score,\n",
    "    \"optimized_score\": optimized_score,\n",
    "    \"train_size\": TRAIN_SIZE,\n",
    "    \"eval_size\": EVAL_SIZE,\n",
    "    \"num_fewshot\": NUM_FEWSHOT,\n",
    "    \"pages_processed\": pages_processed,\n",
    "    \"total_chunks\": len(training_chunks),\n",
    "}\n",
    "\n",
    "with open(output_dir / \"stage1_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Saved results to {output_dir / 'stage1_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized extractor state\n",
    "try:\n",
    "    optimized_extractor.save(output_dir / \"optimized_extractor\")\n",
    "    print(f\"Saved optimized extractor to {output_dir / 'optimized_extractor'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save extractor state: {e}\")\n",
    "    # Alternative: save as JSON\n",
    "    if hasattr(optimized_extractor, 'dump_state'):\n",
    "        state = optimized_extractor.dump_state()\n",
    "        with open(output_dir / \"optimized_extractor_state.json\", \"w\") as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "        print(\"Saved extractor state as JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec590ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save few-shot examples for reference\n",
    "fewshot_data = []\n",
    "for ex in selected_fewshot:\n",
    "    fewshot_data.append({\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"section_context\": ex.section_context,\n",
    "        \"statements\": list(ex.statements),\n",
    "    })\n",
    "\n",
    "with open(output_dir / \"fewshot_examples.json\", \"w\") as f:\n",
    "    json.dump(fewshot_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(fewshot_data)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e226c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. Loaded Albert Einstein as few-shot examples (seed/guidance)\n",
    "2. Fetched and chunked Wikipedia sample pages for training\n",
    "3. Established baseline extraction quality\n",
    "4. Ran MIPROv2 prompt optimization\n",
    "5. Saved the optimized extractor\n",
    "\n",
    "Next steps:\n",
    "- **Stage 2**: Schema matching with optimized statements\n",
    "- **Stage 3**: RDF generation training\n",
    "- **Arbor GRPO**: Fine-tune the full pipeline end-to-end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
