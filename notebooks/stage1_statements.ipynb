{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6df303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 1: Statement Extraction with DSPy\n",
    "# =============================================================================\n",
    "# This notebook orchestrates statement extraction training/evaluation.\n",
    "# All application logic is in ontological_engineer - this notebook coordinates.\n",
    "#\n",
    "# Outputs (with CID provenance):\n",
    "#   - data/training/chunks/*.ipynb - Chunked Wikipedia pages\n",
    "#   - data/training/statements.ipynb - Extracted statements\n",
    "#   - data/training/classifications.ipynb - Per-statement judgments\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/workspaces/wiki3-kg-project')\n",
    "\n",
    "import dspy\n",
    "import json\n",
    "from pathlib import Path\n",
    "from random import shuffle, seed as random_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ontological_engineer import (\n",
    "    # LM Configuration\n",
    "    configure_lm,\n",
    "    # DSPy Modules\n",
    "    StatementExtractor,\n",
    "    StatementQualityJudge,\n",
    "    StatementClassifier,\n",
    "    StatementClassification,\n",
    "    # Data Loading (from provenance-tracked notebooks)\n",
    "    WikipediaPage,\n",
    "    WikipediaChunk,\n",
    "    fetch_top_pages,\n",
    "    fetch_page_content,\n",
    "    chunk_article,\n",
    "    load_sample_from_notebook,\n",
    "    load_chunks_from_notebook,\n",
    "    # Provenance-tracked output generation\n",
    "    generate_chunks_notebook_header,\n",
    "    append_chunk_cell,\n",
    "    generate_statements_notebook_header,\n",
    "    append_statements_cell,\n",
    "    generate_classifications_notebook_header,\n",
    "    append_classifications_cell,\n",
    "    save_notebook,\n",
    "    get_processed_chunk_cids,\n",
    ")\n",
    "from ontological_engineer.judges import statement_quality_metric\n",
    "from ontological_engineer.training.bootstrap import (\n",
    "    load_chunks_from_notebook as load_albert_chunks,\n",
    "    load_facts_from_notebook,\n",
    "    create_training_examples,\n",
    ")\n",
    "from src.cid import compute_cid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f86eb6",
   "metadata": {},
   "source": [
    "## 1. Configure Language Model\n",
    "\n",
    "Connect to LM Studio running Qwen-30B (or your preferred model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e839f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured LM: <dspy.clients.lm.LM object at 0xffff6a5fac80>\n"
     ]
    }
   ],
   "source": [
    "# Configure the LM (defaults to Qwen-30B via LM Studio)\n",
    "lm = configure_lm(\n",
    "    model=\"qwen/qwen3-coder-30b\",\n",
    "    api_base=\"http://host.docker.internal:1234/v1\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(f\"Configured LM: {lm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185bc4f7",
   "metadata": {},
   "source": [
    "## 2. Load Few-Shot Examples (Albert Einstein)\n",
    "\n",
    "Albert Einstein is our gold-standard example. These chunks and their extracted facts\n",
    "serve as few-shot demonstrations for the extractor and judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b715a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 63 chunks from Albert Einstein\n",
      "Loaded 19 fact sets\n",
      "Created 19 few-shot examples\n"
     ]
    }
   ],
   "source": [
    "# Load Albert Einstein data for few-shot examples\n",
    "fewshot_dir = Path(\"/workspaces/wiki3-kg-project/data/albert_einstein/20251218_231446\")\n",
    "\n",
    "# Use load_albert_chunks (from bootstrap module) - NOT load_chunks_from_notebook\n",
    "fewshot_chunks = load_albert_chunks(fewshot_dir / \"chunks.ipynb\")\n",
    "fewshot_facts = load_facts_from_notebook(fewshot_dir / \"facts.ipynb\")\n",
    "\n",
    "print(f\"Loaded {len(fewshot_chunks)} chunks from Albert Einstein\")\n",
    "print(f\"Loaded {len(fewshot_facts)} fact sets\")\n",
    "\n",
    "# Create few-shot examples\n",
    "fewshot_examples = create_training_examples(fewshot_chunks, fewshot_facts)\n",
    "print(f\"Created {len(fewshot_examples)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5911ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few-shot example\n",
    "if fewshot_examples:\n",
    "    ex = fewshot_examples[0]\n",
    "    print(\"Sample few-shot example:\")\n",
    "    print(f\"  Context: {ex.section_context}\")\n",
    "    print(f\"  Text: {ex.chunk_text[:200]}...\")\n",
    "    print(f\"  Statements: {len(ex.statements)} items\")\n",
    "    for stmt in ex.statements[:3]:\n",
    "        print(f\"    - {stmt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06944",
   "metadata": {},
   "source": [
    "## 3. Load Wikipedia Sample for Training\n",
    "\n",
    "Load the 100-page Wikipedia sample from the provenance-tracked notebook.\n",
    "If the notebook doesn't exist, fall back to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9640f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 100 pages from provenance-tracked notebook\n",
      "   Source: /workspaces/wiki3-kg-project/data/training/wikipedia_sample.ipynb\n",
      "\n",
      "First 10 pages:\n",
      "  - Zohran Mamdani (9,344,963 views)\n",
      "  - ChatGPT (3,639,485 views)\n",
      "  - James A. Garfield (3,524,531 views)\n",
      "  - 1989 Tiananmen Square protests and massacre (2,867,005 views)\n",
      "  - 2025 Bihar Legislative Assembly election (2,555,071 views)\n",
      "  - Mira Nair (2,503,516 views)\n",
      "  - Dick Cheney (2,186,840 views)\n",
      "  - 2026 FIFA World Cup (2,155,565 views)\n",
      "  - 1xBet (1,831,684 views)\n",
      "  - Survivor Series: WarGames (2025) (1,590,263 views)\n"
     ]
    }
   ],
   "source": [
    "# Load the Wikipedia sample (prefer provenance-tracked notebook)\n",
    "sample_notebook = Path(\"/workspaces/wiki3-kg-project/data/training/wikipedia_sample.ipynb\")\n",
    "sample_json = Path(\"/workspaces/wiki3-kg-project/data/training/wikipedia_sample.json\")\n",
    "\n",
    "if sample_notebook.exists():\n",
    "    # Load from provenance-tracked notebook\n",
    "    wiki_pages = load_sample_from_notebook(sample_notebook)\n",
    "    print(f\"‚úÖ Loaded {len(wiki_pages)} pages from provenance-tracked notebook\")\n",
    "    print(f\"   Source: {sample_notebook}\")\n",
    "elif sample_json.exists():\n",
    "    # Fall back to JSON format\n",
    "    with open(sample_json) as f:\n",
    "        wiki_sample = json.load(f)\n",
    "    wiki_pages = [WikipediaPage(title=p['title'], views=p['views']) for p in wiki_sample['pages']]\n",
    "    print(f\"‚ö†Ô∏è  Loaded {len(wiki_pages)} pages from JSON (no provenance)\")\n",
    "    print(f\"   Run sample_wikipedia_pages.ipynb to generate provenance-tracked version\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No Wikipedia sample found. Run sample_wikipedia_pages.ipynb first.\")\n",
    "\n",
    "print(f\"\\nFirst 10 pages:\")\n",
    "for p in wiki_pages[:10]:\n",
    "    print(f\"  - {p.title} ({p.views:,} views)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab17fa",
   "metadata": {},
   "source": [
    "## 4. Fetch and Chunk Wikipedia Pages (with CID Provenance)\n",
    "\n",
    "Fetch page content and chunk it. Each page's chunks are saved to a \n",
    "provenance-tracked notebook with CID signatures.\n",
    "\n",
    "**Note**: Uses `fetch_page_content` and `chunk_article` from `ontological_engineer` - \n",
    "no application logic defined in this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be01db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on: Zohran Mamdani\n",
      "  ‚Üí 20 chunks (filtered by min_length=100)\n",
      "  First chunk preview: Zohran Kwame Mamdani (born October 18, 1991) is an American politician who is the mayor-elect of New York City. A member of the Democratic Party and the Democratic Socialists of America, he is set to ...\n"
     ]
    }
   ],
   "source": [
    "# Output directory for chunks notebooks\n",
    "chunks_dir = Path(\"/workspaces/wiki3-kg-project/data/training/chunks\")\n",
    "chunks_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "MAX_PAGES = 20  # Increase for full training run\n",
    "MIN_CHUNK_LENGTH = 100  # Skip very short chunks\n",
    "\n",
    "# Test on one page first\n",
    "test_page = wiki_pages[0]\n",
    "print(f\"Testing on: {test_page.title}\")\n",
    "\n",
    "content = fetch_page_content(test_page.title)\n",
    "if content:\n",
    "    chunks = chunk_article(test_page.title, content)\n",
    "    # Filter short chunks\n",
    "    chunks = [c for c in chunks if len(c.text) >= MIN_CHUNK_LENGTH]\n",
    "    print(f\"  ‚Üí {len(chunks)} chunks (filtered by min_length={MIN_CHUNK_LENGTH})\")\n",
    "    if chunks:\n",
    "        print(f\"  First chunk preview: {chunks[0].text[:200]}...\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Could not fetch content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886bea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and chunking 20 pages...\n",
      "Chunks will be saved to: /workspaces/wiki3-kg-project/data/training/chunks\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:07<00:00,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úÖ Processed 20 pages\n",
      "   Total training chunks: 879\n",
      "   Chunks saved to: /workspaces/wiki3-kg-project/data/training/chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch and chunk all pages, saving each to a provenance-tracked notebook\n",
    "training_chunks = []\n",
    "pages_processed = 0\n",
    "\n",
    "print(f\"Fetching and chunking {min(MAX_PAGES, len(wiki_pages))} pages...\")\n",
    "print(f\"Chunks will be saved to: {chunks_dir}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for page in tqdm(wiki_pages[:MAX_PAGES], desc=\"Processing pages\"):\n",
    "    # Check if already processed\n",
    "    page_slug = page.title.lower().replace(' ', '_').replace('/', '_')\n",
    "    chunks_path = chunks_dir / f\"{page_slug}_chunks.ipynb\"\n",
    "    \n",
    "    if chunks_path.exists():\n",
    "        # Load from existing notebook (skip re-fetching)\n",
    "        existing_chunks = load_chunks_from_notebook(chunks_path)\n",
    "        for c in existing_chunks:\n",
    "            training_chunks.append(WikipediaChunk(\n",
    "                text=c['text'],\n",
    "                section_context=c['section_context'],\n",
    "                chunk_num=c['chunk_num'],\n",
    "                total_chunks=len(existing_chunks),\n",
    "                page_title=page.title,\n",
    "            ))\n",
    "        pages_processed += 1\n",
    "        continue\n",
    "    \n",
    "    # Fetch and chunk\n",
    "    content = fetch_page_content(page.title)\n",
    "    if not content:\n",
    "        continue\n",
    "    \n",
    "    chunks = chunk_article(page.title, content)\n",
    "    chunks = [c for c in chunks if len(c.text) >= MIN_CHUNK_LENGTH]\n",
    "    if not chunks:\n",
    "        continue\n",
    "    \n",
    "    # Create chunks notebook with CID provenance\n",
    "    nb = generate_chunks_notebook_header(\n",
    "        page_title=page.title,\n",
    "        source_url=f\"https://en.wikipedia.org/wiki/{page.title.replace(' ', '_')}\",\n",
    "    )\n",
    "    \n",
    "    # Compute a CID for the full page content (source provenance)\n",
    "    page_cid = compute_cid(content)\n",
    "    \n",
    "    # Add each chunk with CID signature\n",
    "    for chunk in chunks:\n",
    "        chunk_cid, _ = append_chunk_cell(nb, chunk, source_cid=page_cid)\n",
    "    \n",
    "    # Save the chunks notebook\n",
    "    save_notebook(nb, chunks_path)\n",
    "    \n",
    "    training_chunks.extend(chunks)\n",
    "    pages_processed += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"‚úÖ Processed {pages_processed} pages\")\n",
    "print(f\"   Total training chunks: {len(training_chunks)}\")\n",
    "print(f\"   Chunks saved to: {chunks_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf50dd2",
   "metadata": {},
   "source": [
    "## 5. Initialize Extractor with Few-Shot Examples\n",
    "\n",
    "Create the statement extractor and provide Albert Einstein examples as demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e38b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 0 few-shot examples:\n"
     ]
    }
   ],
   "source": [
    "# Select best few-shot examples (ones with good variety of statements)\n",
    "NUM_FEWSHOT = 3\n",
    "\n",
    "# Sort by statement count to get diverse examples\n",
    "sorted_fewshot = sorted(fewshot_examples, key=lambda x: len(x.statements), reverse=True)\n",
    "selected_fewshot = sorted_fewshot[:NUM_FEWSHOT]\n",
    "\n",
    "print(f\"Selected {len(selected_fewshot)} few-shot examples:\")\n",
    "for i, ex in enumerate(selected_fewshot, 1):\n",
    "    print(f\"  {i}. {ex.section_context[:50]}... ({len(ex.statements)} statements)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9896eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor initialized\n",
      "Few-shot examples available: 0\n"
     ]
    }
   ],
   "source": [
    "# Create extractor with few-shot demonstrations\n",
    "extractor = StatementExtractor()\n",
    "\n",
    "# In DSPy, we can provide demonstrations directly\n",
    "# The few-shot examples will be used by MIPROv2 for bootstrapping\n",
    "print(\"Extractor initialized\")\n",
    "print(f\"Few-shot examples available: {len(selected_fewshot)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4419625",
   "metadata": {},
   "source": [
    "## 6. Test Extraction on Training Sample\n",
    "\n",
    "Run the extractor on a few training chunks to verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ea088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on: Zohran Mamdani > Zohran Mamdani\n",
      "Text: Zohran Kwame Mamdani (born October 18, 1991) is an American politician who is the mayor-elect of New York City. A member of the Democratic Party and the Democratic Socialists of America, he is set to become New York's first Muslim and Asian American mayor. Mamdani has served as a member of the New Y...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Extracted 22 statements:\n",
      "  1. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) was born on October 18, 1991.\n",
      "  2. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is an American politician.\n",
      "  3. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is the mayor-elect of [New York City](/wiki/New_York_City).\n",
      "  4. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is a member of the [Democratic Party](/wiki/Democratic_Party).\n",
      "  5. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is a member of the [Democratic Socialists of America](/wiki/Democratic_Socialists_of_America).\n",
      "  6. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is set to become [New York City](/wiki/New_York_City)'s first Muslim mayor.\n",
      "  7. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is set to become [New York City](/wiki/New_York_City)'s first Asian American mayor.\n",
      "  8. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) has served as a member of the [New York State Assembly](/wiki/New_York_State_Assembly) for the 36th district since 2021.\n",
      "  9. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) represents the [Queens](/wiki/Queens) neighborhood of [Astoria](/wiki/Astoria,_New_York).\n",
      "  10. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) was born in [Kampala](/wiki/Kampala), [Uganda](/wiki/Uganda).\n",
      "  ... and 12 more\n"
     ]
    }
   ],
   "source": [
    "# Test on a training chunk\n",
    "if training_chunks:\n",
    "    test_chunk = training_chunks[0]\n",
    "    \n",
    "    print(f\"Testing on: {test_chunk.section_context}\")\n",
    "    print(f\"Text: {test_chunk.text[:300]}...\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    result = extractor(\n",
    "        chunk_text=test_chunk.text,\n",
    "        section_context=test_chunk.section_context,\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracted {len(result.statements)} statements:\")\n",
    "    for i, stmt in enumerate(result.statements[:10], 1):\n",
    "        print(f\"  {i}. {stmt}\")\n",
    "    if len(result.statements) > 10:\n",
    "        print(f\"  ... and {len(result.statements) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a247a7a",
   "metadata": {},
   "source": [
    "## 7. Create Training Dataset\n",
    "\n",
    "Convert chunks into DSPy examples. For training, we need to generate initial extractions\n",
    "that can be scored and optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fab9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 703 examples\n",
      "Dev set: 176 examples\n"
     ]
    }
   ],
   "source": [
    "# Create training examples (without labels - we'll generate and judge them)\n",
    "# For DSPy optimization, we just need the inputs\n",
    "\n",
    "random_seed(42)  # For reproducibility\n",
    "\n",
    "# Convert WikipediaChunk objects to DSPy examples\n",
    "trainset_chunks = list(training_chunks)  # Make a copy\n",
    "shuffle(trainset_chunks)\n",
    "\n",
    "trainset = []\n",
    "for chunk in trainset_chunks:\n",
    "    ex = dspy.Example(\n",
    "        chunk_text=chunk.text,\n",
    "        section_context=chunk.section_context,\n",
    "    ).with_inputs('chunk_text', 'section_context')\n",
    "    trainset.append(ex)\n",
    "\n",
    "# Split into train/dev\n",
    "split_idx = int(len(trainset) * 0.8)\n",
    "devset = trainset[split_idx:]\n",
    "trainset = trainset[:split_idx]\n",
    "\n",
    "print(f\"Training set: {len(trainset)} examples\")\n",
    "print(f\"Dev set: {len(devset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda75e24",
   "metadata": {},
   "source": [
    "## 8. Initialize Judge with Few-Shot Guidance\n",
    "\n",
    "The judge scores extraction quality. We use Albert Einstein examples to calibrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "792d6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize judge\n",
    "judge = StatementQualityJudge()\n",
    "\n",
    "# Test judge on a known good example (Albert Einstein few-shot)\n",
    "if selected_fewshot:\n",
    "    test_ex = selected_fewshot[0]\n",
    "    \n",
    "    evaluation = judge(\n",
    "        chunk_text=test_ex.chunk_text,\n",
    "        section_context=test_ex.section_context,\n",
    "        statements=test_ex.statements,\n",
    "    )\n",
    "    \n",
    "    print(\"Judge calibration on few-shot example:\")\n",
    "    print(f\"  Completeness:      {evaluation.completeness:.2f}\")\n",
    "    print(f\"  Atomicity:         {evaluation.atomicity:.2f}\")\n",
    "    print(f\"  Accuracy:          {evaluation.accuracy:.2f}\")\n",
    "    print(f\"  Link preservation: {evaluation.link_preservation:.2f}\")\n",
    "    print(f\"  ---\")\n",
    "    print(f\"  Weighted score:    {evaluation.weighted_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649d2fc",
   "metadata": {},
   "source": [
    "## 9. Baseline Evaluation\n",
    "\n",
    "Evaluate the unoptimized extractor on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f230a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 8.72 / 10 (87.2%): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 250.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/20 05:01:37 INFO dspy.evaluate.evaluate: Average Metric: 8.724999999999998 / 10 (87.2%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Baseline quality score: 87.25\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline on dev set\n",
    "EVAL_SIZE = min(10, len(devset))  # Limit for speed\n",
    "\n",
    "evaluator = dspy.Evaluate(\n",
    "    devset=devset[:EVAL_SIZE],\n",
    "    metric=statement_quality_metric,\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    ")\n",
    "\n",
    "baseline_extractor = StatementExtractor()\n",
    "baseline_result = evaluator(baseline_extractor)\n",
    "\n",
    "baseline_score = baseline_result.score if hasattr(baseline_result, 'score') else float(baseline_result)\n",
    "print(f\"\\nBaseline quality score: {baseline_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e36a0",
   "metadata": {},
   "source": [
    "## 9b. MLflow Observability Setup\n",
    "\n",
    "MLflow provides tracing, evaluation, and human feedback tools for DSPy pipelines.\n",
    "\n",
    "### Quick Setup (One-time)\n",
    "\n",
    "1. **Install MLflow** (already in requirements or run cell below)\n",
    "2. **Start the MLflow server** in a terminal:\n",
    "   ```bash\n",
    "   cd /workspaces/wiki3-kg-project\n",
    "   mlflow server \\\n",
    "     --backend-store-uri sqlite:///mlflow.sqlite \\\n",
    "     --default-artifact-root ./mlflow-artifacts \\\n",
    "     --host 0.0.0.0 \\\n",
    "     --port 5000\n",
    "   ```\n",
    "3. **Open the UI** at http://localhost:5000 (or via VS Code port forwarding)\n",
    "\n",
    "### What MLflow Provides\n",
    "- **Tracing**: See every LM call, inputs, outputs, latency\n",
    "- **Evaluation**: Compare model versions side-by-side\n",
    "- **Human Feedback**: Add labels/assessments directly in the UI\n",
    "- **Experiment Tracking**: Track metrics across optimization runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87efb9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MLflow configured successfully\n",
      "   Tracking URI: http://127.0.0.1:5000\n",
      "   Experiment: wiki3-kg-stage1-statements\n",
      "\n",
      "üìä Open MLflow UI: http://127.0.0.1:5000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MLflow Setup\n",
    "# =============================================================================\n",
    "# Prerequisites:\n",
    "#   1. Install: pip install \"mlflow>=3.0\"\n",
    "#   2. Start server in terminal (or run: ./scripts/start_mlflow.sh):\n",
    "#      mlflow server --backend-store-uri sqlite:///mlflow.sqlite \\\n",
    "#                    --default-artifact-root ./mlflow-artifacts \\\n",
    "#                    --host 0.0.0.0 --port 5000\n",
    "#   3. Open UI: http://localhost:5000\n",
    "# =============================================================================\n",
    "\n",
    "import mlflow\n",
    "\n",
    "# Configure MLflow - use localhost for local server\n",
    "# For Docker: use host.docker.internal if MLflow runs on host\n",
    "MLFLOW_TRACKING_URI = \"http://127.0.0.1:5000\"\n",
    "\n",
    "try:\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(\"wiki3-kg-stage1-statements\")\n",
    "    \n",
    "    # Enable automatic DSPy tracing - captures all LM calls, modules, predictions\n",
    "    mlflow.dspy.autolog()\n",
    "    \n",
    "    print(f\"‚úÖ MLflow configured successfully\")\n",
    "    print(f\"   Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "    print(f\"   Experiment: wiki3-kg-stage1-statements\")\n",
    "    print(f\"\\nüìä Open MLflow UI: {MLFLOW_TRACKING_URI}\")\n",
    "    MLFLOW_ENABLED = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  MLflow not available: {e}\")\n",
    "    print(f\"\\nüí° To enable MLflow, start the server:\")\n",
    "    print(f\"   mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\")\n",
    "    MLFLOW_ENABLED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de34b2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:01<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Evaluation complete!\n",
      "   Average score: 0.87\n",
      "   Traces logged: 10\n",
      "\n",
      "üìä Review in MLflow UI: http://127.0.0.1:5000\n",
      "   ‚Üí Click 'Traces' tab to see all predictions\n",
      "   ‚Üí Click individual traces to review inputs/outputs\n",
      "   ‚Üí Use 'Feedback' to add human labels\n",
      "üèÉ View run baseline_evaluation at: http://127.0.0.1:5000/#/experiments/1/runs/f8ce0c78fe6f40698771a96f839f6d1a\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-1d34d08e7a4c75d4dc99e04cf0e98b3b&amp;experiment_id=1&amp;trace_id=tr-7354ea6f6160745985c7504bc693da11&amp;experiment_id=1&amp;trace_id=tr-00af5b3a2812859a1337739e8d4f5d27&amp;experiment_id=1&amp;trace_id=tr-8b6870b51d61fac36cd5e85932a447b2&amp;experiment_id=1&amp;trace_id=tr-7e695d0d8a3c3b5e801ef1da45b1ed25&amp;experiment_id=1&amp;trace_id=tr-c19ad58cc35b1c8c0a4c9f7f9384ec2b&amp;experiment_id=1&amp;trace_id=tr-49c8a43f7ed70ed7b194990b6961929e&amp;experiment_id=1&amp;trace_id=tr-5c9d927d84b871bb300568d20de051a6&amp;experiment_id=1&amp;trace_id=tr-8d60593603802b708d03c91e4f8d5238&amp;experiment_id=1&amp;trace_id=tr-6232b17a250741818d1fb54074eff545&amp;experiment_id=1&amp;version=3.7.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=tr-1d34d08e7a4c75d4dc99e04cf0e98b3b), Trace(trace_id=tr-7354ea6f6160745985c7504bc693da11), Trace(trace_id=tr-00af5b3a2812859a1337739e8d4f5d27), Trace(trace_id=tr-8b6870b51d61fac36cd5e85932a447b2), Trace(trace_id=tr-7e695d0d8a3c3b5e801ef1da45b1ed25), Trace(trace_id=tr-c19ad58cc35b1c8c0a4c9f7f9384ec2b), Trace(trace_id=tr-49c8a43f7ed70ed7b194990b6961929e), Trace(trace_id=tr-5c9d927d84b871bb300568d20de051a6), Trace(trace_id=tr-8d60593603802b708d03c91e4f8d5238), Trace(trace_id=tr-6232b17a250741818d1fb54074eff545)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run evaluation with MLflow tracing (if enabled)\n",
    "# Each prediction creates a trace viewable in the MLflow UI\n",
    "\n",
    "if MLFLOW_ENABLED:\n",
    "    with mlflow.start_run(run_name=\"baseline_evaluation\"):\n",
    "        # Log parameters for reproducibility\n",
    "        mlflow.log_param(\"eval_size\", EVAL_SIZE)\n",
    "        mlflow.log_param(\"model\", \"qwen/qwen3-coder-30b\")\n",
    "        mlflow.log_param(\"num_fewshot\", NUM_FEWSHOT)\n",
    "        \n",
    "        # Run extractions on dev set - each one is traced\n",
    "        results = []\n",
    "        for i, ex in enumerate(tqdm(devset[:EVAL_SIZE], desc=\"Evaluating\")):\n",
    "            with mlflow.start_span(name=f\"example_{i}\") as span:\n",
    "                # Run extraction\n",
    "                pred = baseline_extractor(\n",
    "                    chunk_text=ex.chunk_text,\n",
    "                    section_context=ex.section_context,\n",
    "                )\n",
    "                \n",
    "                # Run judge\n",
    "                eval_result = judge(\n",
    "                    chunk_text=ex.chunk_text,\n",
    "                    section_context=ex.section_context,\n",
    "                    statements=pred.statements,\n",
    "                )\n",
    "                \n",
    "                # Log to span for MLflow UI review\n",
    "                span.set_inputs({\n",
    "                    \"chunk_text\": ex.chunk_text[:500],\n",
    "                    \"section_context\": ex.section_context,\n",
    "                })\n",
    "                span.set_outputs({\n",
    "                    \"statements\": list(pred.statements),\n",
    "                    \"completeness\": float(eval_result.completeness),\n",
    "                    \"atomicity\": float(eval_result.atomicity),\n",
    "                    \"accuracy\": float(eval_result.accuracy),\n",
    "                    \"link_preservation\": float(eval_result.link_preservation),\n",
    "                    \"weighted_score\": float(eval_result.weighted_score),\n",
    "                    \"reasoning\": eval_result.reasoning,\n",
    "                })\n",
    "                \n",
    "                results.append({\n",
    "                    \"index\": i,\n",
    "                    \"score\": float(eval_result.weighted_score),\n",
    "                })\n",
    "        \n",
    "        # Log aggregate metrics\n",
    "        avg_score = sum(r[\"score\"] for r in results) / len(results)\n",
    "        mlflow.log_metric(\"avg_quality_score\", avg_score)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Evaluation complete!\")\n",
    "        print(f\"   Average score: {avg_score:.2f}\")\n",
    "        print(f\"   Traces logged: {len(results)}\")\n",
    "        print(f\"\\nüìä Review in MLflow UI: {MLFLOW_TRACKING_URI}\")\n",
    "        print(f\"   ‚Üí Click 'Traces' tab to see all predictions\")\n",
    "        print(f\"   ‚Üí Click individual traces to review inputs/outputs\")\n",
    "        print(f\"   ‚Üí Use 'Feedback' to add human labels\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping MLflow evaluation (server not running)\")\n",
    "    print(\"   Run baseline evaluation with dspy.Evaluate instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "794a21f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent LM calls (use MLflow UI for full traces):\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-12-20T05:01:40.742367]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `chunk_text` (str): Wikipedia article chunk with markdown links preserved\n",
      "2. `section_context` (str): Breadcrumb showing location: Article > Section > Subsection\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `statements` (list[str]): List of atomic statements, each preserving [Entity](/wiki/...) links\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## chunk_text ## ]]\n",
      "{chunk_text}\n",
      "\n",
      "[[ ## section_context ## ]]\n",
      "{section_context}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## statements ## ]]\n",
      "{statements}        # note: the value you produce must adhere to the JSON schema: {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Extract atomic, verifiable statements from Wikipedia text.\n",
      "        \n",
      "        Each statement must:\n",
      "        - Be self-contained (understandable without the original text)\n",
      "        - Preserve markdown links: [Entity Name](/wiki/Entity_Name)\n",
      "        - Contain exactly one verifiable claim\n",
      "        - Not editorialize or interpret beyond what's stated\n",
      "        \n",
      "        Example input chunk:\n",
      "            \"Albert Einstein was born in Ulm, in the Kingdom of W√ºrttemberg \n",
      "            in the German Empire, on 14 March 1879.\"\n",
      "        \n",
      "        Example output statements:\n",
      "            - \"[Albert Einstein](/wiki/Albert_Einstein) was born in [Ulm](/wiki/Ulm).\"\n",
      "            - \"[Albert Einstein](/wiki/Albert_Einstein) was born on 14 March 1879.\"\n",
      "            - \"[Ulm](/wiki/Ulm) was in the [Kingdom of W√ºrttemberg](/wiki/Kingdom_of_W√ºrttemberg).\"\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## chunk_text ## ]]\n",
      "Kirk was the author of several books. Along with Brent Hamachek, he co-wrote the 2016 book Time for a Turning Point: Setting a Course Toward Free Markets and Limited Government for Future Generations, which was published by Post Hill Press, a subsidiary of Simon & Schuster. Under the same publisher, Kirk wrote the 2018 book Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters. Donald Trump Jr. wrote the foreword for the book. In a review for The Weekly Standard, Adam Rubenstein described the book as a \"hot mess\", \"nothing more than a marketing pitch for TPUSA\", and said the \"thin\" book was \"stuffed with reprintings of his tweets and quotes from others\".\n",
      "In 2020, Kirk wrote The MAGA Doctrine: The Only Ideas That Will Win the Future, which was published by Harper Collins. In its review for The New York Times, Gabriel Debenedetti wrote that \"Kirk's musing about whether 'The Art of The Deal' might one day be considered a 'religious tract' comes just nine chapters after the book highlights the importance of 'a healthy dose of skepticism about authority figures and experts who think they knew best.' And that's just pages after its dedication to Donald Trump, which is five chapters before Kirk wonders whether Trump might 'be remembered as the president who brought about world peace.'\" In Open Letters Review, Steve Donoghue said of the book that \"On every page, Kirk writes something that's either trivially, casually wrong ..\n",
      "\n",
      "[[ ## section_context ## ]]\n",
      "Charlie Kirk > Books\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## statements ## ]]` (must be formatted as a valid Python list[str]), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The chunk text contains several verifiable claims about Charlie Kirk's books, their authors, publishers, reviews, and content. Each sentence or group of sentences forms a distinct atomic statement that can stand alone and be verified independently. The statements must preserve markdown links if present (though none are in this case), and each must contain exactly one verifiable claim.\n",
      "\n",
      "The key claims to extract include:\n",
      "1. Kirk authored several books.\n",
      "2. He co-wrote a 2016 book with Brent Hamachek titled \"Time for a Turning Point...\" published by Post Hill Press.\n",
      "3. He wrote another 2018 book titled \"Campus Battlefield...\" under the same publisher.\n",
      "4. Donald Trump Jr. wrote the foreword for \"Campus Battlefield...\".\n",
      "5. Adam Rubenstein reviewed \"Campus Battlefield...\" and called it a \"hot mess\" and criticized its content.\n",
      "6. In 2020, Kirk wrote \"The MAGA Doctrine...\" published by Harper Collins.\n",
      "7. Gabriel Debenedetti reviewed \"The MAGA Doctrine...\" for The New York Times and criticized its inconsistencies.\n",
      "8. Steve Donoghue reviewed \"The MAGA Doctrine...\" for Open Letters Review and criticized its accuracy.\n",
      "\n",
      "[[ ## statements ## ]]\n",
      "[\n",
      "    \"[Charlie Kirk](/wiki/Charlie_Kirk) was the author of several books.\",\n",
      "    \"[Charlie Kirk](/wiki/Charlie_Kirk) co-wrote the 2016 book [Time for a Turning Point: Setting a Course Toward Free Markets and Limited Government for Future Generations](/wiki/Time_for_a_Turning_Point) with [Brent Hamachek](/wiki/Brent_Hamachek).\",\n",
      "    \"The 2016 book [Time for a Turning Point: Setting a Course Toward Free Markets and Limited Government for Future Generations](/wiki/Time_for_a_Turning_Point) was published by [Post Hill Press](/wiki/Post_Hill_Press).\",\n",
      "    \"[Post Hill Press](/wiki/Post_Hill_Press) is a subsidiary of [Simon & Schuster](/wiki/Simon_%26_Schuster).\",\n",
      "    \"[Charlie Kirk](/wiki/Charlie_Kirk) wrote the 2018 book [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield).\",\n",
      "    \"[Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) was published by [Post Hill Press](/wiki/Post_Hill_Press).\",\n",
      "    \"[Donald Trump Jr.](/wiki/Donald_Trump_Jr.) wrote the foreword for [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield).\",\n",
      "    \"In a review for [The Weekly Standard](/wiki/The_Weekly_Standard), [Adam Rubenstein](/wiki/Adam_Rubenstein) described [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) as a 'hot mess'.\",\n",
      "    \"In a review for [The Weekly Standard](/wiki/The_Weekly_Standard), [Adam Rubenstein](/wiki/Adam_Rubenstein) said [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) was 'nothing more than a marketing pitch for TPUSA'.\",\n",
      "    \"In a review for [The Weekly Standard](/wiki/The_Weekly_Standard), [Adam Rubenstein](/wiki/Adam_Rubenstein) said the 'thin' book [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) was 'stuffed with reprintings of his tweets and quotes from others'.\",\n",
      "    \"[Charlie Kirk](/wiki/Charlie_Kirk) wrote the 2020 book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine).\",\n",
      "    \"[The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) was published by [Harper Collins](/wiki/Harper_Collins).\",\n",
      "    \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that [Charlie Kirk](/wiki/Charlie_Kirk)'s musing about whether '[The Art of The Deal](/wiki/The_Art_of_the_Deal)' might one day be considered a 'religious tract' comes just nine chapters after the book highlights the importance of 'a healthy dose of skepticism about authority figures and experts who think they knew best'.\",\n",
      "    \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) highlights the importance of 'a healthy dose of skepticism about authority figures and experts who think they knew best.'\",\n",
      "    \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) dedicates itself to [Donald Trump](/wiki/Donald_Trump).\",\n",
      "    \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) highlights that [Donald Trump](/wiki/Donald_Trump) might 'be remembered as the president who brought about world peace.'\",\n",
      "    \"In [Open Letters Review](/wiki/Open_Letters_Review), [Steve Donoghue](/wiki/Steve_Donoghue) said of the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) that 'On every page, [Charlie Kirk](/wiki/Charlie_Kirk) writes something that's either trivially, casually wrong.'\"\n",
      "]\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2025-12-20T05:01:40.799120]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `chunk_text` (str): Original Wikipedia chunk\n",
      "2. `section_context` (str): Section breadcrumb for context\n",
      "3. `statements` (list[str]): Extracted statements to evaluate\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): Brief explanation of the scores given\n",
      "2. `completeness` (float): Score 0-1: Are all key facts from the chunk captured?\n",
      "3. `atomicity` (float): Score 0-1: Is each statement truly atomic (one claim)?\n",
      "4. `accuracy` (float): Score 0-1: Do statements faithfully represent the source?\n",
      "5. `link_preservation` (float): Score 0-1: Are [Entity](/wiki/...) links preserved correctly?\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## chunk_text ## ]]\n",
      "{chunk_text}\n",
      "\n",
      "[[ ## section_context ## ]]\n",
      "{section_context}\n",
      "\n",
      "[[ ## statements ## ]]\n",
      "{statements}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## completeness ## ]]\n",
      "{completeness}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## atomicity ## ]]\n",
      "{atomicity}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## accuracy ## ]]\n",
      "{accuracy}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## link_preservation ## ]]\n",
      "{link_preservation}        # note: the value you produce must be a single float value\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Judge the quality of extracted statements from a Wikipedia chunk.\n",
      "        \n",
      "        Evaluate on four dimensions:\n",
      "        1. Completeness: Are all key facts from the chunk captured?\n",
      "        2. Atomicity: Is each statement truly atomic (one verifiable claim)?\n",
      "        3. Accuracy: Do statements faithfully represent the source without adding info?\n",
      "        4. Link preservation: Are [Entity](/wiki/...) links preserved correctly?\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## chunk_text ## ]]\n",
      "Kirk was the author of several books. Along with Brent Hamachek, he co-wrote the 2016 book Time for a Turning Point: Setting a Course Toward Free Markets and Limited Government for Future Generations, which was published by Post Hill Press, a subsidiary of Simon & Schuster. Under the same publisher, Kirk wrote the 2018 book Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters. Donald Trump Jr. wrote the foreword for the book. In a review for The Weekly Standard, Adam Rubenstein described the book as a \"hot mess\", \"nothing more than a marketing pitch for TPUSA\", and said the \"thin\" book was \"stuffed with reprintings of his tweets and quotes from others\".\n",
      "In 2020, Kirk wrote The MAGA Doctrine: The Only Ideas That Will Win the Future, which was published by Harper Collins. In its review for The New York Times, Gabriel Debenedetti wrote that \"Kirk's musing about whether 'The Art of The Deal' might one day be considered a 'religious tract' comes just nine chapters after the book highlights the importance of 'a healthy dose of skepticism about authority figures and experts who think they knew best.' And that's just pages after its dedication to Donald Trump, which is five chapters before Kirk wonders whether Trump might 'be remembered as the president who brought about world peace.'\" In Open Letters Review, Steve Donoghue said of the book that \"On every page, Kirk writes something that's either trivially, casually wrong ..\n",
      "\n",
      "[[ ## section_context ## ]]\n",
      "Charlie Kirk > Books\n",
      "\n",
      "[[ ## statements ## ]]\n",
      "[\"[Charlie Kirk](/wiki/Charlie_Kirk) was the author of several books.\", \"[Charlie Kirk](/wiki/Charlie_Kirk) co-wrote the 2016 book [Time for a Turning Point: Setting a Course Toward Free Markets and Limited Government for Future Generations](/wiki/Time_for_a_Turning_Point) with [Brent Hamachek](/wiki/Brent_Hamachek).\", \"The 2016 book [Time for a Turning Point: Setting a Course Toward Free Markets and Limited Government for Future Generations](/wiki/Time_for_a_Turning_Point) was published by [Post Hill Press](/wiki/Post_Hill_Press).\", \"[Post Hill Press](/wiki/Post_Hill_Press) is a subsidiary of [Simon & Schuster](/wiki/Simon_%26_Schuster).\", \"[Charlie Kirk](/wiki/Charlie_Kirk) wrote the 2018 book [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield).\", \"[Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) was published by [Post Hill Press](/wiki/Post_Hill_Press).\", \"[Donald Trump Jr.](/wiki/Donald_Trump_Jr.) wrote the foreword for [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield).\", \"In a review for [The Weekly Standard](/wiki/The_Weekly_Standard), [Adam Rubenstein](/wiki/Adam_Rubenstein) described [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) as a 'hot mess'.\", \"In a review for [The Weekly Standard](/wiki/The_Weekly_Standard), [Adam Rubenstein](/wiki/Adam_Rubenstein) said [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) was 'nothing more than a marketing pitch for TPUSA'.\", \"In a review for [The Weekly Standard](/wiki/The_Weekly_Standard), [Adam Rubenstein](/wiki/Adam_Rubenstein) said the 'thin' book [Campus Battlefield: How Conservatives Can WIN the Battle on Campus and Why It Matters](/wiki/Campus_Battlefield) was 'stuffed with reprintings of his tweets and quotes from others'.\", \"[Charlie Kirk](/wiki/Charlie_Kirk) wrote the 2020 book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine).\", \"[The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) was published by [Harper Collins](/wiki/Harper_Collins).\", \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that [Charlie Kirk](/wiki/Charlie_Kirk)'s musing about whether '[The Art of The Deal](/wiki/The_Art_of_the_Deal)' might one day be considered a 'religious tract' comes just nine chapters after the book highlights the importance of 'a healthy dose of skepticism about authority figures and experts who think they knew best'.\", \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) highlights the importance of 'a healthy dose of skepticism about authority figures and experts who think they knew best.'\", \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) dedicates itself to [Donald Trump](/wiki/Donald_Trump).\", \"In its review for [The New York Times](/wiki/The_New_York_Times), [Gabriel Debenedetti](/wiki/Gabriel_Debenedetti) wrote that the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) highlights that [Donald Trump](/wiki/Donald_Trump) might 'be remembered as the president who brought about world peace.'\", \"In [Open Letters Review](/wiki/Open_Letters_Review), [Steve Donoghue](/wiki/Steve_Donoghue) said of the book [The MAGA Doctrine: The Only Ideas That Will Win the Future](/wiki/The_MAGA_Doctrine) that 'On every page, [Charlie Kirk](/wiki/Charlie_Kirk) writes something that's either trivially, casually wrong.'\"]\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## completeness ## ]]` (must be formatted as a valid Python float), then `[[ ## atomicity ## ]]` (must be formatted as a valid Python float), then `[[ ## accuracy ## ]]` (must be formatted as a valid Python float), then `[[ ## link_preservation ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The extracted statements capture most of the key facts from the chunk, including the books authored by Charlie Kirk, their publication details, publishers, forewords, and reviews. However, some nuances are lost in translation, particularly in the review excerpts where the full context of the criticism is not fully represented. The statements are generally atomic, each containing a single verifiable claim. Links are preserved correctly for all entities mentioned. Accuracy is high, though some statements slightly paraphrase the original text, which may affect precision.\n",
      "\n",
      "[[ ## completeness ## ]]\n",
      "0.9\n",
      "\n",
      "[[ ## atomicity ## ]]\n",
      "0.95\n",
      "\n",
      "[[ ## accuracy ## ]]\n",
      "0.9\n",
      "\n",
      "[[ ## link_preservation ## ]]\n",
      "1.0\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Use dspy.inspect_history() for quick debugging\n",
    "# This shows recent LM calls without needing MLflow server\n",
    "\n",
    "print(\"Recent LM calls (use MLflow UI for full traces):\")\n",
    "print(\"=\" * 60)\n",
    "dspy.inspect_history(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d708ef8",
   "metadata": {},
   "source": [
    "### MLflow Evaluation with Human Feedback\n",
    "\n",
    "Use MLflow's evaluation API to systematically review predictions and collect human labels.\n",
    "The MLflow UI provides a proper interface for reviewing and annotating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de353ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation dataset with 10 examples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>chunk_text</th>\n",
       "      <th>section_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>In an interview with Wired magazine during the...</td>\n",
       "      <td>Charlie Kirk &gt; Republican and pro-Trump activism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>. He has described his upbringing as \"privileg...</td>\n",
       "      <td>Zohran Mamdani &gt; Early life and education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Wikipedia has spawned several sister projects,...</td>\n",
       "      <td>Wikipedia &gt; Sister projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>. Throughout the demonstrations, broadcasts by...</td>\n",
       "      <td>1989 Tiananmen Square protests and massacre &gt; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The visual effects for Wicked were made by Ind...</td>\n",
       "      <td>Wicked (2024 film) &gt; Post-production and visua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         chunk_text  \\\n",
       "0      0  In an interview with Wired magazine during the...   \n",
       "1      1  . He has described his upbringing as \"privileg...   \n",
       "2      2  Wikipedia has spawned several sister projects,...   \n",
       "3      3  . Throughout the demonstrations, broadcasts by...   \n",
       "4      4  The visual effects for Wicked were made by Ind...   \n",
       "\n",
       "                                     section_context  \n",
       "0   Charlie Kirk > Republican and pro-Trump activism  \n",
       "1          Zohran Mamdani > Early life and education  \n",
       "2                        Wikipedia > Sister projects  \n",
       "3  1989 Tiananmen Square protests and massacre > ...  \n",
       "4  Wicked (2024 film) > Post-production and visua...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create evaluation dataset for MLflow\n",
    "import pandas as pd\n",
    "\n",
    "eval_data = []\n",
    "for i, ex in enumerate(devset[:EVAL_SIZE]):\n",
    "    eval_data.append({\n",
    "        \"index\": i,\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"section_context\": ex.section_context,\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "print(f\"Created evaluation dataset with {len(eval_df)} examples\")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b02ec641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying statements for 10 chunks...\n",
      "======================================================================\n",
      "\n",
      "[1/10] Charlie Kirk > Republican and pro-Trump activism...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEVAL_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;241m.\u001b[39msection_context[:\u001b[38;5;241m50\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Extract statements\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mbaseline_extractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msection_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msection_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Classify each statement\u001b[39;00m\n\u001b[1;32m     26\u001b[0m result \u001b[38;5;241m=\u001b[39m classifier(\n\u001b[1;32m     27\u001b[0m     chunk_text\u001b[38;5;241m=\u001b[39mex\u001b[38;5;241m.\u001b[39mchunk_text,\n\u001b[1;32m     28\u001b[0m     section_context\u001b[38;5;241m=\u001b[39mex\u001b[38;5;241m.\u001b[39msection_context,\n\u001b[1;32m     29\u001b[0m     statements\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(pred\u001b[38;5;241m.\u001b[39mstatements),\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/utils/callback.py:346\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.sync_wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     ACTIVE_CALL_ID\u001b[38;5;241m.\u001b[39mset(parent_call_id)\n\u001b[0;32m--> 346\u001b[0m     \u001b[43m_execute_end_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/dspy/utils/callback.py:278\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>._execute_end_callbacks\u001b[0;34m(instance, fn, call_id, results, exception, callbacks)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m callbacks:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 278\u001b[0m         \u001b[43m_get_on_end_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcall_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when applying callback \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcallback\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms end handler on function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/dspy/callback.py:36\u001b[0m, in \u001b[0;36mskip_if_trace_disabled.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m get_autologging_config(FLAVOR_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_traces\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 36\u001b[0m         \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/dspy/callback.py:147\u001b[0m, in \u001b[0;36mMlflowCallback.on_module_end\u001b[0;34m(self, call_id, outputs, exception)\u001b[0m\n\u001b[1;32m    145\u001b[0m                 usage_data[TokenUsageKey\u001b[38;5;241m.\u001b[39mTOTAL_TOKENS] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m usage\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    146\u001b[0m             attributes[SpanAttributeKey\u001b[38;5;241m.\u001b[39mCHAT_USAGE] \u001b[38;5;241m=\u001b[39m usage_data\n\u001b[0;32m--> 147\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_span\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/dspy/callback.py:389\u001b[0m, in \u001b[0;36mMlflowCallback._end_span\u001b[0;34m(self, call_id, outputs, exception, attributes)\u001b[0m\n\u001b[1;32m    386\u001b[0m     st\u001b[38;5;241m.\u001b[39mspan\u001b[38;5;241m.\u001b[39mset_attributes(attributes)\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m     \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     detach_span_from_context(st\u001b[38;5;241m.\u001b[39mtoken)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/entities/span.py:656\u001b[0m, in \u001b[0;36mLiveSpan.end\u001b[0;34m(self, outputs, attributes, status, end_time_ns)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;66;03m# Apply span processors\u001b[39;00m\n\u001b[1;32m    654\u001b[0m     apply_span_processors(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 656\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_span\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_time_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    659\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    660\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to end span \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor full traceback, set logging level to debug.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    662\u001b[0m         exc_info\u001b[38;5;241m=\u001b[39m_logger\u001b[38;5;241m.\u001b[39misEnabledFor(logging\u001b[38;5;241m.\u001b[39mDEBUG),\n\u001b[1;32m    663\u001b[0m     )\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/opentelemetry/sdk/trace/__init__.py:948\u001b[0m, in \u001b[0;36mSpan.end\u001b[0;34m(self, end_time)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;28;01mif\u001b[39;00m end_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m time_ns()\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_span_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_end\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readable_span\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/opentelemetry/sdk/trace/__init__.py:175\u001b[0m, in \u001b[0;36mSynchronousMultiSpanProcessor.on_end\u001b[0;34m(self, span)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, span: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadableSpan\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_span_processors:\n\u001b[0;32m--> 175\u001b[0m         \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/tracing/processor/base_mlflow.py:114\u001b[0m, in \u001b[0;36mBaseMlflowSpanProcessor.on_end\u001b[0;34m(self, span)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrace data with request ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrace_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/opentelemetry/sdk/trace/export/__init__.py:109\u001b[0m, in \u001b[0;36mSimpleSpanProcessor.on_end\u001b[0;34m(self, span)\u001b[0m\n\u001b[1;32m    107\u001b[0m token \u001b[38;5;241m=\u001b[39m attach(set_value(_SUPPRESS_INSTRUMENTATION_KEY, \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan_exporter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-exception-caught\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/tracing/export/mlflow_v3.py:64\u001b[0m, in \u001b[0;36mMlflowV3SpanExporter.export\u001b[0;34m(self, spans)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_export_spans_incrementally:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_export_spans_incrementally(spans)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_traces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspans\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/tracing/export/mlflow_v3.py:157\u001b[0m, in \u001b[0;36mMlflowV3SpanExporter._export_traces\u001b[0;34m(self, spans)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_queue\u001b[38;5;241m.\u001b[39mput(\n\u001b[1;32m    150\u001b[0m         task\u001b[38;5;241m=\u001b[39mTask(\n\u001b[1;32m    151\u001b[0m             handler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_trace,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m         )\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager_trace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/tracing/export/mlflow_v3.py:195\u001b[0m, in \u001b[0;36mMlflowV3SpanExporter._log_trace\u001b[0;34m(self, trace, prompts)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[1;32m    194\u001b[0m     add_size_stats_to_trace_metadata(trace)\n\u001b[0;32m--> 195\u001b[0m     returned_trace_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_log_spans_to_artifacts(returned_trace_info):\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_upload_trace_data(returned_trace_info, trace\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/telemetry/track.py:30\u001b[0m, in \u001b[0;36mrecord_usage_event.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result  \u001b[38;5;66;03m# noqa: RET504\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/tracing/client.py:89\u001b[0m, in \u001b[0;36mTracingClient.start_trace\u001b[0;34m(self, trace_info)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@record_usage_event\u001b[39m(StartTraceEvent)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace_info: TraceInfo) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TraceInfo:\n\u001b[1;32m     80\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    Create a new trace in the backend.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m        The returned TraceInfoV3 object from the backend.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrace_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrace_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:368\u001b[0m, in \u001b[0;36mRestStore.start_trace\u001b[0;34m(self, trace_info)\u001b[0m\n\u001b[1;32m    365\u001b[0m req_body \u001b[38;5;241m=\u001b[39m message_to_json(StartTraceV3(trace\u001b[38;5;241m=\u001b[39mtrace\u001b[38;5;241m.\u001b[39mto_proto()))\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     response_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# NB: _call_endpoint doesn't handle versioning between v2 and v3 endpoint\u001b[39;49;00m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# yet, so manually passing the v3 endpoint here.\u001b[39;49;00m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mStartTraceV3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_V3_TRACE_REST_API_PATH_PREFIX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMLFLOW_ASYNC_TRACE_LOGGING_RETRY_TIMEOUT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TraceInfo\u001b[38;5;241m.\u001b[39mfrom_proto(response_proto\u001b[38;5;241m.\u001b[39mtrace\u001b[38;5;241m.\u001b[39mtrace_info)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MlflowException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/store/tracking/rest_store.py:208\u001b[0m, in \u001b[0;36mRestStore._call_endpoint\u001b[0;34m(self, api, json_body, endpoint, retry_timeout_seconds, response_proto)\u001b[0m\n\u001b[1;32m    206\u001b[0m     endpoint, method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_METHOD_TO_INFO[api]\n\u001b[1;32m    207\u001b[0m response_proto \u001b[38;5;241m=\u001b[39m response_proto \u001b[38;5;129;01mor\u001b[39;00m api\u001b[38;5;241m.\u001b[39mResponse()\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:594\u001b[0m, in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers, retry_timeout_seconds)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m     call_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m json_body\n\u001b[0;32m--> 594\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    596\u001b[0m response \u001b[38;5;241m=\u001b[39m verify_rest_response(response, endpoint)\n\u001b[1;32m    597\u001b[0m response_to_parse \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/rest_utils.py:236\u001b[0m, in \u001b[0;36mhttp_request\u001b[0;34m(host_creds, endpoint, method, max_retries, backoff_factor, backoff_jitter, extra_headers, retry_codes, timeout, raise_on_status, respect_retry_after_header, retry_timeout_seconds, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m fetch_auth(host_creds\u001b[38;5;241m.\u001b[39mauth)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_http_response_with_retries\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackoff_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackoff_jitter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraise_on_status\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost_creds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrespect_retry_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m to:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI request to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed with timeout exception \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mto\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m To increase the timeout, set the environment variable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMLFLOW_HTTP_REQUEST_TIMEOUT\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[38;5;124m to a larger value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mto\u001b[39;00m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/mlflow/utils/request_utils.py:237\u001b[0m, in \u001b[0;36m_get_http_response_with_retries\u001b[0;34m(method, url, max_retries, backoff_factor, backoff_jitter, retry_codes, raise_on_status, allow_redirects, respect_retry_after_header, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m env_value \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLFLOW_ALLOW_HTTP_REDIRECTS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    235\u001b[0m allow_redirects \u001b[38;5;241m=\u001b[39m env_value \u001b[38;5;28;01mif\u001b[39;00m allow_redirects \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m allow_redirects\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_redirects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/requests/adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:942\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    940\u001b[0m     retries\u001b[38;5;241m.\u001b[39msleep(response)\n\u001b[1;32m    941\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url)\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:942\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    940\u001b[0m     retries\u001b[38;5;241m.\u001b[39msleep(response)\n\u001b[1;32m    941\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url)\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "    \u001b[0;31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 942 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:942\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    940\u001b[0m     retries\u001b[38;5;241m.\u001b[39msleep(response)\n\u001b[1;32m    941\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url)\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:940\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    939\u001b[0m response\u001b[38;5;241m.\u001b[39mdrain_conn()\n\u001b[0;32m--> 940\u001b[0m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    943\u001b[0m     method,\n\u001b[1;32m    944\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    958\u001b[0m )\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/urllib3/util/retry.py:363\u001b[0m, in \u001b[0;36mRetry.sleep\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m slept:\n\u001b[1;32m    361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sleep_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspaces/wiki3-kg-project/.venv/lib/python3.10/site-packages/urllib3/util/retry.py:347\u001b[0m, in \u001b[0;36mRetry._sleep_backoff\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backoff \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackoff\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=tr-260a5962dd81b7f57d5911c6a8f1e091&amp;experiment_id=1&amp;version=3.7.0\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(trace_id=tr-260a5962dd81b7f57d5911c6a8f1e091)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Per-Statement Classification for ALL Evaluation Examples\n",
    "# ============================================================================\n",
    "# Uses StatementClassifier to get GOOD/BAD verdicts per statement\n",
    "\n",
    "from ontological_engineer import StatementClassifier, StatementClassification\n",
    "\n",
    "classifier = StatementClassifier()\n",
    "\n",
    "# Store all results for summary\n",
    "all_classification_results = []\n",
    "\n",
    "print(f\"Classifying statements for {EVAL_SIZE} chunks...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx, ex in enumerate(devset[:EVAL_SIZE]):\n",
    "    print(f\"\\n[{idx+1}/{EVAL_SIZE}] {ex.section_context[:50]}...\")\n",
    "    \n",
    "    # Extract statements\n",
    "    pred = baseline_extractor(\n",
    "        chunk_text=ex.chunk_text,\n",
    "        section_context=ex.section_context,\n",
    "    )\n",
    "    \n",
    "    # Classify each statement\n",
    "    result = classifier(\n",
    "        chunk_text=ex.chunk_text,\n",
    "        section_context=ex.section_context,\n",
    "        statements=list(pred.statements),\n",
    "    )\n",
    "    \n",
    "    # Store result\n",
    "    all_classification_results.append({\n",
    "        \"idx\": idx,\n",
    "        \"section\": ex.section_context,\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"statements\": list(pred.statements),\n",
    "        \"score\": result.score,\n",
    "        \"classifications\": result.classifications,\n",
    "        \"missing_facts\": result.missing_facts,\n",
    "    })\n",
    "    \n",
    "    # Show quick summary\n",
    "    good = sum(1 for c in result.classifications if c.is_good)\n",
    "    total = len(result.classifications)\n",
    "    print(f\"   ‚Üí {good}/{total} GOOD ({result.score:.0%})\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "avg_score = sum(r[\"score\"] for r in all_classification_results) / len(all_classification_results)\n",
    "total_good = sum(sum(1 for c in r[\"classifications\"] if c.is_good) for r in all_classification_results)\n",
    "total_bad = sum(sum(1 for c in r[\"classifications\"] if not c.is_good) for r in all_classification_results)\n",
    "print(f\"Average score: {avg_score:.1%}\")\n",
    "print(f\"Total GOOD: {total_good}, Total BAD: {total_bad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8c9996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StatementClassification(index=0, statement='[Charlie Kirk](/wiki/Charlie_Kirk) said in an interview with [Wired](/wiki/Wired_(magazine)) magazine during the [2016 Republican National Convention](/wiki/2016_Republican_National_Convention) that while he \"was not the world\\'s biggest [Donald Trump](/wiki/Donald_Trump) fan\", he would vote for him.', classification='GOOD', reason='atomic, accurate, links preserved'),\n",
       " StatementClassification(index=1, statement=\"[Charlie Kirk](/wiki/Charlie_Kirk) said that [Trump's](/wiki/Donald_Trump) candidacy made [Turning Point](/wiki/Turning_Point_(organization))'s mission more difficult.\", classification='GOOD', reason='atomic, accurate, links preserved'),\n",
       " StatementClassification(index=2, statement='[Charlie Kirk](/wiki/Charlie_Kirk) flipped to supporting [Trump](/wiki/Donald_Trump) at the convention.', classification='GOOD', reason='atomic, accurate, links preserved'),\n",
       " StatementClassification(index=3, statement='[Charlie Kirk](/wiki/Charlie_Kirk) spent the remainder of the [2016 campaign](/wiki/2016_United_States_presidential_election) assisting with travel and media arrangements for [Donald Trump Jr.](/wiki/Donald_Trump_Jr.).', classification='GOOD', reason='atomic, accurate, links preserved'),\n",
       " StatementClassification(index=4, statement='[Charlie Kirk](/wiki/Charlie_Kirk) participated in a [Fox News](/wiki/Fox_News)_event in October 2016 along with [Donald Trump Jr.](/wiki/Donald_Trump_Jr.), [Eric Trump](/wiki/Eric_Trump), and [Lara Trump](/wiki/Lara_Trump).', classification='GOOD', reason='atomic, accurate, links preserved'),\n",
       " StatementClassification(index=5, statement='[Charlie Kirk](/wiki/Charlie_Kirk) became chairman of [Students for Trump](/wiki/Students_for_Trump) in July 2019.', classification='GOOD', reason='atomic, accurate, links preserved'),\n",
       " StatementClassification(index=6, statement='[Students for Trump](/wiki/Students_for_Trump) had been acquired by [Turning Point Action](/wiki/Turning_Point_Action).', classification='GOOD', reason='atomic, accurate, links preserved'),\n",
       " StatementClassification(index=7, statement=\"The unsuccessful effort of [Kirk's](/wiki/Charlie_Kirk) youth mobilization campaign led [TPUSA](/wiki/Turning_Point_United_States_America) and the [Trump campaign](/wiki/Donald_Trump_2016_presidential_campaign) to blame each other for an overall decline in [Trump's](/wiki/Donald_Trump) youth support.\", classification='GOOD', reason='atomic, accurate, links preserved'),\n",
       " StatementClassification(index=8, statement='[Matthew Rosenberg](/wiki/Matthew_Rosenberg) and [Katie Rogers](/wiki/Katie_Rogers) wrote in [The New York Times](/wiki/The_New_York_Times) in April 2020 that [Charlie Kirk](/wiki/Charlie_Kirk) exemplifies \"walking the line between mainstream conservative opinion and outright disinformation\".', classification='GOOD', reason='atomic, accurate, links preserved'),\n",
       " StatementClassification(index=9, statement=\"[Charlie Kirk](/wiki/Charlie_Kirk) both amplifies the [president's](/wiki/Donald_Trump)'s message and helps shape it, according to [Matthew Rosenberg](/wiki/Matthew_Rosenberg) and [Katie Rogers](/wiki/Katie_Rogers).\", classification='GOOD', reason='atomic, accurate, links preserved')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_result.classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56db83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DETAILED PER-CHUNK RESULTS\n",
      "======================================================================\n",
      "\n",
      "üìÑ Chunk 0: Charlie Kirk > Republican and pro-Trump activism...\n",
      "   Score: 100% (10/10 GOOD)\n",
      "\n",
      "üìÑ Chunk 1: Zohran Mamdani > Early life and education...\n",
      "   Score: 100% (23/23 GOOD)\n",
      "\n",
      "üìÑ Chunk 2: Wikipedia > Sister projects...\n",
      "   Score: 100% (10/10 GOOD)\n",
      "\n",
      "üìÑ Chunk 3: 1989 Tiananmen Square protests and massacre > 1986 student d...\n",
      "   Score: 100% (6/6 GOOD)\n",
      "\n",
      "üìÑ Chunk 4: Wicked (2024 film) > Post-production and visual effects...\n",
      "   Score: 100% (16/16 GOOD)\n",
      "\n",
      "üìÑ Chunk 5: Wikipedia > Community...\n",
      "   Score: 80% (8/10 GOOD)\n",
      "   ‚ùå BAD statements:\n",
      "      [3] Wikipedia's preference for cohesiveness has been referred to as \"anti-elitism\"....\n",
      "          Reason: incomplete information, omits key context about the compromise and disregard of credentials that defines \"anti-elitism\"\n",
      "      [7] [Wikipedia](/wiki/Wikipedia) is therefore \"much like any traditional organizatio...\n",
      "          Reason: misrepresents Jimmy Wales' quote, editorializing rather than faithful to source\n",
      "\n",
      "üìÑ Chunk 6: Wikipedia > Sources...\n",
      "   Score: 0% (0/0 GOOD)\n",
      "\n",
      "üìÑ Chunk 7: Wikipedia > Explicit content...\n",
      "   Score: 100% (15/15 GOOD)\n",
      "\n",
      "üìÑ Chunk 8: Marjorie Taylor Greene > Runoff election...\n",
      "   Score: 100% (6/6 GOOD)\n",
      "\n",
      "üìÑ Chunk 9: Charlie Kirk > Books...\n",
      "   Score: 100% (17/17 GOOD)\n"
     ]
    }
   ],
   "source": [
    "# Display detailed results for each chunk\n",
    "print(\"DETAILED PER-CHUNK RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for r in all_classification_results:\n",
    "    good = sum(1 for c in r[\"classifications\"] if c.is_good)\n",
    "    bad = sum(1 for c in r[\"classifications\"] if not c.is_good)\n",
    "    total = len(r[\"classifications\"])\n",
    "    \n",
    "    print(f\"\\nüìÑ Chunk {r['idx']}: {r['section'][:60]}...\")\n",
    "    print(f\"   Score: {r['score']:.0%} ({good}/{total} GOOD)\")\n",
    "    \n",
    "    # Show BAD statements (these need attention)\n",
    "    bad_stmts = [c for c in r[\"classifications\"] if not c.is_good]\n",
    "    if bad_stmts:\n",
    "        print(f\"   ‚ùå BAD statements:\")\n",
    "        for c in bad_stmts:\n",
    "            print(f\"      [{c.index}] {c.statement[:80]}...\")\n",
    "            print(f\"          Reason: {c.reason}\")\n",
    "    \n",
    "    if r[\"missing_facts\"] and r[\"missing_facts\"].lower() != \"none\":\n",
    "        print(f\"   üìù Missing: {r['missing_facts'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c283f",
   "metadata": {},
   "source": [
    "### Export Annotations from MLflow\n",
    "\n",
    "After reviewing and labeling in the MLflow UI, export your annotations for judge improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23dcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest run: d01f36bee590481593cfe6511fc13f11\n",
      "Metrics: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24578/2900277544.py:22: FutureWarning: Parameter 'experiment_ids' is deprecated. Please use 'locations' instead.\n",
      "  traces = client.search_traces(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46 traces\n"
     ]
    }
   ],
   "source": [
    "# Load annotations from MLflow (after you've labeled them in the UI)\n",
    "# MLflow stores feedback as assessments on traces\n",
    "\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "# Get the latest evaluation run\n",
    "experiment = client.get_experiment_by_name(\"wiki3-kg-stage1-statements\")\n",
    "if experiment:\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"start_time DESC\"],\n",
    "        max_results=1,\n",
    "    )\n",
    "    \n",
    "    if runs:\n",
    "        latest_run = runs[0]\n",
    "        print(f\"Latest run: {latest_run.info.run_id}\")\n",
    "        print(f\"Metrics: {latest_run.data.metrics}\")\n",
    "        \n",
    "        # Get traces with assessments (human feedback)\n",
    "        try:\n",
    "            traces = client.search_traces(\n",
    "                experiment_ids=[experiment.experiment_id],\n",
    "                max_results=100,\n",
    "            )\n",
    "            print(f\"Found {len(traces)} traces\")\n",
    "        except Exception as e:\n",
    "            print(f\"Trace search error: {e}\")\n",
    "else:\n",
    "    print(\"No experiment found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52239b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation dataset to /workspaces/wiki3-kg-project/data/training/eval_dataset.json\n",
      "\n",
      "üìã Next steps for human feedback:\n",
      "\n",
      "1. Start MLflow server:\n",
      "   mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\n",
      "\n",
      "2. Open MLflow UI at http://127.0.0.1:5000\n",
      "\n",
      "3. Navigate to the experiment 'wiki3-kg-stage1-statements'\n",
      "\n",
      "4. Click on traces to review predictions\n",
      "\n",
      "5. Use the feedback/assessment features to label quality\n",
      "\n",
      "6. Export labeled data for judge improvement\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use human feedback to improve the judge\n",
    "# After collecting labels in MLflow, create DSPy training examples\n",
    "\n",
    "# For now, save the evaluation data for later use\n",
    "output_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "eval_df.to_json(output_dir / \"eval_dataset.json\", orient=\"records\", indent=2)\n",
    "print(f\"Saved evaluation dataset to {output_dir / 'eval_dataset.json'}\")\n",
    "\n",
    "print(\"\"\"\n",
    "üìã Next steps for human feedback:\n",
    "\n",
    "1. Start MLflow server:\n",
    "   mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\n",
    "\n",
    "2. Open MLflow UI at http://127.0.0.1:5000\n",
    "\n",
    "3. Navigate to the experiment 'wiki3-kg-stage1-statements'\n",
    "\n",
    "4. Click on traces to review predictions\n",
    "\n",
    "5. Use the feedback/assessment features to label quality\n",
    "\n",
    "6. Export labeled data for judge improvement\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b175b",
   "metadata": {},
   "source": [
    "## 10. MIPROv2 Prompt Optimization\n",
    "\n",
    "Use DSPy's MIPROv2 optimizer to improve the extractor's prompts.\n",
    "This uses the few-shot examples to bootstrap better demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f711918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import MIPROv2\n",
    "\n",
    "# Configure optimizer\n",
    "optimizer = MIPROv2(\n",
    "    metric=statement_quality_metric,\n",
    "    num_candidates=3,  # Number of prompt candidates to try\n",
    "    init_temperature=0.7,\n",
    ")\n",
    "\n",
    "# Use few-shot examples for bootstrapping demonstrations\n",
    "# Use training set for optimization\n",
    "TRAIN_SIZE = min(20, len(trainset))  # Limit for speed\n",
    "\n",
    "print(f\"Optimizing with {TRAIN_SIZE} training examples...\")\n",
    "print(f\"Using {len(selected_fewshot)} few-shot demos for bootstrapping...\")\n",
    "\n",
    "optimized_extractor = optimizer.compile(\n",
    "    StatementExtractor(),\n",
    "    trainset=trainset[:TRAIN_SIZE],\n",
    "    num_batches=2,\n",
    "    max_bootstrapped_demos=NUM_FEWSHOT,\n",
    "    # Provide few-shot examples as initial demos\n",
    "    # demos=selected_fewshot,  # Uncomment if supported\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d59aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate optimized extractor\n",
    "optimized_result = evaluator(optimized_extractor)\n",
    "optimized_score = optimized_result.score if hasattr(optimized_result, 'score') else float(optimized_result)\n",
    "\n",
    "print(f\"Baseline score:  {baseline_score:.2f}\")\n",
    "print(f\"Optimized score: {optimized_score:.2f}\")\n",
    "print(f\"Improvement:     {optimized_score - baseline_score:+.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fab06",
   "metadata": {},
   "source": [
    "## 11. Inspect Optimized Prompts\n",
    "\n",
    "See what prompts MIPROv2 discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a08e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the optimized module\n",
    "print(\"Optimized extractor configuration:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Try to access the optimized signature/demos\n",
    "if hasattr(optimized_extractor, 'demos'):\n",
    "    print(f\"\\nDemonstrations: {len(optimized_extractor.demos)}\")\n",
    "    for i, demo in enumerate(optimized_extractor.demos[:2], 1):\n",
    "        print(f\"  Demo {i}: {demo.section_context[:50]}...\")\n",
    "\n",
    "# Check for any instruction changes\n",
    "if hasattr(optimized_extractor, 'signature'):\n",
    "    print(f\"\\nSignature: {optimized_extractor.signature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0c651",
   "metadata": {},
   "source": [
    "## 12. Save Results\n",
    "\n",
    "Save the optimized extractor and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e45006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training metadata\n",
    "output_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save training results\n",
    "results = {\n",
    "    \"baseline_score\": baseline_score,\n",
    "    \"optimized_score\": optimized_score,\n",
    "    \"train_size\": TRAIN_SIZE,\n",
    "    \"eval_size\": EVAL_SIZE,\n",
    "    \"num_fewshot\": NUM_FEWSHOT,\n",
    "    \"pages_processed\": pages_processed,\n",
    "    \"total_chunks\": len(training_chunks),\n",
    "}\n",
    "\n",
    "with open(output_dir / \"stage1_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Saved results to {output_dir / 'stage1_results.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized extractor state\n",
    "try:\n",
    "    optimized_extractor.save(output_dir / \"optimized_extractor\")\n",
    "    print(f\"Saved optimized extractor to {output_dir / 'optimized_extractor'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save extractor state: {e}\")\n",
    "    # Alternative: save as JSON\n",
    "    if hasattr(optimized_extractor, 'dump_state'):\n",
    "        state = optimized_extractor.dump_state()\n",
    "        with open(output_dir / \"optimized_extractor_state.json\", \"w\") as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "        print(\"Saved extractor state as JSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec590ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save few-shot examples for reference\n",
    "fewshot_data = []\n",
    "for ex in selected_fewshot:\n",
    "    fewshot_data.append({\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"section_context\": ex.section_context,\n",
    "        \"statements\": list(ex.statements),\n",
    "    })\n",
    "\n",
    "with open(output_dir / \"fewshot_examples.json\", \"w\") as f:\n",
    "    json.dump(fewshot_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(fewshot_data)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e226c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. Loaded Albert Einstein as few-shot examples (seed/guidance)\n",
    "2. Fetched and chunked Wikipedia sample pages for training\n",
    "3. Established baseline extraction quality\n",
    "4. Ran MIPROv2 prompt optimization\n",
    "5. Saved the optimized extractor\n",
    "\n",
    "Next steps:\n",
    "- **Stage 2**: Schema matching with optimized statements\n",
    "- **Stage 3**: RDF generation training\n",
    "- **Arbor GRPO**: Fine-tune the full pipeline end-to-end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
