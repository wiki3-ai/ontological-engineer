{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6df303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Stage 1: Statement Extraction with DSPy\n",
    "# =============================================================================\n",
    "# This notebook orchestrates statement extraction training/evaluation.\n",
    "# All application logic is in ontological_engineer - this notebook coordinates.\n",
    "#\n",
    "# Outputs (with CID provenance):\n",
    "#   - data/training/chunks/*.ipynb - Chunked Wikipedia pages\n",
    "#   - data/training/stage1_config.json - Configuration with CID\n",
    "#   - data/training/trainset.json - Training examples with CID\n",
    "#   - data/training/devset.json - Dev examples with CID\n",
    "#   - data/training/fewshot_examples.json - Few-shot examples with CID\n",
    "#   - data/training/baseline_results.json - Baseline evaluation with CID\n",
    "#\n",
    "# Next: Run stage1_optimize.ipynb for MIPROv2 optimization\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/workspaces/wiki3-kg-project')\n",
    "\n",
    "import dspy\n",
    "import json\n",
    "from pathlib import Path\n",
    "from random import shuffle, seed as random_seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ontological_engineer import (\n",
    "    # LM Configuration\n",
    "    configure_lm,\n",
    "    # DSPy Modules\n",
    "    StatementExtractor,\n",
    "    StatementQualityJudge,\n",
    "    StatementClassifier,\n",
    "    StatementClassification,\n",
    "    # Data Loading (from provenance-tracked notebooks)\n",
    "    WikipediaPage,\n",
    "    WikipediaChunk,\n",
    "    load_sample_from_notebook,\n",
    "    load_chunks_from_notebook,\n",
    "    # Processing (all logic in module!)\n",
    "    process_wikipedia_sample,\n",
    "    fetch_page_content,\n",
    "    chunk_article,\n",
    "    # Provenance notebook generation\n",
    "    save_notebook,\n",
    "    get_processed_chunk_cids,\n",
    ")\n",
    "from ontological_engineer.judges import statement_quality_metric\n",
    "from ontological_engineer.training.bootstrap import (\n",
    "    load_chunks_from_notebook as load_albert_chunks,\n",
    "    load_facts_from_notebook,\n",
    "    create_training_examples,\n",
    ")\n",
    "from ontological_engineer.training import (\n",
    "    save_stage1_config,\n",
    "    save_trainset,\n",
    "    save_devset,\n",
    "    save_fewshot_examples,\n",
    "    check_baseline_cache,\n",
    "    save_baseline_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f86eb6",
   "metadata": {},
   "source": [
    "## 1. Configure Language Model\n",
    "\n",
    "Connect to LM Studio running Qwen-30B (or your preferred model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e839f659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured LM: <dspy.clients.lm.LM object at 0xffff5aa0a6e0>\n"
     ]
    }
   ],
   "source": [
    "# Configure the LM (defaults to Qwen-30B via LM Studio)\n",
    "MODEL = \"qwen/qwen3-coder-30b\"\n",
    "API_BASE = \"http://host.docker.internal:1234/v1\"\n",
    "TEMPERATURE = 0.7\n",
    "NUM_FEWSHOT = 3\n",
    "\n",
    "lm = configure_lm(\n",
    "    model=MODEL,\n",
    "    api_base=API_BASE,\n",
    "    temperature=TEMPERATURE,\n",
    ")\n",
    "\n",
    "print(f\"Configured LM: {lm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185bc4f7",
   "metadata": {},
   "source": [
    "## 2. Load Few-Shot Examples (Albert Einstein)\n",
    "\n",
    "Albert Einstein is our gold-standard example. These chunks and their extracted facts\n",
    "serve as few-shot demonstrations for the extractor and judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b715a0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 63 chunks from Albert Einstein\n",
      "Loaded 19 fact sets\n",
      "Created 19 few-shot examples\n"
     ]
    }
   ],
   "source": [
    "# Load Albert Einstein data for few-shot examples\n",
    "fewshot_dir = Path(\"/workspaces/wiki3-kg-project/data/albert_einstein/20251218_231446\")\n",
    "\n",
    "# Use load_albert_chunks (from bootstrap module) - NOT load_chunks_from_notebook\n",
    "fewshot_chunks = load_albert_chunks(fewshot_dir / \"chunks.ipynb\")\n",
    "fewshot_facts = load_facts_from_notebook(fewshot_dir / \"facts.ipynb\")\n",
    "\n",
    "print(f\"Loaded {len(fewshot_chunks)} chunks from Albert Einstein\")\n",
    "print(f\"Loaded {len(fewshot_facts)} fact sets\")\n",
    "\n",
    "# Create few-shot examples\n",
    "fewshot_examples = create_training_examples(fewshot_chunks, fewshot_facts)\n",
    "print(f\"Created {len(fewshot_examples)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5911ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample few-shot example:\n",
      "  Context: Albert Einstein > Introduction\n",
      "  Text: Albert Einstein (14 March 1879 ‚Äì 18 April 1955) was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory...\n",
      "  Statements: 28 items\n",
      "    - Albert Einstein was a German-born theoretical physicist.\n",
      "    - Albert Einstein developed the theory of relativity.\n",
      "    - Albert Einstein made important contributions to quantum theory.\n"
     ]
    }
   ],
   "source": [
    "# Show a few-shot example\n",
    "if fewshot_examples:\n",
    "    ex = fewshot_examples[0]\n",
    "    print(\"Sample few-shot example:\")\n",
    "    print(f\"  Context: {ex.section_context}\")\n",
    "    print(f\"  Text: {ex.chunk_text[:200]}...\")\n",
    "    print(f\"  Statements: {len(ex.statements)} items\")\n",
    "    for stmt in ex.statements[:3]:\n",
    "        print(f\"    - {stmt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb06944",
   "metadata": {},
   "source": [
    "## 3. Load Wikipedia Sample for Training\n",
    "\n",
    "Load the 100-page Wikipedia sample from the provenance-tracked notebook.\n",
    "If the notebook doesn't exist, fall back to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9640f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 100 pages from provenance-tracked notebook\n",
      "   Source: /workspaces/wiki3-kg-project/data/training/wikipedia_sample.ipynb\n",
      "\n",
      "First 10 pages:\n",
      "  - Zohran Mamdani (9,344,963 views)\n",
      "  - ChatGPT (3,639,485 views)\n",
      "  - James A. Garfield (3,524,531 views)\n",
      "  - 1989 Tiananmen Square protests and massacre (2,867,005 views)\n",
      "  - 2025 Bihar Legislative Assembly election (2,555,071 views)\n",
      "  - Mira Nair (2,503,516 views)\n",
      "  - Dick Cheney (2,186,840 views)\n",
      "  - 2026 FIFA World Cup (2,155,565 views)\n",
      "  - 1xBet (1,831,684 views)\n",
      "  - Survivor Series: WarGames (2025) (1,590,263 views)\n"
     ]
    }
   ],
   "source": [
    "# Load the Wikipedia sample (prefer provenance-tracked notebook)\n",
    "sample_notebook = Path(\"/workspaces/wiki3-kg-project/data/training/wikipedia_sample.ipynb\")\n",
    "sample_json = Path(\"/workspaces/wiki3-kg-project/data/training/wikipedia_sample.json\")\n",
    "\n",
    "if sample_notebook.exists():\n",
    "    # Load from provenance-tracked notebook\n",
    "    wiki_pages = load_sample_from_notebook(sample_notebook)\n",
    "    print(f\"‚úÖ Loaded {len(wiki_pages)} pages from provenance-tracked notebook\")\n",
    "    print(f\"   Source: {sample_notebook}\")\n",
    "elif sample_json.exists():\n",
    "    # Fall back to JSON format\n",
    "    with open(sample_json) as f:\n",
    "        wiki_sample = json.load(f)\n",
    "    wiki_pages = [WikipediaPage(title=p['title'], views=p['views']) for p in wiki_sample['pages']]\n",
    "    print(f\"‚ö†Ô∏è  Loaded {len(wiki_pages)} pages from JSON (no provenance)\")\n",
    "    print(f\"   Run sample_wikipedia_pages.ipynb to generate provenance-tracked version\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No Wikipedia sample found. Run sample_wikipedia_pages.ipynb first.\")\n",
    "\n",
    "print(f\"\\nFirst 10 pages:\")\n",
    "for p in wiki_pages[:10]:\n",
    "    print(f\"  - {p.title} ({p.views:,} views)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceab17fa",
   "metadata": {},
   "source": [
    "## 4. Fetch and Chunk Wikipedia Pages (with CID Provenance)\n",
    "\n",
    "Fetch page content and chunk it. Each page's chunks are saved to a \n",
    "provenance-tracked notebook with CID signatures.\n",
    "\n",
    "**Note**: Uses `fetch_page_content` and `chunk_article` from `ontological_engineer` - \n",
    "no application logic defined in this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be01db74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on: Zohran Mamdani\n",
      "  ‚Üí 20 chunks (filtered by min_length=60)\n",
      "  First chunk preview: Zohran Kwame Mamdani (born October 18, 1991) is an American politician who is the mayor-elect of New York City. A member of the Democratic Party and the Democratic Socialists of America, he is set to ...\n"
     ]
    }
   ],
   "source": [
    "# Processing parameters\n",
    "chunks_dir = Path(\"/workspaces/wiki3-kg-project/data/training/chunks\")\n",
    "MAX_PAGES = len(wiki_pages)\n",
    "MIN_CHUNK_LENGTH = 60  # Skip very short chunks\n",
    "\n",
    "# Quick test on one page first\n",
    "test_page = wiki_pages[0]\n",
    "print(f\"Testing on: {test_page.title}\")\n",
    "\n",
    "content = fetch_page_content(test_page.title)\n",
    "if content:\n",
    "    chunks = chunk_article(test_page.title, content)\n",
    "    chunks = [c for c in chunks if len(c.text) >= MIN_CHUNK_LENGTH]\n",
    "    print(f\"  ‚Üí {len(chunks)} chunks (filtered by min_length={MIN_CHUNK_LENGTH})\")\n",
    "    if chunks:\n",
    "        print(f\"  First chunk preview: {chunks[0].text[:200]}...\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è Could not fetch content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "886bea3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pages: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 169.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Processed 99 pages\n",
      "   Total training chunks: 3428\n",
      "   Chunks saved to: /workspaces/wiki3-kg-project/data/training/chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all pages - logic is in process_wikipedia_sample()\n",
    "# Handles: fetching, chunking, saving with CID provenance, incremental processing\n",
    "\n",
    "training_chunks, pages_processed = process_wikipedia_sample(\n",
    "    pages=wiki_pages,\n",
    "    output_dir=chunks_dir,\n",
    "    max_pages=MAX_PAGES,\n",
    "    min_chunk_length=MIN_CHUNK_LENGTH,\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Processed {pages_processed} pages\")\n",
    "print(f\"   Total training chunks: {len(training_chunks)}\")\n",
    "print(f\"   Chunks saved to: {chunks_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf50dd2",
   "metadata": {},
   "source": [
    "## 5. Initialize Extractor with Few-Shot Examples\n",
    "\n",
    "Create the statement extractor and provide Albert Einstein examples as demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e38b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 few-shot examples:\n",
      "  1. Albert Einstein > Introduction... (28 statements)\n",
      "  2. Albert Einstein > Life and career > Personal views... (28 statements)\n",
      "  3. Albert Einstein > Introduction... (27 statements)\n"
     ]
    }
   ],
   "source": [
    "# Select best few-shot examples (ones with good variety of statements)\n",
    "NUM_FEWSHOT = 3\n",
    "\n",
    "# Sort by statement count to get diverse examples\n",
    "sorted_fewshot = sorted(fewshot_examples, key=lambda x: len(x.statements), reverse=True)\n",
    "selected_fewshot = sorted_fewshot[:NUM_FEWSHOT]\n",
    "\n",
    "print(f\"Selected {len(selected_fewshot)} few-shot examples:\")\n",
    "for i, ex in enumerate(selected_fewshot, 1):\n",
    "    print(f\"  {i}. {ex.section_context[:50]}... ({len(ex.statements)} statements)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9896eb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor initialized\n",
      "Few-shot examples available: 3\n"
     ]
    }
   ],
   "source": [
    "# Create extractor with few-shot demonstrations\n",
    "extractor = StatementExtractor()\n",
    "\n",
    "# In DSPy, we can provide demonstrations directly\n",
    "# The few-shot examples will be used by MIPROv2 for bootstrapping\n",
    "print(\"Extractor initialized\")\n",
    "print(f\"Few-shot examples available: {len(selected_fewshot)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4419625",
   "metadata": {},
   "source": [
    "## 6. Test Extraction on Training Sample\n",
    "\n",
    "Run the extractor on a few training chunks to verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ea088a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on: Zohran Mamdani > Zohran Mamdani\n",
      "Text: Zohran Kwame Mamdani (born October 18, 1991) is an American politician who is the mayor-elect of New York City. A member of the Democratic Party and the Democratic Socialists of America, he is set to become New York's first Muslim and Asian American mayor. Mamdani has served as a member of the New Y...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Extracted 22 statements:\n",
      "  1. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) was born on October 18, 1991.\n",
      "  2. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is an American politician.\n",
      "  3. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is the mayor-elect of [New York City](/wiki/New_York_City).\n",
      "  4. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is a member of the [Democratic Party](/wiki/Democratic_Party).\n",
      "  5. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is a member of the [Democratic Socialists of America](/wiki/Democratic_Socialists_of_America).\n",
      "  6. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is set to become [New York City](/wiki/New_York_City)'s first Muslim mayor.\n",
      "  7. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) is set to become [New York City](/wiki/New_York_City)'s first Asian American mayor.\n",
      "  8. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) has served as a member of the [New York State Assembly](/wiki/New_York_State_Assembly) for the 36th district since 2021.\n",
      "  9. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) represents the [Queens](/wiki/Queens) neighborhood of [Astoria](/wiki/Astoria,_New_York).\n",
      "  10. [Zohran Kwame Mamdani](/wiki/Zohran_Kwame_Mamdani) was born in [Kampala](/wiki/Kampala), [Uganda](/wiki/Uganda).\n",
      "  ... and 12 more\n"
     ]
    }
   ],
   "source": [
    "# Test on a training chunk\n",
    "if training_chunks:\n",
    "    test_chunk = training_chunks[0]\n",
    "    \n",
    "    print(f\"Testing on: {test_chunk.section_context}\")\n",
    "    print(f\"Text: {test_chunk.text[:300]}...\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    result = extractor(\n",
    "        chunk_text=test_chunk.text,\n",
    "        section_context=test_chunk.section_context,\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracted {len(result.statements)} statements:\")\n",
    "    for i, stmt in enumerate(result.statements[:10], 1):\n",
    "        print(f\"  {i}. {stmt}\")\n",
    "    if len(result.statements) > 10:\n",
    "        print(f\"  ... and {len(result.statements) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a247a7a",
   "metadata": {},
   "source": [
    "## 7. Create Training Dataset\n",
    "\n",
    "Convert chunks into DSPy examples. For training, we need to generate initial extractions\n",
    "that can be scored and optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fab9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 2742 examples\n",
      "Dev set: 686 examples\n",
      "\n",
      "‚úÖ Saved artifacts with provenance:\n",
      "   Config CID: bafkreif4oalzea27khswyqty3vuiziahmzetluaagguhm3435hou7wpgaa\n",
      "   Trainset CID: bafkreihisr4j4qzgb3gbtjoz6d3frwmrcnif2foi26vedicxkkbk3tn5si\n",
      "   Devset CID: bafkreictctqrjhp2ikuuoinyim46ddus2epcbrxj7brjfruoq4nlp53yhq\n",
      "   Fewshot CID: bafkreibsv3kwtrlkob2nzudlmhpql2fpdxtv7ezommob7rqhiqji5elh6i\n"
     ]
    }
   ],
   "source": [
    "# Create training examples (without labels - we'll generate and judge them)\n",
    "# For DSPy optimization, we just need the inputs\n",
    "\n",
    "random_seed(42)  # For reproducibility\n",
    "\n",
    "# Convert WikipediaChunk objects to DSPy examples\n",
    "trainset_chunks = list(training_chunks)  # Make a copy\n",
    "shuffle(trainset_chunks)\n",
    "\n",
    "trainset = []\n",
    "for chunk in trainset_chunks:\n",
    "    ex = dspy.Example(\n",
    "        chunk_text=chunk.text,\n",
    "        section_context=chunk.section_context,\n",
    "    ).with_inputs('chunk_text', 'section_context')\n",
    "    trainset.append(ex)\n",
    "\n",
    "# Split into train/dev\n",
    "split_idx = int(len(trainset) * 0.8)\n",
    "devset = trainset[split_idx:]\n",
    "trainset = trainset[:split_idx]\n",
    "\n",
    "print(f\"Training set: {len(trainset)} examples\")\n",
    "print(f\"Dev set: {len(devset)} examples\")\n",
    "\n",
    "# Save datasets with CID provenance for stage1_optimize.ipynb\n",
    "training_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "\n",
    "trainset_cid = save_trainset(trainset, training_dir)\n",
    "devset_cid = save_devset(devset, training_dir)\n",
    "fewshot_cid = save_fewshot_examples(selected_fewshot, training_dir)\n",
    "\n",
    "# Save config with CID provenance\n",
    "config = save_stage1_config(\n",
    "    output_dir=training_dir,\n",
    "    model=MODEL,\n",
    "    api_base=API_BASE,\n",
    "    temperature=TEMPERATURE,\n",
    "    num_fewshot=NUM_FEWSHOT,\n",
    "    train_size=len(trainset),\n",
    "    dev_size=len(devset),\n",
    "    pages_processed=pages_processed,\n",
    "    total_chunks=len(training_chunks),\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Saved artifacts with provenance:\")\n",
    "print(f\"   Config CID: {config['cid']}\")\n",
    "print(f\"   Trainset CID: {trainset_cid}\")\n",
    "print(f\"   Devset CID: {devset_cid}\")\n",
    "print(f\"   Fewshot CID: {fewshot_cid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda75e24",
   "metadata": {},
   "source": [
    "## 8. Initialize Judge with Few-Shot Guidance\n",
    "\n",
    "The judge scores extraction quality. We use Albert Einstein examples to calibrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "792d6e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge calibration on few-shot example:\n",
      "  Completeness:      0.95\n",
      "  Atomicity:         0.85\n",
      "  Accuracy:          0.95\n",
      "  Link preservation: 1.00\n",
      "  ---\n",
      "  Weighted score:    0.94\n"
     ]
    }
   ],
   "source": [
    "# Initialize judge\n",
    "judge = StatementQualityJudge()\n",
    "\n",
    "# Test judge on a known good example (Albert Einstein few-shot)\n",
    "if selected_fewshot:\n",
    "    test_ex = selected_fewshot[0]\n",
    "    \n",
    "    evaluation = judge(\n",
    "        chunk_text=test_ex.chunk_text,\n",
    "        section_context=test_ex.section_context,\n",
    "        statements=test_ex.statements,\n",
    "    )\n",
    "    \n",
    "    print(\"Judge calibration on few-shot example:\")\n",
    "    print(f\"  Completeness:      {evaluation.completeness:.2f}\")\n",
    "    print(f\"  Atomicity:         {evaluation.atomicity:.2f}\")\n",
    "    print(f\"  Accuracy:          {evaluation.accuracy:.2f}\")\n",
    "    print(f\"  Link preservation: {evaluation.link_preservation:.2f}\")\n",
    "    print(f\"  ---\")\n",
    "    print(f\"  Weighted score:    {evaluation.weighted_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649d2fc",
   "metadata": {},
   "source": [
    "## 9. Baseline Evaluation\n",
    "\n",
    "Evaluate the unoptimized extractor on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f230a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extractor CID: bafkreifceqfihybmpbxhqo4ox2ubndggakeuu4wbi7lzepi53xypi5wbnm\n",
      "Computing baseline evaluation (686 examples)...\n",
      "Average Metric: 60.84 / 65 (93.6%):   9%|‚ñâ         | 65/686 [07:41<1:54:44, 11.09s/it]"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline on dev set (with CID-based cache check)\n",
    "from ontological_engineer.training import compute_module_cid\n",
    "\n",
    "EVAL_SIZE = len(devset)\n",
    "\n",
    "# Create baseline extractor and compute its CID\n",
    "baseline_extractor = StatementExtractor()\n",
    "extractor_cid = compute_module_cid(baseline_extractor)\n",
    "print(f\"Extractor CID: {extractor_cid}\")\n",
    "\n",
    "# Check if we already have results for these exact inputs (including extractor definition)\n",
    "cached_baseline = check_baseline_cache(\n",
    "    training_dir=training_dir,\n",
    "    config_cid=config['cid'],\n",
    "    devset_cid=devset_cid,\n",
    "    eval_size=EVAL_SIZE,\n",
    "    extractor_cid=extractor_cid,\n",
    ")\n",
    "\n",
    "if cached_baseline:\n",
    "    # Use cached results - inputs match!\n",
    "    baseline_score = cached_baseline['score']\n",
    "    baseline_cid = cached_baseline['cid']\n",
    "    print(f\"‚úÖ Using cached baseline results (inputs unchanged)\")\n",
    "    print(f\"   Score: {baseline_score:.2f}\")\n",
    "    print(f\"   CID: {baseline_cid}\")\n",
    "else:\n",
    "    # Compute baseline - inputs changed or no cache\n",
    "    print(f\"Computing baseline evaluation ({EVAL_SIZE} examples)...\")\n",
    "    \n",
    "    evaluator = dspy.Evaluate(\n",
    "        devset=devset[:EVAL_SIZE],\n",
    "        metric=statement_quality_metric,\n",
    "        num_threads=1,\n",
    "        display_progress=True,\n",
    "    )\n",
    "\n",
    "    baseline_result = evaluator(baseline_extractor)\n",
    "\n",
    "    baseline_score = baseline_result.score if hasattr(baseline_result, 'score') else float(baseline_result)\n",
    "    print(f\"\\nBaseline quality score: {baseline_score:.2f}\")\n",
    "\n",
    "    # Save baseline results with CID provenance (includes input_cid for cache validation)\n",
    "    baseline_cid = save_baseline_results(\n",
    "        output_dir=training_dir,\n",
    "        score=baseline_score,\n",
    "        eval_size=EVAL_SIZE,\n",
    "        config_cid=config['cid'],\n",
    "        extractor_cid=extractor_cid,\n",
    "        devset_cid=devset_cid,\n",
    "    )\n",
    "    print(f\"Saved baseline results CID: {baseline_cid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e36a0",
   "metadata": {},
   "source": [
    "## 9b. MLflow Observability Setup\n",
    "\n",
    "MLflow provides tracing, evaluation, and human feedback tools for DSPy pipelines.\n",
    "\n",
    "### Quick Setup (One-time)\n",
    "\n",
    "1. **Install MLflow** (already in requirements or run cell below)\n",
    "2. **Start the MLflow server** in a terminal:\n",
    "   ```bash\n",
    "   cd /workspaces/wiki3-kg-project\n",
    "   mlflow server \\\n",
    "     --backend-store-uri sqlite:///mlflow.sqlite \\\n",
    "     --default-artifact-root ./mlflow-artifacts \\\n",
    "     --host 0.0.0.0 \\\n",
    "     --port 5000\n",
    "   ```\n",
    "3. **Open the UI** at http://localhost:5000 (or via VS Code port forwarding)\n",
    "\n",
    "### What MLflow Provides\n",
    "- **Tracing**: See every LM call, inputs, outputs, latency\n",
    "- **Evaluation**: Compare model versions side-by-side\n",
    "- **Human Feedback**: Add labels/assessments directly in the UI\n",
    "- **Experiment Tracking**: Track metrics across optimization runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87efb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MLflow Setup for DSPy Optimization Tracking\n",
    "# =============================================================================\n",
    "# Reference: https://dspy.ai/tutorials/optimizer_tracking/\n",
    "#\n",
    "# Prerequisites:\n",
    "#   1. Install: pip install \"mlflow>=2.21.1\"\n",
    "#   2. Start server (use SQL store for tracing):\n",
    "#      mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\n",
    "#   3. Open UI: http://localhost:5000\n",
    "# =============================================================================\n",
    "\n",
    "import mlflow\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"http://127.0.0.1:5000\"\n",
    "\n",
    "try:\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    mlflow.set_experiment(\"wiki3-kg-stage1-statements\")\n",
    "    \n",
    "    # Enable autologging with full optimizer tracking (per DSPy docs)\n",
    "    # https://dspy.ai/tutorials/optimizer_tracking/\n",
    "    mlflow.dspy.autolog(\n",
    "        log_compiles=True,           # Track optimization process\n",
    "        log_evals=True,              # Track evaluation results  \n",
    "        log_traces_from_compile=True # Track program traces during optimization\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ MLflow configured successfully\")\n",
    "    print(f\"   Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "    print(f\"   Experiment: wiki3-kg-stage1-statements\")\n",
    "    print(f\"\\nüìä Open MLflow UI: {MLFLOW_TRACKING_URI}\")\n",
    "    MLFLOW_ENABLED = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  MLflow not available: {e}\")\n",
    "    print(f\"\\nüí° To enable MLflow, start the server:\")\n",
    "    print(f\"   mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\")\n",
    "    MLFLOW_ENABLED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation with MLflow tracing (if enabled)\n",
    "# Each prediction creates a trace viewable in the MLflow UI\n",
    "\n",
    "if MLFLOW_ENABLED:\n",
    "    with mlflow.start_run(run_name=\"baseline_evaluation\"):\n",
    "        # Log parameters for reproducibility\n",
    "        mlflow.log_param(\"eval_size\", EVAL_SIZE)\n",
    "        mlflow.log_param(\"model\", \"qwen/qwen3-coder-30b\")\n",
    "        mlflow.log_param(\"num_fewshot\", NUM_FEWSHOT)\n",
    "        \n",
    "        # Run extractions on dev set - each one is traced\n",
    "        results = []\n",
    "        for i, ex in enumerate(tqdm(devset[:EVAL_SIZE], desc=\"Evaluating\")):\n",
    "            with mlflow.start_span(name=f\"example_{i}\") as span:\n",
    "                # Run extraction\n",
    "                pred = baseline_extractor(\n",
    "                    chunk_text=ex.chunk_text,\n",
    "                    section_context=ex.section_context,\n",
    "                )\n",
    "                \n",
    "                # Run judge\n",
    "                eval_result = judge(\n",
    "                    chunk_text=ex.chunk_text,\n",
    "                    section_context=ex.section_context,\n",
    "                    statements=pred.statements,\n",
    "                )\n",
    "                \n",
    "                # Log to span for MLflow UI review\n",
    "                span.set_inputs({\n",
    "                    \"chunk_text\": ex.chunk_text[:500],\n",
    "                    \"section_context\": ex.section_context,\n",
    "                })\n",
    "                span.set_outputs({\n",
    "                    \"statements\": list(pred.statements),\n",
    "                    \"completeness\": float(eval_result.completeness),\n",
    "                    \"atomicity\": float(eval_result.atomicity),\n",
    "                    \"accuracy\": float(eval_result.accuracy),\n",
    "                    \"link_preservation\": float(eval_result.link_preservation),\n",
    "                    \"weighted_score\": float(eval_result.weighted_score),\n",
    "                    \"reasoning\": eval_result.reasoning,\n",
    "                })\n",
    "                \n",
    "                results.append({\n",
    "                    \"index\": i,\n",
    "                    \"score\": float(eval_result.weighted_score),\n",
    "                })\n",
    "        \n",
    "        # Log aggregate metrics\n",
    "        avg_score = sum(r[\"score\"] for r in results) / len(results)\n",
    "        mlflow.log_metric(\"avg_quality_score\", avg_score)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Evaluation complete!\")\n",
    "        print(f\"   Average score: {avg_score:.2f}\")\n",
    "        print(f\"   Traces logged: {len(results)}\")\n",
    "        print(f\"\\nüìä Review in MLflow UI: {MLFLOW_TRACKING_URI}\")\n",
    "        print(f\"   ‚Üí Click 'Traces' tab to see all predictions\")\n",
    "        print(f\"   ‚Üí Click individual traces to review inputs/outputs\")\n",
    "        print(f\"   ‚Üí Use 'Feedback' to add human labels\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping MLflow evaluation (server not running)\")\n",
    "    print(\"   Run baseline evaluation with dspy.Evaluate instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use dspy.inspect_history() for quick debugging\n",
    "# This shows recent LM calls without needing MLflow server\n",
    "\n",
    "print(\"Recent LM calls (use MLflow UI for full traces):\")\n",
    "print(\"=\" * 60)\n",
    "dspy.inspect_history(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d708ef8",
   "metadata": {},
   "source": [
    "### MLflow Evaluation with Human Feedback\n",
    "\n",
    "Use MLflow's evaluation API to systematically review predictions and collect human labels.\n",
    "The MLflow UI provides a proper interface for reviewing and annotating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de353ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation dataset for MLflow\n",
    "import pandas as pd\n",
    "\n",
    "eval_data = []\n",
    "for i, ex in enumerate(devset[:EVAL_SIZE]):\n",
    "    eval_data.append({\n",
    "        \"index\": i,\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"section_context\": ex.section_context,\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(eval_data)\n",
    "print(f\"Created evaluation dataset with {len(eval_df)} examples\")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02ec641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Per-Statement Classification for ALL Evaluation Examples\n",
    "# ============================================================================\n",
    "# Uses StatementClassifier to get GOOD/BAD verdicts per statement\n",
    "\n",
    "from ontological_engineer import StatementClassifier, StatementClassification\n",
    "\n",
    "classifier = StatementClassifier()\n",
    "\n",
    "# Store all results for summary\n",
    "all_classification_results = []\n",
    "\n",
    "print(f\"Classifying statements for {EVAL_SIZE} chunks...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx, ex in enumerate(devset[:EVAL_SIZE]):\n",
    "    print(f\"\\n[{idx+1}/{EVAL_SIZE}] {ex.section_context[:50]}...\")\n",
    "    \n",
    "    # Extract statements\n",
    "    pred = baseline_extractor(\n",
    "        chunk_text=ex.chunk_text,\n",
    "        section_context=ex.section_context,\n",
    "    )\n",
    "    \n",
    "    # Classify each statement\n",
    "    result = classifier(\n",
    "        chunk_text=ex.chunk_text,\n",
    "        section_context=ex.section_context,\n",
    "        statements=list(pred.statements),\n",
    "    )\n",
    "    \n",
    "    # Store result\n",
    "    all_classification_results.append({\n",
    "        \"idx\": idx,\n",
    "        \"section\": ex.section_context,\n",
    "        \"chunk_text\": ex.chunk_text,\n",
    "        \"statements\": list(pred.statements),\n",
    "        \"score\": result.score,\n",
    "        \"classifications\": result.classifications,\n",
    "        \"missing_facts\": result.missing_facts,\n",
    "    })\n",
    "    \n",
    "    # Show quick summary\n",
    "    good = sum(1 for c in result.classifications if c.is_good)\n",
    "    total = len(result.classifications)\n",
    "    print(f\"   ‚Üí {good}/{total} GOOD ({result.score:.0%})\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "avg_score = sum(r[\"score\"] for r in all_classification_results) / len(all_classification_results)\n",
    "total_good = sum(sum(1 for c in r[\"classifications\"] if c.is_good) for r in all_classification_results)\n",
    "total_bad = sum(sum(1 for c in r[\"classifications\"] if not c.is_good) for r in all_classification_results)\n",
    "print(f\"Average score: {avg_score:.1%}\")\n",
    "print(f\"Total GOOD: {total_good}, Total BAD: {total_bad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56db83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed results for each chunk\n",
    "print(\"DETAILED PER-CHUNK RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for r in all_classification_results:\n",
    "    good = sum(1 for c in r[\"classifications\"] if c.is_good)\n",
    "    bad = sum(1 for c in r[\"classifications\"] if not c.is_good)\n",
    "    total = len(r[\"classifications\"])\n",
    "    \n",
    "    print(f\"\\nüìÑ Chunk {r['idx']}: {r['section'][:60]}...\")\n",
    "    print(f\"   Score: {r['score']:.0%} ({good}/{total} GOOD)\")\n",
    "    \n",
    "    # Show BAD statements (these need attention)\n",
    "    bad_stmts = [c for c in r[\"classifications\"] if not c.is_good]\n",
    "    if bad_stmts:\n",
    "        print(f\"   ‚ùå BAD statements:\")\n",
    "        for c in bad_stmts:\n",
    "            print(f\"      [{c.index}] {c.statement[:80]}...\")\n",
    "            print(f\"          Reason: {c.reason}\")\n",
    "    \n",
    "    if r[\"missing_facts\"] and r[\"missing_facts\"].lower() != \"none\":\n",
    "        print(f\"   üìù Missing: {r['missing_facts'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c283f",
   "metadata": {},
   "source": [
    "### Export Annotations from MLflow\n",
    "\n",
    "After reviewing and labeling in the MLflow UI, export your annotations for judge improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e23dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from MLflow (after you've labeled them in the UI)\n",
    "# MLflow stores feedback as assessments on traces\n",
    "\n",
    "client = mlflow.MlflowClient()\n",
    "\n",
    "# Get the latest evaluation run\n",
    "experiment = client.get_experiment_by_name(\"wiki3-kg-stage1-statements\")\n",
    "if experiment:\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"start_time DESC\"],\n",
    "        max_results=1,\n",
    "    )\n",
    "    \n",
    "    if runs:\n",
    "        latest_run = runs[0]\n",
    "        print(f\"Latest run: {latest_run.info.run_id}\")\n",
    "        print(f\"Metrics: {latest_run.data.metrics}\")\n",
    "        \n",
    "        # Get traces with assessments (human feedback)\n",
    "        try:\n",
    "            traces = client.search_traces(\n",
    "                experiment_ids=[experiment.experiment_id],\n",
    "                max_results=100,\n",
    "            )\n",
    "            print(f\"Found {len(traces)} traces\")\n",
    "        except Exception as e:\n",
    "            print(f\"Trace search error: {e}\")\n",
    "else:\n",
    "    print(\"No experiment found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52239b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use human feedback to improve the judge\n",
    "# After collecting labels in MLflow, create DSPy training examples\n",
    "\n",
    "# For now, save the evaluation data for later use\n",
    "output_dir = Path(\"/workspaces/wiki3-kg-project/data/training\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "eval_df.to_json(output_dir / \"eval_dataset.json\", orient=\"records\", indent=2)\n",
    "print(f\"Saved evaluation dataset to {output_dir / 'eval_dataset.json'}\")\n",
    "\n",
    "print(\"\"\"\n",
    "üìã Next steps for human feedback:\n",
    "\n",
    "1. Start MLflow server:\n",
    "   mlflow server --backend-store-uri sqlite:///mlflow.sqlite --port 5000\n",
    "\n",
    "2. Open MLflow UI at http://127.0.0.1:5000\n",
    "\n",
    "3. Navigate to the experiment 'wiki3-kg-stage1-statements'\n",
    "\n",
    "4. Click on traces to review predictions\n",
    "\n",
    "5. Use the feedback/assessment features to label quality\n",
    "\n",
    "6. Export labeled data for judge improvement\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473e226c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook:\n",
    "1. Loaded Albert Einstein as few-shot examples (seed/guidance)\n",
    "2. Fetched and chunked Wikipedia sample pages for training\n",
    "3. Created train/dev datasets with CID provenance\n",
    "4. Established baseline extraction quality\n",
    "5. Saved all artifacts for optimization\n",
    "\n",
    "**Artifacts saved** (in `data/training/`):\n",
    "- `stage1_config.json` - Model config with CID\n",
    "- `trainset.json` - Training examples with CID\n",
    "- `devset.json` - Dev examples with CID\n",
    "- `fewshot_examples.json` - Few-shot examples with CID\n",
    "- `baseline_results.json` - Baseline score with CID\n",
    "\n",
    "**Next step**: Run `stage1_optimize.ipynb` for MIPROv2 optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
